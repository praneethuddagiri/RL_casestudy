{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_state_0_9_1_act_0_2 = []\n",
    "\n",
    "track_state_4_1_4_act_3_0 = []\n",
    "\n",
    "track_state_1_14_6_act_4_1 = []\n",
    "\n",
    "states_tracked = {'0_9_1_0_2':[], '4_1_4_3_0':[], '1_14_6_4_1':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state-action and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01      \n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon = 0\n",
    "        self.epsilon_decay = 0.0005\n",
    "        self.epsilon_min = 0.00001\n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Write your code here: Add layers to your neural nets        \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Write your code here:\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after we generate each sample from the environment\n",
    "        possible_actions_index,actions = env.requests(state)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.choice(possible_actions_index)\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            state = np.array(env.state_encod_arch2(state)).reshape(1, 36)\n",
    "\n",
    "            # Use the model to predict the Q_values.\n",
    "            q_value = self.model.predict(state)\n",
    "\n",
    "            # truncate the array to only those actions that are part of the ride  requests.\n",
    "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
    "            \n",
    "            return possible_actions_index[np.argmax(q_vals_possible)]\n",
    "        \n",
    "    \n",
    "\n",
    "    def append_sample(self, state, action_idx, reward, next_state, done):\n",
    "        # Write your code here:\n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action_idx, reward, next_state, done))\n",
    "    \n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))# write here\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))# write here\n",
    "            actions, rewards = [], []\n",
    "            done_arr = []\n",
    "            \n",
    "            # Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                state, action_idx, reward, next_state, done = mini_batch[i]\n",
    "                update_input[i] = env.state_encod_arch2(state)\n",
    "                update_output[i] = env.state_encod_arch2(next_state)\n",
    "                actions.append(action_idx)\n",
    "                rewards.append(reward)\n",
    "                done_arr.append(done)\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "        #Predict the target from earlier model\n",
    "        target = self.model.predict(update_input)\n",
    "            \n",
    "        #Get the target for the Q-network\n",
    "        target_val = self.model.predict(update_output)\n",
    "        \n",
    "        #update target vals\n",
    "        for i in range(self.batch_size):\n",
    "            if done_arr[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else: # non-terminal state\n",
    "                target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_val[i])\n",
    "      \n",
    "                \n",
    "        # 4. Fit your model and track the loss values\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 20000\n",
    "\n",
    "max_episode_time = 24*30\n",
    "\n",
    "total_reward = 0\n",
    "avg_rewards_arr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 10, avg_reward -96.2, memory_length 1362, epsilon 0.9955101098295706 total_time 724.0\n",
      "episode 20, avg_reward -229.3, memory_length 2000, epsilon 0.9905449824429005 total_time 720.0\n",
      "episode 30, avg_reward -187.7, memory_length 2000, epsilon 0.9856046187323824 total_time 726.0\n",
      "episode 40, avg_reward -234.3, memory_length 2000, epsilon 0.9806888951886662 total_time 728.0\n",
      "episode 50, avg_reward -150.0, memory_length 2000, epsilon 0.9757976889184073 total_time 723.0\n",
      "episode 60, avg_reward -180.5, memory_length 2000, epsilon 0.9709308776411942 total_time 724.0\n",
      "episode 70, avg_reward -244.1, memory_length 2000, epsilon 0.9660883396864915 total_time 723.0\n",
      "episode 80, avg_reward -192.8, memory_length 2000, epsilon 0.9612699539905982 total_time 720.0\n",
      "episode 90, avg_reward -237.7, memory_length 2000, epsilon 0.9564756000936208 total_time 721.0\n",
      "episode 100, avg_reward -113.7, memory_length 2000, epsilon 0.9517051581364622 total_time 723.0\n",
      "Saving model weights for episode:100\n",
      "episode 110, avg_reward -100.2, memory_length 2000, epsilon 0.9469585088578251 total_time 722.0\n",
      "episode 120, avg_reward -127.9, memory_length 2000, epsilon 0.94223553359123 total_time 722.0\n",
      "episode 130, avg_reward -4.5, memory_length 2000, epsilon 0.9375361142620497 total_time 721.0\n",
      "episode 140, avg_reward -53.6, memory_length 2000, epsilon 0.932860133384556 total_time 723.0\n",
      "episode 150, avg_reward -14.3, memory_length 2000, epsilon 0.9282074740589834 total_time 720.0\n",
      "episode 160, avg_reward -141.0, memory_length 2000, epsilon 0.9235780199686064 total_time 722.0\n",
      "episode 170, avg_reward -70.7, memory_length 2000, epsilon 0.9189716553768317 total_time 724.0\n",
      "episode 180, avg_reward -149.6, memory_length 2000, epsilon 0.9143882651243046 total_time 720.0\n",
      "episode 190, avg_reward -6.5, memory_length 2000, epsilon 0.90982773462603 total_time 723.0\n",
      "episode 200, avg_reward -78.4, memory_length 2000, epsilon 0.905289949868508 total_time 723.0\n",
      "Saving model weights for episode:200\n",
      "episode 210, avg_reward -31.8, memory_length 2000, epsilon 0.9007747974068832 total_time 729.0\n",
      "episode 220, avg_reward -110.5, memory_length 2000, epsilon 0.896282164362109 total_time 721.0\n",
      "episode 230, avg_reward -35.1, memory_length 2000, epsilon 0.8918119384181252 total_time 723.0\n",
      "episode 240, avg_reward -153.6, memory_length 2000, epsilon 0.8873640078190504 total_time 726.0\n",
      "episode 250, avg_reward 25.7, memory_length 2000, epsilon 0.8829382613663882 total_time 720.0\n",
      "episode 260, avg_reward -118.5, memory_length 2000, epsilon 0.8785345884162464 total_time 723.0\n",
      "episode 270, avg_reward -63.5, memory_length 2000, epsilon 0.8741528788765721 total_time 720.0\n",
      "episode 280, avg_reward -64.3, memory_length 2000, epsilon 0.8697930232043986 total_time 722.0\n",
      "episode 290, avg_reward -80.2, memory_length 2000, epsilon 0.8654549124031069 total_time 724.0\n",
      "episode 300, avg_reward -144.5, memory_length 2000, epsilon 0.861138438019701 total_time 731.0\n",
      "Saving model weights for episode:300\n",
      "episode 310, avg_reward 71.8, memory_length 2000, epsilon 0.8568434921420968 total_time 722.0\n",
      "episode 320, avg_reward -19.5, memory_length 2000, epsilon 0.8525699673964233 total_time 724.0\n",
      "episode 330, avg_reward -56.3, memory_length 2000, epsilon 0.8483177569443394 total_time 724.0\n",
      "episode 340, avg_reward -91.8, memory_length 2000, epsilon 0.8440867544803625 total_time 720.0\n",
      "episode 350, avg_reward -17.8, memory_length 2000, epsilon 0.8398768542292104 total_time 726.0\n",
      "episode 360, avg_reward -27.5, memory_length 2000, epsilon 0.8356879509431577 total_time 726.0\n",
      "episode 370, avg_reward -89.9, memory_length 2000, epsilon 0.8315199398994041 total_time 726.0\n",
      "episode 380, avg_reward 31.3, memory_length 2000, epsilon 0.8273727168974562 total_time 725.0\n",
      "episode 390, avg_reward -29.4, memory_length 2000, epsilon 0.8232461782565231 total_time 721.0\n",
      "episode 400, avg_reward 25.8, memory_length 2000, epsilon 0.819140220812924 total_time 721.0\n",
      "Saving model weights for episode:400\n",
      "episode 410, avg_reward -75.5, memory_length 2000, epsilon 0.8150547419175087 total_time 720.0\n",
      "episode 420, avg_reward -43.7, memory_length 2000, epsilon 0.8109896394330922 total_time 724.0\n",
      "episode 430, avg_reward 0.9, memory_length 2000, epsilon 0.8069448117319006 total_time 723.0\n",
      "episode 440, avg_reward 53.5, memory_length 2000, epsilon 0.8029201576930307 total_time 720.0\n",
      "episode 450, avg_reward -110.1, memory_length 2000, epsilon 0.7989155766999219 total_time 729.0\n",
      "episode 460, avg_reward 3.0, memory_length 2000, epsilon 0.7949309686378409 total_time 722.0\n",
      "episode 470, avg_reward -17.6, memory_length 2000, epsilon 0.7909662338913784 total_time 721.0\n",
      "episode 480, avg_reward -88.7, memory_length 2000, epsilon 0.7870212733419595 total_time 726.0\n",
      "episode 490, avg_reward -5.7, memory_length 2000, epsilon 0.7830959883653648 total_time 722.0\n",
      "episode 500, avg_reward -52.2, memory_length 2000, epsilon 0.7791902808292654 total_time 728.0\n",
      "Saving model weights for episode:500\n",
      "episode 510, avg_reward -17.2, memory_length 2000, epsilon 0.7753040530907698 total_time 720.0\n",
      "episode 520, avg_reward -1.9, memory_length 2000, epsilon 0.7714372079939819 total_time 727.0\n",
      "episode 530, avg_reward 1.8, memory_length 2000, epsilon 0.7675896488675729 total_time 721.0\n",
      "episode 540, avg_reward 37.9, memory_length 2000, epsilon 0.7637612795223642 total_time 721.0\n",
      "episode 550, avg_reward 43.0, memory_length 2000, epsilon 0.7599520042489227 total_time 720.0\n",
      "episode 560, avg_reward -99.6, memory_length 2000, epsilon 0.7561617278151684 total_time 721.0\n",
      "episode 570, avg_reward 96.4, memory_length 2000, epsilon 0.752390355463993 total_time 722.0\n",
      "episode 580, avg_reward 59.0, memory_length 2000, epsilon 0.7486377929108913 total_time 728.0\n",
      "episode 590, avg_reward 92.8, memory_length 2000, epsilon 0.7449039463416037 total_time 724.0\n",
      "episode 600, avg_reward -51.3, memory_length 2000, epsilon 0.741188722409772 total_time 724.0\n",
      "Saving model weights for episode:600\n",
      "episode 610, avg_reward -12.8, memory_length 2000, epsilon 0.737492028234604 total_time 723.0\n",
      "episode 620, avg_reward -16.4, memory_length 2000, epsilon 0.733813771398553 total_time 729.0\n",
      "episode 630, avg_reward -29.4, memory_length 2000, epsilon 0.7301538599450065 total_time 721.0\n",
      "episode 640, avg_reward 47.8, memory_length 2000, epsilon 0.7265122023759873 total_time 729.0\n",
      "episode 650, avg_reward 106.2, memory_length 2000, epsilon 0.722888707649867 total_time 721.0\n",
      "episode 660, avg_reward -57.3, memory_length 2000, epsilon 0.7192832851790882 total_time 723.0\n",
      "episode 670, avg_reward 32.4, memory_length 2000, epsilon 0.7156958448279017 total_time 722.0\n",
      "episode 680, avg_reward 118.4, memory_length 2000, epsilon 0.7121262969101118 total_time 721.0\n",
      "episode 690, avg_reward 90.8, memory_length 2000, epsilon 0.7085745521868345 total_time 730.0\n",
      "episode 700, avg_reward 161.7, memory_length 2000, epsilon 0.7050405218642668 total_time 720.0\n",
      "Saving model weights for episode:700\n",
      "episode 710, avg_reward 100.9, memory_length 2000, epsilon 0.7015241175914667 total_time 722.0\n",
      "episode 720, avg_reward 97.3, memory_length 2000, epsilon 0.6980252514581441 total_time 730.0\n",
      "episode 730, avg_reward 94.8, memory_length 2000, epsilon 0.6945438359924634 total_time 723.0\n",
      "episode 740, avg_reward -109.3, memory_length 2000, epsilon 0.6910797841588567 total_time 726.0\n",
      "episode 750, avg_reward 77.7, memory_length 2000, epsilon 0.6876330093558478 total_time 724.0\n",
      "episode 760, avg_reward 57.3, memory_length 2000, epsilon 0.6842034254138871 total_time 728.0\n",
      "episode 770, avg_reward 131.6, memory_length 2000, epsilon 0.6807909465931973 total_time 723.0\n",
      "episode 780, avg_reward -17.7, memory_length 2000, epsilon 0.6773954875816302 total_time 720.0\n",
      "episode 790, avg_reward -16.4, memory_length 2000, epsilon 0.6740169634925337 total_time 728.0\n",
      "episode 800, avg_reward 93.9, memory_length 2000, epsilon 0.6706552898626296 total_time 721.0\n",
      "Saving model weights for episode:800\n",
      "episode 810, avg_reward 49.8, memory_length 2000, epsilon 0.6673103826499021 total_time 730.0\n",
      "episode 820, avg_reward 106.6, memory_length 2000, epsilon 0.6639821582314965 total_time 720.0\n",
      "episode 830, avg_reward 213.1, memory_length 2000, epsilon 0.6606705334016293 total_time 721.0\n",
      "episode 840, avg_reward 194.2, memory_length 2000, epsilon 0.657375425369507 total_time 724.0\n",
      "episode 850, avg_reward 86.6, memory_length 2000, epsilon 0.6540967517572572 total_time 721.0\n",
      "episode 860, avg_reward 85.2, memory_length 2000, epsilon 0.650834430597869 total_time 720.0\n",
      "episode 870, avg_reward 149.5, memory_length 2000, epsilon 0.6475883803331434 total_time 730.0\n",
      "episode 880, avg_reward -79.6, memory_length 2000, epsilon 0.6443585198116547 total_time 720.0\n",
      "episode 890, avg_reward -14.8, memory_length 2000, epsilon 0.6411447682867216 total_time 722.0\n",
      "episode 900, avg_reward 174.5, memory_length 2000, epsilon 0.6379470454143887 total_time 721.0\n",
      "Saving model weights for episode:900\n",
      "episode 910, avg_reward 246.3, memory_length 2000, epsilon 0.6347652712514176 total_time 725.0\n",
      "episode 920, avg_reward 179.7, memory_length 2000, epsilon 0.6315993662532885 total_time 723.0\n",
      "episode 930, avg_reward 82.5, memory_length 2000, epsilon 0.6284492512722115 total_time 727.0\n",
      "episode 940, avg_reward 127.9, memory_length 2000, epsilon 0.6253148475551482 total_time 720.0\n",
      "episode 950, avg_reward 128.7, memory_length 2000, epsilon 0.6221960767418422 total_time 721.0\n",
      "episode 960, avg_reward 190.5, memory_length 2000, epsilon 0.6190928608628609 total_time 721.0\n",
      "episode 970, avg_reward -3.2, memory_length 2000, epsilon 0.6160051223376455 total_time 720.0\n",
      "episode 980, avg_reward 141.1, memory_length 2000, epsilon 0.6129327839725722 total_time 720.0\n",
      "episode 990, avg_reward 90.2, memory_length 2000, epsilon 0.6098757689590218 total_time 725.0\n",
      "episode 1000, avg_reward 242.4, memory_length 2000, epsilon 0.6068340008714599 total_time 724.0\n",
      "Saving model weights for episode:1000\n",
      "episode 1010, avg_reward 180.7, memory_length 2000, epsilon 0.6038074036655255 total_time 722.0\n",
      "episode 1020, avg_reward 287.7, memory_length 2000, epsilon 0.6007959016761313 total_time 723.0\n",
      "episode 1030, avg_reward 260.0, memory_length 2000, epsilon 0.5977994196155705 total_time 724.0\n",
      "episode 1040, avg_reward 238.9, memory_length 2000, epsilon 0.5948178825716354 total_time 722.0\n",
      "episode 1050, avg_reward 173.7, memory_length 2000, epsilon 0.5918512160057446 total_time 725.0\n",
      "episode 1060, avg_reward 239.7, memory_length 2000, epsilon 0.5888993457510797 total_time 729.0\n",
      "episode 1070, avg_reward 148.6, memory_length 2000, epsilon 0.5859621980107305 total_time 722.0\n",
      "episode 1080, avg_reward 149.5, memory_length 2000, epsilon 0.5830396993558503 total_time 729.0\n",
      "episode 1090, avg_reward 118.7, memory_length 2000, epsilon 0.5801317767238208 total_time 720.0\n",
      "episode 1100, avg_reward 267.5, memory_length 2000, epsilon 0.5772383574164245 total_time 726.0\n",
      "Saving model weights for episode:1100\n",
      "episode 1110, avg_reward 209.2, memory_length 2000, epsilon 0.5743593690980282 total_time 726.0\n",
      "episode 1120, avg_reward 286.0, memory_length 2000, epsilon 0.571494739793774 total_time 726.0\n",
      "episode 1130, avg_reward 228.6, memory_length 2000, epsilon 0.5686443978877799 total_time 720.0\n",
      "episode 1140, avg_reward 168.5, memory_length 2000, epsilon 0.56580827212135 total_time 726.0\n",
      "episode 1150, avg_reward 183.5, memory_length 2000, epsilon 0.5629862915911923 total_time 720.0\n",
      "episode 1160, avg_reward 162.9, memory_length 2000, epsilon 0.5601783857476466 total_time 725.0\n",
      "episode 1170, avg_reward 246.7, memory_length 2000, epsilon 0.5573844843929205 total_time 723.0\n",
      "episode 1180, avg_reward 215.0, memory_length 2000, epsilon 0.5546045176793348 total_time 723.0\n",
      "episode 1190, avg_reward 325.6, memory_length 2000, epsilon 0.5518384161075767 total_time 723.0\n",
      "episode 1200, avg_reward 186.9, memory_length 2000, epsilon 0.5490861105249629 total_time 725.0\n",
      "Saving model weights for episode:1200\n",
      "episode 1210, avg_reward 257.8, memory_length 2000, epsilon 0.5463475321237106 total_time 724.0\n",
      "episode 1220, avg_reward 304.7, memory_length 2000, epsilon 0.5436226124392168 total_time 723.0\n",
      "episode 1230, avg_reward 370.7, memory_length 2000, epsilon 0.5409112833483479 total_time 724.0\n",
      "episode 1240, avg_reward 448.5, memory_length 2000, epsilon 0.538213477067735 total_time 731.0\n",
      "episode 1250, avg_reward 265.7, memory_length 2000, epsilon 0.5355291261520809 total_time 725.0\n",
      "episode 1260, avg_reward 257.5, memory_length 2000, epsilon 0.5328581634924728 total_time 721.0\n",
      "episode 1270, avg_reward 243.2, memory_length 2000, epsilon 0.5302005223147049 total_time 723.0\n",
      "episode 1280, avg_reward 356.4, memory_length 2000, epsilon 0.5275561361776097 total_time 721.0\n",
      "episode 1290, avg_reward 228.4, memory_length 2000, epsilon 0.5249249389713958 total_time 725.0\n",
      "episode 1300, avg_reward 279.1, memory_length 2000, epsilon 0.522306864915996 total_time 724.0\n",
      "Saving model weights for episode:1300\n",
      "episode 1310, avg_reward 294.6, memory_length 2000, epsilon 0.5197018485594226 total_time 726.0\n",
      "episode 1320, avg_reward 257.7, memory_length 2000, epsilon 0.517109824776131 total_time 721.0\n",
      "episode 1330, avg_reward 348.3, memory_length 2000, epsilon 0.5145307287653916 total_time 721.0\n",
      "episode 1340, avg_reward 367.9, memory_length 2000, epsilon 0.5119644960496699 total_time 729.0\n",
      "episode 1350, avg_reward 323.8, memory_length 2000, epsilon 0.5094110624730143 total_time 722.0\n",
      "episode 1360, avg_reward 347.2, memory_length 2000, epsilon 0.5068703641994523 total_time 726.0\n",
      "episode 1370, avg_reward 469.7, memory_length 2000, epsilon 0.5043423377113948 total_time 720.0\n",
      "episode 1380, avg_reward 350.9, memory_length 2000, epsilon 0.501826919808048 total_time 726.0\n",
      "episode 1390, avg_reward 463.6, memory_length 2000, epsilon 0.4993240476038332 total_time 721.0\n",
      "episode 1400, avg_reward 267.5, memory_length 2000, epsilon 0.496833658526815 total_time 723.0\n",
      "Saving model weights for episode:1400\n",
      "episode 1410, avg_reward 346.9, memory_length 2000, epsilon 0.4943556903171367 total_time 720.0\n",
      "episode 1420, avg_reward 333.5, memory_length 2000, epsilon 0.4918900810254641 total_time 720.0\n",
      "episode 1430, avg_reward 331.6, memory_length 2000, epsilon 0.48943676901143646 total_time 722.0\n",
      "episode 1440, avg_reward 372.8, memory_length 2000, epsilon 0.4869956929421256 total_time 737.0\n",
      "episode 1450, avg_reward 336.0, memory_length 2000, epsilon 0.4845667917905026 total_time 730.0\n",
      "episode 1460, avg_reward 263.6, memory_length 2000, epsilon 0.4821500048339123 total_time 720.0\n",
      "episode 1470, avg_reward 345.4, memory_length 2000, epsilon 0.4797452716525548 total_time 720.0\n",
      "episode 1480, avg_reward 411.7, memory_length 2000, epsilon 0.47735253212797546 total_time 721.0\n",
      "episode 1490, avg_reward 476.4, memory_length 2000, epsilon 0.4749717264415614 total_time 724.0\n",
      "episode 1500, avg_reward 422.4, memory_length 2000, epsilon 0.4726027950730465 total_time 722.0\n",
      "Saving model weights for episode:1500\n",
      "episode 1510, avg_reward 422.9, memory_length 2000, epsilon 0.4702456787990232 total_time 721.0\n",
      "episode 1520, avg_reward 397.8, memory_length 2000, epsilon 0.4679003186914618 total_time 725.0\n",
      "episode 1530, avg_reward 355.1, memory_length 2000, epsilon 0.46556665611623754 total_time 730.0\n",
      "episode 1540, avg_reward 375.9, memory_length 2000, epsilon 0.46324463273166455 total_time 726.0\n",
      "episode 1550, avg_reward 373.7, memory_length 2000, epsilon 0.46093419048703715 total_time 723.0\n",
      "episode 1560, avg_reward 410.6, memory_length 2000, epsilon 0.4586352716211789 total_time 720.0\n",
      "episode 1570, avg_reward 417.4, memory_length 2000, epsilon 0.4563478186609985 total_time 730.0\n",
      "episode 1580, avg_reward 497.3, memory_length 2000, epsilon 0.4540717744200527 total_time 722.0\n",
      "episode 1590, avg_reward 425.8, memory_length 2000, epsilon 0.451807081997117 total_time 723.0\n",
      "episode 1600, avg_reward 455.3, memory_length 2000, epsilon 0.44955368477476293 total_time 726.0\n",
      "Saving model weights for episode:1600\n",
      "episode 1610, avg_reward 415.9, memory_length 2000, epsilon 0.4473115264179424 total_time 734.0\n",
      "episode 1620, avg_reward 540.5, memory_length 2000, epsilon 0.4450805508725799 total_time 722.0\n",
      "episode 1630, avg_reward 523.0, memory_length 2000, epsilon 0.4428607023641705 total_time 723.0\n",
      "episode 1640, avg_reward 603.5, memory_length 2000, epsilon 0.44065192539638587 total_time 721.0\n",
      "episode 1650, avg_reward 605.3, memory_length 2000, epsilon 0.4384541647496868 total_time 724.0\n",
      "episode 1660, avg_reward 569.0, memory_length 2000, epsilon 0.43626736547994266 total_time 726.0\n",
      "episode 1670, avg_reward 516.2, memory_length 2000, epsilon 0.43409147291705774 total_time 723.0\n",
      "episode 1680, avg_reward 569.6, memory_length 2000, epsilon 0.43192643266360475 total_time 723.0\n",
      "episode 1690, avg_reward 514.6, memory_length 2000, epsilon 0.4297721905934645 total_time 723.0\n",
      "episode 1700, avg_reward 462.2, memory_length 2000, epsilon 0.4276286928504731 total_time 721.0\n",
      "Saving model weights for episode:1700\n",
      "episode 1710, avg_reward 668.5, memory_length 2000, epsilon 0.4254958858470753 total_time 729.0\n",
      "episode 1720, avg_reward 485.4, memory_length 2000, epsilon 0.4233737162629849 total_time 724.0\n",
      "episode 1730, avg_reward 620.1, memory_length 2000, epsilon 0.4212621310438519 total_time 724.0\n",
      "episode 1740, avg_reward 528.8, memory_length 2000, epsilon 0.4191610773999357 total_time 723.0\n",
      "episode 1750, avg_reward 547.0, memory_length 2000, epsilon 0.4170705028047858 total_time 722.0\n",
      "episode 1760, avg_reward 503.5, memory_length 2000, epsilon 0.4149903549939285 total_time 723.0\n",
      "episode 1770, avg_reward 461.5, memory_length 2000, epsilon 0.41292058196356013 total_time 720.0\n",
      "episode 1780, avg_reward 445.8, memory_length 2000, epsilon 0.4108611319692471 total_time 722.0\n",
      "episode 1790, avg_reward 576.6, memory_length 2000, epsilon 0.4088119535246324 total_time 720.0\n",
      "episode 1800, avg_reward 542.1, memory_length 2000, epsilon 0.40677299540014816 total_time 722.0\n",
      "Saving model weights for episode:1800\n",
      "episode 1810, avg_reward 494.8, memory_length 2000, epsilon 0.404744206621735 total_time 733.0\n",
      "episode 1820, avg_reward 474.8, memory_length 2000, epsilon 0.40272553646956777 total_time 723.0\n",
      "episode 1830, avg_reward 568.8, memory_length 2000, epsilon 0.40071693447678763 total_time 724.0\n",
      "episode 1840, avg_reward 524.8, memory_length 2000, epsilon 0.3987183504282401 total_time 722.0\n",
      "episode 1850, avg_reward 423.1, memory_length 2000, epsilon 0.3967297343592199 total_time 721.0\n",
      "episode 1860, avg_reward 518.2, memory_length 2000, epsilon 0.3947510365542216 total_time 723.0\n",
      "episode 1870, avg_reward 477.2, memory_length 2000, epsilon 0.3927822075456972 total_time 722.0\n",
      "episode 1880, avg_reward 518.7, memory_length 2000, epsilon 0.3908231981128189 total_time 722.0\n",
      "episode 1890, avg_reward 461.8, memory_length 2000, epsilon 0.38887395928024876 total_time 720.0\n",
      "episode 1900, avg_reward 426.0, memory_length 2000, epsilon 0.3869344423169145 total_time 720.0\n",
      "Saving model weights for episode:1900\n",
      "episode 1910, avg_reward 237.0, memory_length 2000, epsilon 0.38500459873479104 total_time 721.0\n",
      "episode 1920, avg_reward 429.6, memory_length 2000, epsilon 0.38308438028768826 total_time 730.0\n",
      "episode 1930, avg_reward 403.8, memory_length 2000, epsilon 0.38117373897004503 total_time 720.0\n",
      "episode 1940, avg_reward 486.9, memory_length 2000, epsilon 0.37927262701572884 total_time 725.0\n",
      "episode 1950, avg_reward 417.7, memory_length 2000, epsilon 0.37738099689684185 total_time 721.0\n",
      "episode 1960, avg_reward 442.4, memory_length 2000, epsilon 0.37549880132253255 total_time 729.0\n",
      "episode 1970, avg_reward 432.8, memory_length 2000, epsilon 0.37362599323781354 total_time 721.0\n",
      "episode 1980, avg_reward 442.4, memory_length 2000, epsilon 0.3717625258223852 total_time 722.0\n",
      "episode 1990, avg_reward 537.9, memory_length 2000, epsilon 0.3699083524894651 total_time 720.0\n",
      "episode 2000, avg_reward 512.3, memory_length 2000, epsilon 0.3680634268846233 total_time 720.0\n",
      "Saving model weights for episode:2000\n",
      "episode 2010, avg_reward 399.6, memory_length 2000, epsilon 0.3662277028846236 total_time 722.0\n",
      "episode 2020, avg_reward 487.5, memory_length 2000, epsilon 0.3644011345962703 total_time 723.0\n",
      "episode 2030, avg_reward 525.3, memory_length 2000, epsilon 0.36258367635526134 total_time 720.0\n",
      "episode 2040, avg_reward 511.0, memory_length 2000, epsilon 0.36077528272504567 total_time 728.0\n",
      "episode 2050, avg_reward 491.6, memory_length 2000, epsilon 0.3589759084956886 total_time 726.0\n",
      "episode 2060, avg_reward 469.4, memory_length 2000, epsilon 0.35718550868274057 total_time 723.0\n",
      "episode 2070, avg_reward 469.7, memory_length 2000, epsilon 0.3554040385261131 total_time 721.0\n",
      "episode 2080, avg_reward 394.0, memory_length 2000, epsilon 0.3536314534889593 total_time 720.0\n",
      "episode 2090, avg_reward 409.3, memory_length 2000, epsilon 0.3518677092565612 total_time 726.0\n",
      "episode 2100, avg_reward 425.4, memory_length 2000, epsilon 0.3501127617352208 total_time 720.0\n",
      "Saving model weights for episode:2100\n",
      "episode 2110, avg_reward 404.4, memory_length 2000, epsilon 0.348366567051159 total_time 733.0\n",
      "episode 2120, avg_reward 384.2, memory_length 2000, epsilon 0.34662908154941746 total_time 722.0\n",
      "episode 2130, avg_reward 446.3, memory_length 2000, epsilon 0.34490026179276834 total_time 720.0\n",
      "episode 2140, avg_reward 563.5, memory_length 2000, epsilon 0.34318006456062755 total_time 723.0\n",
      "episode 2150, avg_reward 563.5, memory_length 2000, epsilon 0.34146844684797484 total_time 726.0\n",
      "episode 2160, avg_reward 425.6, memory_length 2000, epsilon 0.3397653658642781 total_time 722.0\n",
      "episode 2170, avg_reward 484.3, memory_length 2000, epsilon 0.3380707790324241 total_time 728.0\n",
      "episode 2180, avg_reward 502.6, memory_length 2000, epsilon 0.33638464398765383 total_time 724.0\n",
      "episode 2190, avg_reward 436.2, memory_length 2000, epsilon 0.3347069185765032 total_time 720.0\n",
      "episode 2200, avg_reward 399.7, memory_length 2000, epsilon 0.33303756085574976 total_time 727.0\n",
      "Saving model weights for episode:2200\n",
      "episode 2210, avg_reward 388.7, memory_length 2000, epsilon 0.33137652909136334 total_time 721.0\n",
      "episode 2220, avg_reward 522.3, memory_length 2000, epsilon 0.3297237817574635 total_time 720.0\n",
      "episode 2230, avg_reward 523.8, memory_length 2000, epsilon 0.3280792775352806 total_time 728.0\n",
      "episode 2240, avg_reward 367.9, memory_length 2000, epsilon 0.3264429753121237 total_time 723.0\n",
      "episode 2250, avg_reward 447.5, memory_length 2000, epsilon 0.32481483418035173 total_time 722.0\n",
      "episode 2260, avg_reward 465.3, memory_length 2000, epsilon 0.3231948134363518 total_time 721.0\n",
      "episode 2270, avg_reward 431.1, memory_length 2000, epsilon 0.32158287257952084 total_time 726.0\n",
      "episode 2280, avg_reward 460.3, memory_length 2000, epsilon 0.3199789713112535 total_time 730.0\n",
      "episode 2290, avg_reward 535.4, memory_length 2000, epsilon 0.31838306953393447 total_time 728.0\n",
      "episode 2300, avg_reward 607.6, memory_length 2000, epsilon 0.31679512734993637 total_time 722.0\n",
      "Saving model weights for episode:2300\n",
      "episode 2310, avg_reward 509.8, memory_length 2000, epsilon 0.31521510506062167 total_time 724.0\n",
      "episode 2320, avg_reward 638.1, memory_length 2000, epsilon 0.313642963165351 total_time 724.0\n",
      "episode 2330, avg_reward 676.1, memory_length 2000, epsilon 0.31207866236049503 total_time 723.0\n",
      "episode 2340, avg_reward 604.3, memory_length 2000, epsilon 0.3105221635384522 total_time 725.0\n",
      "episode 2350, avg_reward 702.0, memory_length 2000, epsilon 0.3089734277866708 total_time 730.0\n",
      "episode 2360, avg_reward 633.9, memory_length 2000, epsilon 0.30743241638667657 total_time 722.0\n",
      "episode 2370, avg_reward 508.9, memory_length 2000, epsilon 0.305899090813104 total_time 730.0\n",
      "episode 2380, avg_reward 622.6, memory_length 2000, epsilon 0.30437341273273416 total_time 721.0\n",
      "episode 2390, avg_reward 632.3, memory_length 2000, epsilon 0.3028553440035353 total_time 722.0\n",
      "episode 2400, avg_reward 805.5, memory_length 2000, epsilon 0.30134484667371036 total_time 724.0\n",
      "Saving model weights for episode:2400\n",
      "episode 2410, avg_reward 762.4, memory_length 2000, epsilon 0.2998418829807472 total_time 726.0\n",
      "episode 2420, avg_reward 619.5, memory_length 2000, epsilon 0.29834641535047546 total_time 722.0\n",
      "episode 2430, avg_reward 740.7, memory_length 2000, epsilon 0.29685840639612626 total_time 724.0\n",
      "episode 2440, avg_reward 575.2, memory_length 2000, epsilon 0.29537781891739845 total_time 721.0\n",
      "episode 2450, avg_reward 654.0, memory_length 2000, epsilon 0.2939046158995279 total_time 720.0\n",
      "episode 2460, avg_reward 601.5, memory_length 2000, epsilon 0.29243876051236223 total_time 721.0\n",
      "episode 2470, avg_reward 794.7, memory_length 2000, epsilon 0.29098021610944064 total_time 730.0\n",
      "episode 2480, avg_reward 660.1, memory_length 2000, epsilon 0.289528946227077 total_time 720.0\n",
      "episode 2490, avg_reward 616.1, memory_length 2000, epsilon 0.2880849145834487 total_time 723.0\n",
      "episode 2500, avg_reward 595.2, memory_length 2000, epsilon 0.2866480850776894 total_time 723.0\n",
      "Saving model weights for episode:2500\n",
      "episode 2510, avg_reward 701.3, memory_length 2000, epsilon 0.28521842178898665 total_time 723.0\n",
      "episode 2520, avg_reward 663.8, memory_length 2000, epsilon 0.28379588897568375 total_time 727.0\n",
      "episode 2530, avg_reward 604.8, memory_length 2000, epsilon 0.28238045107438636 total_time 723.0\n",
      "episode 2540, avg_reward 442.6, memory_length 2000, epsilon 0.28097207269907304 total_time 721.0\n",
      "episode 2550, avg_reward 580.8, memory_length 2000, epsilon 0.27957071864021127 total_time 724.0\n",
      "episode 2560, avg_reward 480.0, memory_length 2000, epsilon 0.27817635386387646 total_time 723.0\n",
      "episode 2570, avg_reward 651.1, memory_length 2000, epsilon 0.27678894351087663 total_time 723.0\n",
      "episode 2580, avg_reward 616.4, memory_length 2000, epsilon 0.2754084528958806 total_time 722.0\n",
      "episode 2590, avg_reward 771.7, memory_length 2000, epsilon 0.27403484750655127 total_time 720.0\n",
      "episode 2600, avg_reward 734.0, memory_length 2000, epsilon 0.2726680930026822 total_time 729.0\n",
      "Saving model weights for episode:2600\n",
      "episode 2610, avg_reward 922.0, memory_length 2000, epsilon 0.2713081552153397 total_time 724.0\n",
      "episode 2620, avg_reward 874.0, memory_length 2000, epsilon 0.26995500014600815 total_time 723.0\n",
      "episode 2630, avg_reward 802.2, memory_length 2000, epsilon 0.26860859396574055 total_time 721.0\n",
      "episode 2640, avg_reward 810.4, memory_length 2000, epsilon 0.267268903014312 total_time 720.0\n",
      "episode 2650, avg_reward 830.5, memory_length 2000, epsilon 0.2659358937993792 total_time 725.0\n",
      "episode 2660, avg_reward 752.2, memory_length 2000, epsilon 0.26460953299564216 total_time 729.0\n",
      "episode 2670, avg_reward 899.6, memory_length 2000, epsilon 0.26328978744401177 total_time 732.0\n",
      "episode 2680, avg_reward 862.8, memory_length 2000, epsilon 0.2619766241507805 total_time 728.0\n",
      "episode 2690, avg_reward 739.5, memory_length 2000, epsilon 0.26067001028679765 total_time 721.0\n",
      "episode 2700, avg_reward 686.1, memory_length 2000, epsilon 0.2593699131866486 total_time 720.0\n",
      "Saving model weights for episode:2700\n",
      "episode 2710, avg_reward 659.8, memory_length 2000, epsilon 0.25807630034783796 total_time 721.0\n",
      "episode 2720, avg_reward 760.5, memory_length 2000, epsilon 0.25678913942997755 total_time 720.0\n",
      "episode 2730, avg_reward 839.6, memory_length 2000, epsilon 0.2555083982539773 total_time 724.0\n",
      "episode 2740, avg_reward 846.3, memory_length 2000, epsilon 0.25423404480124123 total_time 721.0\n",
      "episode 2750, avg_reward 699.3, memory_length 2000, epsilon 0.2529660472128665 total_time 731.0\n",
      "episode 2760, avg_reward 659.8, memory_length 2000, epsilon 0.2517043737888474 total_time 721.0\n",
      "episode 2770, avg_reward 628.7, memory_length 2000, epsilon 0.2504489929872826 total_time 720.0\n",
      "episode 2780, avg_reward 796.4, memory_length 2000, epsilon 0.24919987342358682 total_time 725.0\n",
      "episode 2790, avg_reward 821.6, memory_length 2000, epsilon 0.24795698386970572 total_time 726.0\n",
      "episode 2800, avg_reward 749.2, memory_length 2000, epsilon 0.24672029325333586 total_time 728.0\n",
      "Saving model weights for episode:2800\n",
      "episode 2810, avg_reward 701.6, memory_length 2000, epsilon 0.2454897706571473 total_time 727.0\n",
      "episode 2820, avg_reward 792.1, memory_length 2000, epsilon 0.24426538531801115 total_time 723.0\n",
      "episode 2830, avg_reward 625.8, memory_length 2000, epsilon 0.24304710662623008 total_time 720.0\n",
      "episode 2840, avg_reward 624.2, memory_length 2000, epsilon 0.2418349041247734 total_time 720.0\n",
      "episode 2850, avg_reward 512.2, memory_length 2000, epsilon 0.2406287475085154 total_time 720.0\n",
      "episode 2860, avg_reward 855.9, memory_length 2000, epsilon 0.23942860662347792 total_time 720.0\n",
      "episode 2870, avg_reward 720.5, memory_length 2000, epsilon 0.23823445146607622 total_time 725.0\n",
      "episode 2880, avg_reward 734.3, memory_length 2000, epsilon 0.23704625218236927 total_time 720.0\n",
      "episode 2890, avg_reward 758.6, memory_length 2000, epsilon 0.235863979067313 total_time 721.0\n",
      "episode 2900, avg_reward 840.6, memory_length 2000, epsilon 0.23468760256401805 total_time 720.0\n",
      "Saving model weights for episode:2900\n",
      "episode 2910, avg_reward 860.6, memory_length 2000, epsilon 0.23351709326301048 total_time 721.0\n",
      "episode 2920, avg_reward 706.2, memory_length 2000, epsilon 0.2323524219014969 total_time 722.0\n",
      "episode 2930, avg_reward 833.9, memory_length 2000, epsilon 0.2311935593626325 total_time 724.0\n",
      "episode 2940, avg_reward 629.8, memory_length 2000, epsilon 0.23004047667479355 total_time 720.0\n",
      "episode 2950, avg_reward 853.5, memory_length 2000, epsilon 0.22889314501085276 total_time 725.0\n",
      "episode 2960, avg_reward 833.2, memory_length 2000, epsilon 0.22775153568745873 total_time 731.0\n",
      "episode 2970, avg_reward 615.5, memory_length 2000, epsilon 0.226615620164319 total_time 723.0\n",
      "episode 2980, avg_reward 742.7, memory_length 2000, epsilon 0.22548537004348623 total_time 721.0\n",
      "episode 2990, avg_reward 890.9, memory_length 2000, epsilon 0.22436075706864864 total_time 724.0\n",
      "episode 3000, avg_reward 904.5, memory_length 2000, epsilon 0.22324175312442318 total_time 726.0\n",
      "Saving model weights for episode:3000\n",
      "episode 3010, avg_reward 941.8, memory_length 2000, epsilon 0.22212833023565307 total_time 726.0\n",
      "episode 3020, avg_reward 729.0, memory_length 2000, epsilon 0.221020460566708 total_time 727.0\n",
      "episode 3030, avg_reward 655.5, memory_length 2000, epsilon 0.21991811642078862 total_time 721.0\n",
      "episode 3040, avg_reward 858.0, memory_length 2000, epsilon 0.21882127023923378 total_time 720.0\n",
      "episode 3050, avg_reward 865.6, memory_length 2000, epsilon 0.21772989460083195 total_time 722.0\n",
      "episode 3060, avg_reward 818.1, memory_length 2000, epsilon 0.2166439622211352 total_time 720.0\n",
      "episode 3070, avg_reward 927.4, memory_length 2000, epsilon 0.21556344595177754 total_time 723.0\n",
      "episode 3080, avg_reward 938.0, memory_length 2000, epsilon 0.21448831877979593 total_time 722.0\n",
      "episode 3090, avg_reward 902.1, memory_length 2000, epsilon 0.21341855382695513 total_time 721.0\n",
      "episode 3100, avg_reward 1058.7, memory_length 2000, epsilon 0.21235412434907552 total_time 724.0\n",
      "Saving model weights for episode:3100\n",
      "episode 3110, avg_reward 922.0, memory_length 2000, epsilon 0.2112950037353648 total_time 722.0\n",
      "episode 3120, avg_reward 993.0, memory_length 2000, epsilon 0.21024116550775238 total_time 725.0\n",
      "episode 3130, avg_reward 890.3, memory_length 2000, epsilon 0.20919258332022778 total_time 724.0\n",
      "episode 3140, avg_reward 977.2, memory_length 2000, epsilon 0.20814923095818155 total_time 727.0\n",
      "episode 3150, avg_reward 916.2, memory_length 2000, epsilon 0.20711108233775047 total_time 721.0\n",
      "episode 3160, avg_reward 821.4, memory_length 2000, epsilon 0.20607811150516483 total_time 720.0\n",
      "episode 3170, avg_reward 850.0, memory_length 2000, epsilon 0.2050502926361001 total_time 721.0\n",
      "episode 3180, avg_reward 768.3, memory_length 2000, epsilon 0.20402760003503095 total_time 724.0\n",
      "episode 3190, avg_reward 817.1, memory_length 2000, epsilon 0.2030100081345892 total_time 727.0\n",
      "episode 3200, avg_reward 744.0, memory_length 2000, epsilon 0.20199749149492416 total_time 729.0\n",
      "Saving model weights for episode:3200\n",
      "episode 3210, avg_reward 834.2, memory_length 2000, epsilon 0.20099002480306727 total_time 720.0\n",
      "episode 3220, avg_reward 828.2, memory_length 2000, epsilon 0.1999875828722987 total_time 721.0\n",
      "episode 3230, avg_reward 690.7, memory_length 2000, epsilon 0.1989901406415179 total_time 725.0\n",
      "episode 3240, avg_reward 712.6, memory_length 2000, epsilon 0.19799767317461728 total_time 728.0\n",
      "episode 3250, avg_reward 764.3, memory_length 2000, epsilon 0.19701015565985838 total_time 732.0\n",
      "episode 3260, avg_reward 859.8, memory_length 2000, epsilon 0.19602756340925195 total_time 725.0\n",
      "episode 3270, avg_reward 832.4, memory_length 2000, epsilon 0.1950498718579405 total_time 721.0\n",
      "episode 3280, avg_reward 794.9, memory_length 2000, epsilon 0.1940770565635844 total_time 723.0\n",
      "episode 3290, avg_reward 756.3, memory_length 2000, epsilon 0.19310909320575054 total_time 728.0\n",
      "episode 3300, avg_reward 914.3, memory_length 2000, epsilon 0.19214595758530462 total_time 724.0\n",
      "Saving model weights for episode:3300\n",
      "episode 3310, avg_reward 861.6, memory_length 2000, epsilon 0.1911876256238059 total_time 720.0\n",
      "episode 3320, avg_reward 775.8, memory_length 2000, epsilon 0.19023407336290554 total_time 723.0\n",
      "episode 3330, avg_reward 626.9, memory_length 2000, epsilon 0.18928527696374722 total_time 724.0\n",
      "episode 3340, avg_reward 621.6, memory_length 2000, epsilon 0.18834121270637166 total_time 721.0\n",
      "episode 3350, avg_reward 760.1, memory_length 2000, epsilon 0.18740185698912315 total_time 730.0\n",
      "episode 3360, avg_reward 741.1, memory_length 2000, epsilon 0.18646718632805995 total_time 720.0\n",
      "episode 3370, avg_reward 769.9, memory_length 2000, epsilon 0.18553717735636674 total_time 721.0\n",
      "episode 3380, avg_reward 916.5, memory_length 2000, epsilon 0.1846118068237709 total_time 726.0\n",
      "episode 3390, avg_reward 1043.9, memory_length 2000, epsilon 0.1836910515959608 total_time 722.0\n",
      "episode 3400, avg_reward 811.0, memory_length 2000, epsilon 0.1827748886540079 total_time 733.0\n",
      "Saving model weights for episode:3400\n",
      "episode 3410, avg_reward 798.8, memory_length 2000, epsilon 0.18186329509379084 total_time 725.0\n",
      "episode 3420, avg_reward 901.7, memory_length 2000, epsilon 0.1809562481254232 total_time 727.0\n",
      "episode 3430, avg_reward 839.1, memory_length 2000, epsilon 0.18005372507268352 total_time 720.0\n",
      "episode 3440, avg_reward 723.7, memory_length 2000, epsilon 0.17915570337244846 total_time 723.0\n",
      "episode 3450, avg_reward 928.6, memory_length 2000, epsilon 0.17826216057412872 total_time 721.0\n",
      "episode 3460, avg_reward 903.3, memory_length 2000, epsilon 0.17737307433910787 total_time 724.0\n",
      "episode 3470, avg_reward 922.6, memory_length 2000, epsilon 0.17648842244018367 total_time 720.0\n",
      "episode 3480, avg_reward 900.9, memory_length 2000, epsilon 0.17560818276101256 total_time 724.0\n",
      "episode 3490, avg_reward 591.9, memory_length 2000, epsilon 0.1747323332955568 total_time 725.0\n",
      "episode 3500, avg_reward 640.3, memory_length 2000, epsilon 0.17386085214753402 total_time 720.0\n",
      "Saving model weights for episode:3500\n",
      "episode 3510, avg_reward 837.9, memory_length 2000, epsilon 0.17299371752987022 total_time 722.0\n",
      "episode 3520, avg_reward 912.5, memory_length 2000, epsilon 0.17213090776415474 total_time 723.0\n",
      "episode 3530, avg_reward 860.8, memory_length 2000, epsilon 0.17127240128009855 total_time 723.0\n",
      "episode 3540, avg_reward 866.8, memory_length 2000, epsilon 0.17041817661499478 total_time 724.0\n",
      "episode 3550, avg_reward 977.2, memory_length 2000, epsilon 0.16956821241318237 total_time 723.0\n",
      "episode 3560, avg_reward 859.1, memory_length 2000, epsilon 0.16872248742551194 total_time 721.0\n",
      "episode 3570, avg_reward 944.8, memory_length 2000, epsilon 0.1678809805088148 total_time 720.0\n",
      "episode 3580, avg_reward 778.4, memory_length 2000, epsilon 0.16704367062537417 total_time 722.0\n",
      "episode 3590, avg_reward 980.2, memory_length 2000, epsilon 0.16621053684239942 total_time 720.0\n",
      "episode 3600, avg_reward 961.9, memory_length 2000, epsilon 0.1653815583315025 total_time 722.0\n",
      "Saving model weights for episode:3600\n",
      "episode 3610, avg_reward 796.4, memory_length 2000, epsilon 0.16455671436817754 total_time 720.0\n",
      "episode 3620, avg_reward 706.7, memory_length 2000, epsilon 0.1637359843312824 total_time 720.0\n",
      "episode 3630, avg_reward 770.4, memory_length 2000, epsilon 0.1629193477025235 total_time 726.0\n",
      "episode 3640, avg_reward 842.5, memory_length 2000, epsilon 0.1621067840659425 total_time 724.0\n",
      "episode 3650, avg_reward 847.3, memory_length 2000, epsilon 0.1612982731074063 total_time 721.0\n",
      "episode 3660, avg_reward 928.5, memory_length 2000, epsilon 0.16049379461409866 total_time 720.0\n",
      "episode 3670, avg_reward 913.9, memory_length 2000, epsilon 0.1596933284740155 total_time 722.0\n",
      "episode 3680, avg_reward 925.1, memory_length 2000, epsilon 0.1588968546754615 total_time 726.0\n",
      "episode 3690, avg_reward 850.1, memory_length 2000, epsilon 0.15810435330655032 total_time 722.0\n",
      "episode 3700, avg_reward 905.3, memory_length 2000, epsilon 0.15731580455470637 total_time 721.0\n",
      "Saving model weights for episode:3700\n",
      "episode 3710, avg_reward 919.0, memory_length 2000, epsilon 0.1565311887061699 total_time 721.0\n",
      "episode 3720, avg_reward 826.4, memory_length 2000, epsilon 0.15575048614550369 total_time 721.0\n",
      "episode 3730, avg_reward 818.3, memory_length 2000, epsilon 0.15497367735510315 total_time 723.0\n",
      "episode 3740, avg_reward 833.8, memory_length 2000, epsilon 0.15420074291470803 total_time 720.0\n",
      "episode 3750, avg_reward 934.9, memory_length 2000, epsilon 0.15343166350091708 total_time 725.0\n",
      "episode 3760, avg_reward 903.1, memory_length 2000, epsilon 0.1526664198867049 total_time 722.0\n",
      "episode 3770, avg_reward 787.5, memory_length 2000, epsilon 0.15190499294094123 total_time 721.0\n",
      "episode 3780, avg_reward 852.7, memory_length 2000, epsilon 0.15114736362791287 total_time 727.0\n",
      "episode 3790, avg_reward 997.9, memory_length 2000, epsilon 0.15039351300684742 total_time 726.0\n",
      "episode 3800, avg_reward 1009.6, memory_length 2000, epsilon 0.14964342223144017 total_time 725.0\n",
      "Saving model weights for episode:3800\n",
      "episode 3810, avg_reward 674.7, memory_length 2000, epsilon 0.14889707254938264 total_time 726.0\n",
      "episode 3820, avg_reward 787.0, memory_length 2000, epsilon 0.1481544453018939 total_time 720.0\n",
      "episode 3830, avg_reward 769.8, memory_length 2000, epsilon 0.14741552192325408 total_time 724.0\n",
      "episode 3840, avg_reward 746.8, memory_length 2000, epsilon 0.14668028394034027 total_time 723.0\n",
      "episode 3850, avg_reward 808.4, memory_length 2000, epsilon 0.14594871297216455 total_time 723.0\n",
      "episode 3860, avg_reward 747.9, memory_length 2000, epsilon 0.14522079072941466 total_time 727.0\n",
      "episode 3870, avg_reward 932.7, memory_length 2000, epsilon 0.14449649901399655 total_time 725.0\n",
      "episode 3880, avg_reward 884.6, memory_length 2000, epsilon 0.1437758197185797 total_time 725.0\n",
      "episode 3890, avg_reward 731.8, memory_length 2000, epsilon 0.14305873482614412 total_time 724.0\n",
      "episode 3900, avg_reward 736.1, memory_length 2000, epsilon 0.14234522640953018 total_time 720.0\n",
      "Saving model weights for episode:3900\n",
      "episode 3910, avg_reward 873.1, memory_length 2000, epsilon 0.1416352766309903 total_time 725.0\n",
      "episode 3920, avg_reward 780.5, memory_length 2000, epsilon 0.14092886774174304 total_time 722.0\n",
      "episode 3930, avg_reward 896.2, memory_length 2000, epsilon 0.14022598208152937 total_time 728.0\n",
      "episode 3940, avg_reward 818.6, memory_length 2000, epsilon 0.1395266020781712 total_time 728.0\n",
      "episode 3950, avg_reward 845.2, memory_length 2000, epsilon 0.13883071024713198 total_time 731.0\n",
      "episode 3960, avg_reward 843.9, memory_length 2000, epsilon 0.1381382891910797 total_time 724.0\n",
      "episode 3970, avg_reward 893.4, memory_length 2000, epsilon 0.13744932159945195 total_time 722.0\n",
      "episode 3980, avg_reward 969.5, memory_length 2000, epsilon 0.13676379024802296 total_time 725.0\n",
      "episode 3990, avg_reward 963.9, memory_length 2000, epsilon 0.13608167799847334 total_time 724.0\n",
      "episode 4000, avg_reward 970.9, memory_length 2000, epsilon 0.13540296779796124 total_time 723.0\n",
      "Saving model weights for episode:4000\n",
      "episode 4010, avg_reward 1028.3, memory_length 2000, epsilon 0.13472764267869633 total_time 721.0\n",
      "episode 4020, avg_reward 959.3, memory_length 2000, epsilon 0.13405568575751547 total_time 723.0\n",
      "episode 4030, avg_reward 900.7, memory_length 2000, epsilon 0.13338708023546064 total_time 720.0\n",
      "episode 4040, avg_reward 980.8, memory_length 2000, epsilon 0.13272180939735895 total_time 726.0\n",
      "episode 4050, avg_reward 1033.2, memory_length 2000, epsilon 0.1320598566114047 total_time 720.0\n",
      "episode 4060, avg_reward 903.9, memory_length 2000, epsilon 0.13140120532874397 total_time 721.0\n",
      "episode 4070, avg_reward 946.0, memory_length 2000, epsilon 0.13074583908306023 total_time 724.0\n",
      "episode 4080, avg_reward 1028.2, memory_length 2000, epsilon 0.13009374149016328 total_time 722.0\n",
      "episode 4090, avg_reward 1011.8, memory_length 2000, epsilon 0.12944489624757924 total_time 726.0\n",
      "episode 4100, avg_reward 918.1, memory_length 2000, epsilon 0.12879928713414338 total_time 721.0\n",
      "Saving model weights for episode:4100\n",
      "episode 4110, avg_reward 984.0, memory_length 2000, epsilon 0.12815689800959415 total_time 720.0\n",
      "episode 4120, avg_reward 972.9, memory_length 2000, epsilon 0.12751771281417004 total_time 728.0\n",
      "episode 4130, avg_reward 935.7, memory_length 2000, epsilon 0.12688171556820776 total_time 722.0\n",
      "episode 4140, avg_reward 870.5, memory_length 2000, epsilon 0.12624889037174317 total_time 720.0\n",
      "episode 4150, avg_reward 934.2, memory_length 2000, epsilon 0.12561922140411333 total_time 721.0\n",
      "episode 4160, avg_reward 958.7, memory_length 2000, epsilon 0.12499269292356129 total_time 734.0\n",
      "episode 4170, avg_reward 932.8, memory_length 2000, epsilon 0.12436928926684232 total_time 723.0\n",
      "episode 4180, avg_reward 897.1, memory_length 2000, epsilon 0.12374899484883264 total_time 725.0\n",
      "episode 4190, avg_reward 1083.9, memory_length 2000, epsilon 0.12313179416213946 total_time 722.0\n",
      "episode 4200, avg_reward 941.3, memory_length 2000, epsilon 0.12251767177671344 total_time 721.0\n",
      "Saving model weights for episode:4200\n",
      "episode 4210, avg_reward 1022.5, memory_length 2000, epsilon 0.12190661233946291 total_time 726.0\n",
      "episode 4220, avg_reward 1033.0, memory_length 2000, epsilon 0.12129860057387025 total_time 727.0\n",
      "episode 4230, avg_reward 802.4, memory_length 2000, epsilon 0.12069362127960957 total_time 725.0\n",
      "episode 4240, avg_reward 863.4, memory_length 2000, epsilon 0.120091659332167 total_time 720.0\n",
      "episode 4250, avg_reward 1015.0, memory_length 2000, epsilon 0.11949269968246252 total_time 725.0\n",
      "episode 4260, avg_reward 995.7, memory_length 2000, epsilon 0.11889672735647365 total_time 723.0\n",
      "episode 4270, avg_reward 992.1, memory_length 2000, epsilon 0.11830372745486127 total_time 723.0\n",
      "episode 4280, avg_reward 858.0, memory_length 2000, epsilon 0.11771368515259692 total_time 721.0\n",
      "episode 4290, avg_reward 900.7, memory_length 2000, epsilon 0.11712658569859231 total_time 720.0\n",
      "episode 4300, avg_reward 932.5, memory_length 2000, epsilon 0.11654241441533046 total_time 727.0\n",
      "Saving model weights for episode:4300\n",
      "episode 4310, avg_reward 1094.8, memory_length 2000, epsilon 0.11596115669849898 total_time 720.0\n",
      "episode 4320, avg_reward 908.2, memory_length 2000, epsilon 0.1153827980166246 total_time 721.0\n",
      "episode 4330, avg_reward 1133.4, memory_length 2000, epsilon 0.11480732391071016 total_time 722.0\n",
      "episode 4340, avg_reward 983.9, memory_length 2000, epsilon 0.114234719993873 total_time 724.0\n",
      "episode 4350, avg_reward 991.7, memory_length 2000, epsilon 0.11366497195098545 total_time 728.0\n",
      "episode 4360, avg_reward 947.9, memory_length 2000, epsilon 0.11309806553831675 total_time 730.0\n",
      "episode 4370, avg_reward 894.0, memory_length 2000, epsilon 0.11253398658317702 total_time 723.0\n",
      "episode 4380, avg_reward 918.2, memory_length 2000, epsilon 0.11197272098356298 total_time 726.0\n",
      "episode 4390, avg_reward 857.4, memory_length 2000, epsilon 0.1114142547078055 total_time 734.0\n",
      "episode 4400, avg_reward 974.7, memory_length 2000, epsilon 0.11085857379421853 total_time 720.0\n",
      "Saving model weights for episode:4400\n",
      "episode 4410, avg_reward 959.9, memory_length 2000, epsilon 0.11030566435075032 total_time 720.0\n",
      "episode 4420, avg_reward 1014.9, memory_length 2000, epsilon 0.1097555125546359 total_time 723.0\n",
      "episode 4430, avg_reward 1055.5, memory_length 2000, epsilon 0.10920810465205186 total_time 725.0\n",
      "episode 4440, avg_reward 922.4, memory_length 2000, epsilon 0.10866342695777204 total_time 723.0\n",
      "episode 4450, avg_reward 1043.6, memory_length 2000, epsilon 0.10812146585482571 total_time 720.0\n",
      "episode 4460, avg_reward 1048.8, memory_length 2000, epsilon 0.10758220779415704 total_time 724.0\n",
      "episode 4470, avg_reward 988.8, memory_length 2000, epsilon 0.10704563929428652 total_time 721.0\n",
      "episode 4480, avg_reward 977.4, memory_length 2000, epsilon 0.10651174694097365 total_time 728.0\n",
      "episode 4490, avg_reward 908.6, memory_length 2000, epsilon 0.1059805173868818 total_time 724.0\n",
      "episode 4500, avg_reward 1025.1, memory_length 2000, epsilon 0.10545193735124445 total_time 720.0\n",
      "Saving model weights for episode:4500\n",
      "episode 4510, avg_reward 954.7, memory_length 2000, epsilon 0.10492599361953311 total_time 720.0\n",
      "episode 4520, avg_reward 856.4, memory_length 2000, epsilon 0.10440267304312723 total_time 724.0\n",
      "episode 4530, avg_reward 1113.9, memory_length 2000, epsilon 0.10388196253898506 total_time 722.0\n",
      "episode 4540, avg_reward 972.0, memory_length 2000, epsilon 0.10336384908931688 total_time 723.0\n",
      "episode 4550, avg_reward 847.2, memory_length 2000, epsilon 0.10284831974125944 total_time 723.0\n",
      "episode 4560, avg_reward 857.1, memory_length 2000, epsilon 0.10233536160655225 total_time 720.0\n",
      "episode 4570, avg_reward 939.5, memory_length 2000, epsilon 0.1018249618612152 total_time 726.0\n",
      "episode 4580, avg_reward 990.1, memory_length 2000, epsilon 0.10131710774522808 total_time 727.0\n",
      "episode 4590, avg_reward 1003.8, memory_length 2000, epsilon 0.10081178656221147 total_time 720.0\n",
      "episode 4600, avg_reward 826.1, memory_length 2000, epsilon 0.10030898567910958 total_time 724.0\n",
      "Saving model weights for episode:4600\n",
      "episode 4610, avg_reward 931.0, memory_length 2000, epsilon 0.0998086925258741 total_time 723.0\n",
      "episode 4620, avg_reward 1006.2, memory_length 2000, epsilon 0.09931089459515013 total_time 720.0\n",
      "episode 4630, avg_reward 910.8, memory_length 2000, epsilon 0.09881557944196347 total_time 720.0\n",
      "episode 4640, avg_reward 1111.1, memory_length 2000, epsilon 0.09832273468340952 total_time 722.0\n",
      "episode 4650, avg_reward 943.1, memory_length 2000, epsilon 0.09783234799834366 total_time 722.0\n",
      "episode 4660, avg_reward 954.3, memory_length 2000, epsilon 0.09734440712707318 total_time 720.0\n",
      "episode 4670, avg_reward 1134.6, memory_length 2000, epsilon 0.09685889987105087 total_time 728.0\n",
      "episode 4680, avg_reward 1040.0, memory_length 2000, epsilon 0.09637581409257011 total_time 725.0\n",
      "episode 4690, avg_reward 1078.1, memory_length 2000, epsilon 0.09589513771446125 total_time 727.0\n",
      "episode 4700, avg_reward 1042.6, memory_length 2000, epsilon 0.09541685871978979 total_time 728.0\n",
      "Saving model weights for episode:4700\n",
      "episode 4710, avg_reward 926.7, memory_length 2000, epsilon 0.09494096515155591 total_time 731.0\n",
      "episode 4720, avg_reward 1008.5, memory_length 2000, epsilon 0.09446744511239571 total_time 727.0\n",
      "episode 4730, avg_reward 826.0, memory_length 2000, epsilon 0.09399628676428351 total_time 721.0\n",
      "episode 4740, avg_reward 1018.3, memory_length 2000, epsilon 0.09352747832823605 total_time 729.0\n",
      "episode 4750, avg_reward 935.1, memory_length 2000, epsilon 0.09306100808401803 total_time 727.0\n",
      "episode 4760, avg_reward 809.7, memory_length 2000, epsilon 0.092596864369849 total_time 726.0\n",
      "episode 4770, avg_reward 945.9, memory_length 2000, epsilon 0.09213503558211202 total_time 723.0\n",
      "episode 4780, avg_reward 1028.6, memory_length 2000, epsilon 0.09167551017506329 total_time 722.0\n",
      "episode 4790, avg_reward 1016.3, memory_length 2000, epsilon 0.0912182766605437 total_time 724.0\n",
      "episode 4800, avg_reward 855.9, memory_length 2000, epsilon 0.09076332360769154 total_time 723.0\n",
      "Saving model weights for episode:4800\n",
      "episode 4810, avg_reward 917.1, memory_length 2000, epsilon 0.09031063964265688 total_time 732.0\n",
      "episode 4820, avg_reward 916.5, memory_length 2000, epsilon 0.08986021344831698 total_time 720.0\n",
      "episode 4830, avg_reward 1008.8, memory_length 2000, epsilon 0.0894120337639935 total_time 725.0\n",
      "episode 4840, avg_reward 937.1, memory_length 2000, epsilon 0.08896608938517096 total_time 721.0\n",
      "episode 4850, avg_reward 1051.1, memory_length 2000, epsilon 0.08852236916321674 total_time 721.0\n",
      "episode 4860, avg_reward 980.0, memory_length 2000, epsilon 0.08808086200510215 total_time 721.0\n",
      "episode 4870, avg_reward 1007.7, memory_length 2000, epsilon 0.08764155687312523 total_time 725.0\n",
      "episode 4880, avg_reward 1069.0, memory_length 2000, epsilon 0.08720444278463478 total_time 727.0\n",
      "episode 4890, avg_reward 1095.7, memory_length 2000, epsilon 0.08676950881175588 total_time 720.0\n",
      "episode 4900, avg_reward 1007.1, memory_length 2000, epsilon 0.08633674408111651 total_time 720.0\n",
      "Saving model weights for episode:4900\n",
      "episode 4910, avg_reward 962.2, memory_length 2000, epsilon 0.08590613777357589 total_time 722.0\n",
      "episode 4920, avg_reward 1028.6, memory_length 2000, epsilon 0.08547767912395386 total_time 722.0\n",
      "episode 4930, avg_reward 1104.9, memory_length 2000, epsilon 0.08505135742076192 total_time 724.0\n",
      "episode 4940, avg_reward 966.4, memory_length 2000, epsilon 0.08462716200593527 total_time 724.0\n",
      "episode 4950, avg_reward 1004.8, memory_length 2000, epsilon 0.08420508227456643 total_time 733.0\n",
      "episode 4960, avg_reward 1027.9, memory_length 2000, epsilon 0.0837851076746401 total_time 722.0\n",
      "episode 4970, avg_reward 1046.8, memory_length 2000, epsilon 0.08336722770676949 total_time 722.0\n",
      "episode 4980, avg_reward 1038.5, memory_length 2000, epsilon 0.08295143192393359 total_time 728.0\n",
      "episode 4990, avg_reward 935.5, memory_length 2000, epsilon 0.08253770993121619 total_time 728.0\n",
      "episode 5000, avg_reward 1043.1, memory_length 2000, epsilon 0.0821260513855459 total_time 722.0\n",
      "Saving model weights for episode:5000\n",
      "episode 5010, avg_reward 1060.8, memory_length 2000, epsilon 0.08171644599543762 total_time 728.0\n",
      "episode 5020, avg_reward 1081.3, memory_length 2000, epsilon 0.08130888352073534 total_time 722.0\n",
      "episode 5030, avg_reward 1029.2, memory_length 2000, epsilon 0.08090335377235591 total_time 721.0\n",
      "episode 5040, avg_reward 1112.8, memory_length 2000, epsilon 0.0804998466120345 total_time 724.0\n",
      "episode 5050, avg_reward 989.7, memory_length 2000, epsilon 0.08009835195207107 total_time 722.0\n",
      "episode 5060, avg_reward 1009.9, memory_length 2000, epsilon 0.07969885975507827 total_time 726.0\n",
      "episode 5070, avg_reward 972.7, memory_length 2000, epsilon 0.07930136003373034 total_time 724.0\n",
      "episode 5080, avg_reward 929.9, memory_length 2000, epsilon 0.07890584285051352 total_time 724.0\n",
      "episode 5090, avg_reward 1045.2, memory_length 2000, epsilon 0.07851229831747762 total_time 723.0\n",
      "episode 5100, avg_reward 1099.1, memory_length 2000, epsilon 0.07812071659598888 total_time 721.0\n",
      "Saving model weights for episode:5100\n",
      "episode 5110, avg_reward 958.3, memory_length 2000, epsilon 0.07773108789648382 total_time 723.0\n",
      "episode 5120, avg_reward 1213.0, memory_length 2000, epsilon 0.07734340247822467 total_time 723.0\n",
      "episode 5130, avg_reward 1089.4, memory_length 2000, epsilon 0.07695765064905576 total_time 725.0\n",
      "episode 5140, avg_reward 1117.1, memory_length 2000, epsilon 0.07657382276516132 total_time 730.0\n",
      "episode 5150, avg_reward 1021.4, memory_length 2000, epsilon 0.07619190923082422 total_time 725.0\n",
      "episode 5160, avg_reward 1083.1, memory_length 2000, epsilon 0.07581190049818624 total_time 725.0\n",
      "episode 5170, avg_reward 891.2, memory_length 2000, epsilon 0.0754337870670092 total_time 726.0\n",
      "episode 5180, avg_reward 1005.2, memory_length 2000, epsilon 0.07505755948443774 total_time 726.0\n",
      "episode 5190, avg_reward 920.2, memory_length 2000, epsilon 0.07468320834476262 total_time 721.0\n",
      "episode 5200, avg_reward 1018.6, memory_length 2000, epsilon 0.07431072428918589 total_time 722.0\n",
      "Saving model weights for episode:5200\n",
      "episode 5210, avg_reward 867.1, memory_length 2000, epsilon 0.07394009800558671 total_time 724.0\n",
      "episode 5220, avg_reward 934.0, memory_length 2000, epsilon 0.07357132022828873 total_time 726.0\n",
      "episode 5230, avg_reward 901.0, memory_length 2000, epsilon 0.07320438173782833 total_time 728.0\n",
      "episode 5240, avg_reward 1101.7, memory_length 2000, epsilon 0.07283927336072409 total_time 720.0\n",
      "episode 5250, avg_reward 988.2, memory_length 2000, epsilon 0.07247598596924758 total_time 723.0\n",
      "episode 5260, avg_reward 1000.5, memory_length 2000, epsilon 0.07211451048119506 total_time 725.0\n",
      "episode 5270, avg_reward 1048.8, memory_length 2000, epsilon 0.07175483785966058 total_time 722.0\n",
      "episode 5280, avg_reward 950.5, memory_length 2000, epsilon 0.07139695911280983 total_time 724.0\n",
      "episode 5290, avg_reward 935.0, memory_length 2000, epsilon 0.07104086529365548 total_time 727.0\n",
      "episode 5300, avg_reward 899.9, memory_length 2000, epsilon 0.07068654749983351 total_time 723.0\n",
      "Saving model weights for episode:5300\n",
      "episode 5310, avg_reward 830.8, memory_length 2000, epsilon 0.07033399687338064 total_time 727.0\n",
      "episode 5320, avg_reward 1059.1, memory_length 2000, epsilon 0.06998320460051284 total_time 725.0\n",
      "episode 5330, avg_reward 954.7, memory_length 2000, epsilon 0.06963416191140502 total_time 720.0\n",
      "episode 5340, avg_reward 789.5, memory_length 2000, epsilon 0.06928686007997174 total_time 720.0\n",
      "episode 5350, avg_reward 1035.8, memory_length 2000, epsilon 0.06894129042364917 total_time 724.0\n",
      "episode 5360, avg_reward 941.5, memory_length 2000, epsilon 0.0685974443031779 total_time 723.0\n",
      "episode 5370, avg_reward 1045.8, memory_length 2000, epsilon 0.068255313122387 total_time 725.0\n",
      "episode 5380, avg_reward 1101.0, memory_length 2000, epsilon 0.06791488832797908 total_time 721.0\n",
      "episode 5390, avg_reward 1037.9, memory_length 2000, epsilon 0.06757616140931665 total_time 725.0\n",
      "episode 5400, avg_reward 1160.7, memory_length 2000, epsilon 0.06723912389820902 total_time 723.0\n",
      "Saving model weights for episode:5400\n",
      "episode 5410, avg_reward 1121.4, memory_length 2000, epsilon 0.0669037673687009 total_time 721.0\n",
      "episode 5420, avg_reward 1101.7, memory_length 2000, epsilon 0.06657008343686154 total_time 720.0\n",
      "episode 5430, avg_reward 1046.8, memory_length 2000, epsilon 0.06623806376057532 total_time 723.0\n",
      "episode 5440, avg_reward 1045.2, memory_length 2000, epsilon 0.06590770003933302 total_time 721.0\n",
      "episode 5450, avg_reward 999.7, memory_length 2000, epsilon 0.06557898401402441 total_time 720.0\n",
      "episode 5460, avg_reward 883.7, memory_length 2000, epsilon 0.06525190746673168 total_time 730.0\n",
      "episode 5470, avg_reward 950.7, memory_length 2000, epsilon 0.0649264622205242 total_time 720.0\n",
      "episode 5480, avg_reward 1013.8, memory_length 2000, epsilon 0.06460264013925382 total_time 722.0\n",
      "episode 5490, avg_reward 973.3, memory_length 2000, epsilon 0.06428043312735164 total_time 724.0\n",
      "episode 5500, avg_reward 846.8, memory_length 2000, epsilon 0.06395983312962555 total_time 720.0\n",
      "Saving model weights for episode:5500\n",
      "episode 5510, avg_reward 957.3, memory_length 2000, epsilon 0.06364083213105899 total_time 722.0\n",
      "episode 5520, avg_reward 1071.3, memory_length 2000, epsilon 0.06332342215661033 total_time 726.0\n",
      "episode 5530, avg_reward 955.1, memory_length 2000, epsilon 0.06300759527101368 total_time 727.0\n",
      "episode 5540, avg_reward 976.2, memory_length 2000, epsilon 0.06269334357858045 total_time 720.0\n",
      "episode 5550, avg_reward 1106.9, memory_length 2000, epsilon 0.06238065922300194 total_time 723.0\n",
      "episode 5560, avg_reward 973.4, memory_length 2000, epsilon 0.062069534387153034 total_time 721.0\n",
      "episode 5570, avg_reward 1174.0, memory_length 2000, epsilon 0.0617599612928966 total_time 729.0\n",
      "episode 5580, avg_reward 1083.8, memory_length 2000, epsilon 0.061451932200889146 total_time 724.0\n",
      "episode 5590, avg_reward 1195.9, memory_length 2000, epsilon 0.06114543941038732 total_time 726.0\n",
      "episode 5600, avg_reward 992.2, memory_length 2000, epsilon 0.06084047525905543 total_time 720.0\n",
      "Saving model weights for episode:5600\n",
      "episode 5610, avg_reward 1066.6, memory_length 2000, epsilon 0.0605370321227738 total_time 726.0\n",
      "episode 5620, avg_reward 1138.8, memory_length 2000, epsilon 0.060235102415448216 total_time 723.0\n",
      "episode 5630, avg_reward 1208.8, memory_length 2000, epsilon 0.059934678588820234 total_time 723.0\n",
      "episode 5640, avg_reward 1134.4, memory_length 2000, epsilon 0.059635753132278604 total_time 730.0\n",
      "episode 5650, avg_reward 1292.4, memory_length 2000, epsilon 0.05933831857267131 total_time 728.0\n",
      "episode 5660, avg_reward 1096.8, memory_length 2000, epsilon 0.05904236747411887 total_time 721.0\n",
      "episode 5670, avg_reward 1049.9, memory_length 2000, epsilon 0.05874789243782839 total_time 730.0\n",
      "episode 5680, avg_reward 1039.2, memory_length 2000, epsilon 0.05845488610190866 total_time 724.0\n",
      "episode 5690, avg_reward 1222.9, memory_length 2000, epsilon 0.058163341141186015 total_time 721.0\n",
      "episode 5700, avg_reward 1157.2, memory_length 2000, epsilon 0.05787325026702124 total_time 722.0\n",
      "Saving model weights for episode:5700\n",
      "episode 5710, avg_reward 1041.0, memory_length 2000, epsilon 0.05758460622712735 total_time 724.0\n",
      "episode 5720, avg_reward 1085.5, memory_length 2000, epsilon 0.05729740180538836 total_time 727.0\n",
      "episode 5730, avg_reward 1032.1, memory_length 2000, epsilon 0.05701162982167875 total_time 724.0\n",
      "episode 5740, avg_reward 1140.9, memory_length 2000, epsilon 0.056727283131684035 total_time 724.0\n",
      "episode 5750, avg_reward 1008.6, memory_length 2000, epsilon 0.05644435462672214 total_time 722.0\n",
      "episode 5760, avg_reward 1053.7, memory_length 2000, epsilon 0.05616283723356575 total_time 727.0\n",
      "episode 5770, avg_reward 1124.2, memory_length 2000, epsilon 0.05588272391426535 total_time 724.0\n",
      "episode 5780, avg_reward 1102.5, memory_length 2000, epsilon 0.05560400766597337 total_time 720.0\n",
      "episode 5790, avg_reward 1019.2, memory_length 2000, epsilon 0.05532668152076908 total_time 721.0\n",
      "episode 5800, avg_reward 1022.9, memory_length 2000, epsilon 0.05505073854548439 total_time 728.0\n",
      "Saving model weights for episode:5800\n",
      "episode 5810, avg_reward 997.5, memory_length 2000, epsilon 0.05477617184153059 total_time 723.0\n",
      "episode 5820, avg_reward 1143.8, memory_length 2000, epsilon 0.054502974544725746 total_time 720.0\n",
      "episode 5830, avg_reward 977.4, memory_length 2000, epsilon 0.05423113982512323 total_time 723.0\n",
      "episode 5840, avg_reward 963.8, memory_length 2000, epsilon 0.05396066088684085 total_time 720.0\n",
      "episode 5850, avg_reward 1104.8, memory_length 2000, epsilon 0.05369153096789113 total_time 722.0\n",
      "episode 5860, avg_reward 982.8, memory_length 2000, epsilon 0.053423743340012035 total_time 720.0\n",
      "episode 5870, avg_reward 1051.8, memory_length 2000, epsilon 0.05315729130849893 total_time 722.0\n",
      "episode 5880, avg_reward 987.6, memory_length 2000, epsilon 0.05289216821203713 total_time 731.0\n",
      "episode 5890, avg_reward 941.2, memory_length 2000, epsilon 0.052628367422535446 total_time 723.0\n",
      "episode 5900, avg_reward 977.6, memory_length 2000, epsilon 0.052365882344960396 total_time 723.0\n",
      "Saving model weights for episode:5900\n",
      "episode 5910, avg_reward 1159.4, memory_length 2000, epsilon 0.05210470641717137 total_time 722.0\n",
      "episode 5920, avg_reward 1052.8, memory_length 2000, epsilon 0.05184483310975652 total_time 721.0\n",
      "episode 5930, avg_reward 1138.4, memory_length 2000, epsilon 0.051586255925869705 total_time 720.0\n",
      "episode 5940, avg_reward 1181.8, memory_length 2000, epsilon 0.05132896840106782 total_time 721.0\n",
      "episode 5950, avg_reward 954.3, memory_length 2000, epsilon 0.05107296410314935 total_time 723.0\n",
      "episode 5960, avg_reward 952.4, memory_length 2000, epsilon 0.05081823663199349 total_time 733.0\n",
      "episode 5970, avg_reward 1036.6, memory_length 2000, epsilon 0.05056477961940023 total_time 724.0\n",
      "episode 5980, avg_reward 1021.8, memory_length 2000, epsilon 0.05031258672893105 total_time 730.0\n",
      "episode 5990, avg_reward 1092.8, memory_length 2000, epsilon 0.05006165165575053 total_time 721.0\n",
      "episode 6000, avg_reward 1111.6, memory_length 2000, epsilon 0.049811968126468764 total_time 734.0\n",
      "Saving model weights for episode:6000\n",
      "episode 6010, avg_reward 1144.3, memory_length 2000, epsilon 0.04956352989898456 total_time 727.0\n",
      "episode 6020, avg_reward 964.9, memory_length 2000, epsilon 0.049316330762329275 total_time 723.0\n",
      "episode 6030, avg_reward 962.7, memory_length 2000, epsilon 0.0490703645365116 total_time 720.0\n",
      "episode 6040, avg_reward 859.6, memory_length 2000, epsilon 0.048825625072363085 total_time 720.0\n",
      "episode 6050, avg_reward 1133.7, memory_length 2000, epsilon 0.04858210625138437 total_time 723.0\n",
      "episode 6060, avg_reward 1069.1, memory_length 2000, epsilon 0.048339801985592276 total_time 723.0\n",
      "episode 6070, avg_reward 1082.5, memory_length 2000, epsilon 0.048098706217367525 total_time 721.0\n",
      "episode 6080, avg_reward 1159.0, memory_length 2000, epsilon 0.04785881291930335 total_time 720.0\n",
      "episode 6090, avg_reward 1150.0, memory_length 2000, epsilon 0.04762011609405478 total_time 725.0\n",
      "episode 6100, avg_reward 1005.8, memory_length 2000, epsilon 0.0473826097741888 total_time 720.0\n",
      "Saving model weights for episode:6100\n",
      "episode 6110, avg_reward 1065.9, memory_length 2000, epsilon 0.04714628802203503 total_time 724.0\n",
      "episode 6120, avg_reward 1194.7, memory_length 2000, epsilon 0.04691114492953734 total_time 720.0\n",
      "episode 6130, avg_reward 1152.7, memory_length 2000, epsilon 0.04667717461810616 total_time 722.0\n",
      "episode 6140, avg_reward 1117.3, memory_length 2000, epsilon 0.04644437123847156 total_time 725.0\n",
      "episode 6150, avg_reward 1137.1, memory_length 2000, epsilon 0.0462127289705369 total_time 720.0\n",
      "episode 6160, avg_reward 1321.4, memory_length 2000, epsilon 0.04598224202323342 total_time 722.0\n",
      "episode 6170, avg_reward 1298.1, memory_length 2000, epsilon 0.04575290463437541 total_time 723.0\n",
      "episode 6180, avg_reward 1096.9, memory_length 2000, epsilon 0.04552471107051625 total_time 723.0\n",
      "episode 6190, avg_reward 1070.0, memory_length 2000, epsilon 0.04529765562680493 total_time 721.0\n",
      "episode 6200, avg_reward 1272.8, memory_length 2000, epsilon 0.04507173262684353 total_time 721.0\n",
      "Saving model weights for episode:6200\n",
      "episode 6210, avg_reward 1042.6, memory_length 2000, epsilon 0.044846936422545274 total_time 728.0\n",
      "episode 6220, avg_reward 1009.9, memory_length 2000, epsilon 0.04462326139399338 total_time 725.0\n",
      "episode 6230, avg_reward 966.7, memory_length 2000, epsilon 0.044400701949300465 total_time 721.0\n",
      "episode 6240, avg_reward 988.9, memory_length 2000, epsilon 0.044179252524468825 total_time 720.0\n",
      "episode 6250, avg_reward 1095.0, memory_length 2000, epsilon 0.04395890758325128 total_time 721.0\n",
      "episode 6260, avg_reward 952.2, memory_length 2000, epsilon 0.043739661617012864 total_time 720.0\n",
      "episode 6270, avg_reward 1013.2, memory_length 2000, epsilon 0.04352150914459298 total_time 722.0\n",
      "episode 6280, avg_reward 1099.4, memory_length 2000, epsilon 0.04330444471216846 total_time 721.0\n",
      "episode 6290, avg_reward 945.9, memory_length 2000, epsilon 0.043088462893117185 total_time 724.0\n",
      "episode 6300, avg_reward 1057.5, memory_length 2000, epsilon 0.04287355828788241 total_time 723.0\n",
      "Saving model weights for episode:6300\n",
      "episode 6310, avg_reward 1130.5, memory_length 2000, epsilon 0.04265972552383786 total_time 725.0\n",
      "episode 6320, avg_reward 1055.8, memory_length 2000, epsilon 0.04244695925515326 total_time 723.0\n",
      "episode 6330, avg_reward 1277.7, memory_length 2000, epsilon 0.04223525416266082 total_time 726.0\n",
      "episode 6340, avg_reward 1019.3, memory_length 2000, epsilon 0.042024604953722185 total_time 721.0\n",
      "episode 6350, avg_reward 1068.4, memory_length 2000, epsilon 0.041815006362096195 total_time 720.0\n",
      "episode 6360, avg_reward 1165.0, memory_length 2000, epsilon 0.04160645314780712 total_time 732.0\n",
      "episode 6370, avg_reward 1100.0, memory_length 2000, epsilon 0.04139894009701375 total_time 721.0\n",
      "episode 6380, avg_reward 868.3, memory_length 2000, epsilon 0.041192462021878984 total_time 725.0\n",
      "episode 6390, avg_reward 924.9, memory_length 2000, epsilon 0.04098701376044022 total_time 723.0\n",
      "episode 6400, avg_reward 1103.4, memory_length 2000, epsilon 0.04078259017648021 total_time 723.0\n",
      "Saving model weights for episode:6400\n",
      "episode 6410, avg_reward 1166.8, memory_length 2000, epsilon 0.04057918615939871 total_time 726.0\n",
      "episode 6420, avg_reward 1121.4, memory_length 2000, epsilon 0.04037679662408468 total_time 726.0\n",
      "episode 6430, avg_reward 1043.1, memory_length 2000, epsilon 0.04017541651078923 total_time 727.0\n",
      "episode 6440, avg_reward 1026.7, memory_length 2000, epsilon 0.039975040784999014 total_time 720.0\n",
      "episode 6450, avg_reward 1162.5, memory_length 2000, epsilon 0.03977566443731047 total_time 722.0\n",
      "episode 6460, avg_reward 994.6, memory_length 2000, epsilon 0.03957728248330448 total_time 721.0\n",
      "episode 6470, avg_reward 1127.3, memory_length 2000, epsilon 0.03937988996342191 total_time 724.0\n",
      "episode 6480, avg_reward 1099.2, memory_length 2000, epsilon 0.03918348194283947 total_time 721.0\n",
      "episode 6490, avg_reward 1070.0, memory_length 2000, epsilon 0.038988053511346404 total_time 728.0\n",
      "episode 6500, avg_reward 1192.5, memory_length 2000, epsilon 0.038793599783221736 total_time 724.0\n",
      "Saving model weights for episode:6500\n",
      "episode 6510, avg_reward 1019.3, memory_length 2000, epsilon 0.03860011589711217 total_time 721.0\n",
      "episode 6520, avg_reward 1085.9, memory_length 2000, epsilon 0.03840759701591045 total_time 721.0\n",
      "episode 6530, avg_reward 1128.4, memory_length 2000, epsilon 0.038216038326634526 total_time 720.0\n",
      "episode 6540, avg_reward 1235.9, memory_length 2000, epsilon 0.03802543504030719 total_time 721.0\n",
      "episode 6550, avg_reward 1163.3, memory_length 2000, epsilon 0.03783578239183634 total_time 726.0\n",
      "episode 6560, avg_reward 1145.4, memory_length 2000, epsilon 0.037647075639895916 total_time 727.0\n",
      "episode 6570, avg_reward 1078.4, memory_length 2000, epsilon 0.03745931006680728 total_time 726.0\n",
      "episode 6580, avg_reward 1073.4, memory_length 2000, epsilon 0.03727248097842131 total_time 731.0\n",
      "episode 6590, avg_reward 1129.6, memory_length 2000, epsilon 0.03708658370400107 total_time 727.0\n",
      "episode 6600, avg_reward 1188.8, memory_length 2000, epsilon 0.03690161359610504 total_time 723.0\n",
      "Saving model weights for episode:6600\n",
      "episode 6610, avg_reward 1103.0, memory_length 2000, epsilon 0.036717566030470876 total_time 724.0\n",
      "episode 6620, avg_reward 1024.5, memory_length 2000, epsilon 0.036534436405899845 total_time 722.0\n",
      "episode 6630, avg_reward 1189.1, memory_length 2000, epsilon 0.03635222014414178 total_time 721.0\n",
      "episode 6640, avg_reward 1130.9, memory_length 2000, epsilon 0.03617091268978068 total_time 725.0\n",
      "episode 6650, avg_reward 1317.1, memory_length 2000, epsilon 0.035990509510120734 total_time 723.0\n",
      "episode 6660, avg_reward 1114.2, memory_length 2000, epsilon 0.035811006095073046 total_time 723.0\n",
      "episode 6670, avg_reward 1142.1, memory_length 2000, epsilon 0.03563239795704288 total_time 721.0\n",
      "episode 6680, avg_reward 1133.0, memory_length 2000, epsilon 0.035454680630817505 total_time 723.0\n",
      "episode 6690, avg_reward 1215.3, memory_length 2000, epsilon 0.03527784967345451 total_time 723.0\n",
      "episode 6700, avg_reward 1237.2, memory_length 2000, epsilon 0.03510190066417073 total_time 722.0\n",
      "Saving model weights for episode:6700\n",
      "episode 6710, avg_reward 1181.1, memory_length 2000, epsilon 0.03492682920423177 total_time 723.0\n",
      "episode 6720, avg_reward 1148.1, memory_length 2000, epsilon 0.03475263091684203 total_time 722.0\n",
      "episode 6730, avg_reward 1233.2, memory_length 2000, epsilon 0.034579301447035256 total_time 721.0\n",
      "episode 6740, avg_reward 1202.6, memory_length 2000, epsilon 0.034406836461565664 total_time 722.0\n",
      "episode 6750, avg_reward 1287.8, memory_length 2000, epsilon 0.034235231648799616 total_time 720.0\n",
      "episode 6760, avg_reward 1263.2, memory_length 2000, epsilon 0.03406448271860789 total_time 720.0\n",
      "episode 6770, avg_reward 1283.1, memory_length 2000, epsilon 0.03389458540225832 total_time 724.0\n",
      "episode 6780, avg_reward 1309.5, memory_length 2000, epsilon 0.033725535452309156 total_time 724.0\n",
      "episode 6790, avg_reward 1153.8, memory_length 2000, epsilon 0.03355732864250284 total_time 722.0\n",
      "episode 6800, avg_reward 1256.8, memory_length 2000, epsilon 0.03338996076766034 total_time 723.0\n",
      "Saving model weights for episode:6800\n",
      "episode 6810, avg_reward 1168.2, memory_length 2000, epsilon 0.03322342764357612 total_time 723.0\n",
      "episode 6820, avg_reward 1231.9, memory_length 2000, epsilon 0.03305772510691338 total_time 720.0\n",
      "episode 6830, avg_reward 1289.8, memory_length 2000, epsilon 0.03289284901510006 total_time 720.0\n",
      "episode 6840, avg_reward 1324.2, memory_length 2000, epsilon 0.03272879524622528 total_time 720.0\n",
      "episode 6850, avg_reward 1301.8, memory_length 2000, epsilon 0.03256555969893629 total_time 726.0\n",
      "episode 6860, avg_reward 1282.4, memory_length 2000, epsilon 0.032403138292335903 total_time 722.0\n",
      "episode 6870, avg_reward 1278.3, memory_length 2000, epsilon 0.03224152696588049 total_time 720.0\n",
      "episode 6880, avg_reward 1078.0, memory_length 2000, epsilon 0.03208072167927846 total_time 727.0\n",
      "episode 6890, avg_reward 1352.8, memory_length 2000, epsilon 0.03192071841238929 total_time 722.0\n",
      "episode 6900, avg_reward 1173.7, memory_length 2000, epsilon 0.031761513165122976 total_time 720.0\n",
      "Saving model weights for episode:6900\n",
      "episode 6910, avg_reward 1146.5, memory_length 2000, epsilon 0.031603101957340035 total_time 723.0\n",
      "episode 6920, avg_reward 1150.4, memory_length 2000, epsilon 0.031445480828752 total_time 724.0\n",
      "episode 6930, avg_reward 1149.8, memory_length 2000, epsilon 0.0312886458388225 total_time 720.0\n",
      "episode 6940, avg_reward 1102.8, memory_length 2000, epsilon 0.03113259306666858 total_time 723.0\n",
      "episode 6950, avg_reward 1166.9, memory_length 2000, epsilon 0.030977318610962822 total_time 720.0\n",
      "episode 6960, avg_reward 1116.3, memory_length 2000, epsilon 0.030822818589835724 total_time 731.0\n",
      "episode 6970, avg_reward 1283.8, memory_length 2000, epsilon 0.030669089140778743 total_time 723.0\n",
      "episode 6980, avg_reward 1220.2, memory_length 2000, epsilon 0.030516126420547632 total_time 732.0\n",
      "episode 6990, avg_reward 1349.3, memory_length 2000, epsilon 0.030363926605066417 total_time 723.0\n",
      "episode 7000, avg_reward 1355.4, memory_length 2000, epsilon 0.03021248588933177 total_time 726.0\n",
      "Saving model weights for episode:7000\n",
      "episode 7010, avg_reward 1168.3, memory_length 2000, epsilon 0.03006180048731794 total_time 722.0\n",
      "episode 7020, avg_reward 1261.7, memory_length 2000, epsilon 0.02991186663188201 total_time 729.0\n",
      "episode 7030, avg_reward 1259.7, memory_length 2000, epsilon 0.02976268057466979 total_time 725.0\n",
      "episode 7040, avg_reward 1051.3, memory_length 2000, epsilon 0.029614238586022065 total_time 722.0\n",
      "episode 7050, avg_reward 1185.1, memory_length 2000, epsilon 0.029466536954881415 total_time 724.0\n",
      "episode 7060, avg_reward 1288.4, memory_length 2000, epsilon 0.02931957198869935 total_time 725.0\n",
      "episode 7070, avg_reward 1130.0, memory_length 2000, epsilon 0.029173340013344068 total_time 720.0\n",
      "episode 7080, avg_reward 1261.6, memory_length 2000, epsilon 0.029027837373008562 total_time 721.0\n",
      "episode 7090, avg_reward 1132.5, memory_length 2000, epsilon 0.028883060430119237 total_time 721.0\n",
      "episode 7100, avg_reward 1284.3, memory_length 2000, epsilon 0.02873900556524501 total_time 725.0\n",
      "Saving model weights for episode:7100\n",
      "episode 7110, avg_reward 1204.2, memory_length 2000, epsilon 0.028595669177006733 total_time 722.0\n",
      "episode 7120, avg_reward 1292.9, memory_length 2000, epsilon 0.02845304768198724 total_time 727.0\n",
      "episode 7130, avg_reward 1223.0, memory_length 2000, epsilon 0.028311137514641718 total_time 722.0\n",
      "episode 7140, avg_reward 995.9, memory_length 2000, epsilon 0.028169935127208612 total_time 721.0\n",
      "episode 7150, avg_reward 1043.2, memory_length 2000, epsilon 0.028029436989620873 total_time 725.0\n",
      "episode 7160, avg_reward 1027.0, memory_length 2000, epsilon 0.02788963958941774 total_time 724.0\n",
      "episode 7170, avg_reward 1077.6, memory_length 2000, epsilon 0.02775053943165692 total_time 723.0\n",
      "episode 7180, avg_reward 1205.4, memory_length 2000, epsilon 0.027612133038827244 total_time 723.0\n",
      "episode 7190, avg_reward 1205.0, memory_length 2000, epsilon 0.027474416950761672 total_time 726.0\n",
      "episode 7200, avg_reward 1243.2, memory_length 2000, epsilon 0.02733738772455083 total_time 720.0\n",
      "Saving model weights for episode:7200\n",
      "episode 7210, avg_reward 1316.1, memory_length 2000, epsilon 0.027201041934456913 total_time 726.0\n",
      "episode 7220, avg_reward 1148.6, memory_length 2000, epsilon 0.027065376171828092 total_time 724.0\n",
      "episode 7230, avg_reward 1235.4, memory_length 2000, epsilon 0.026930387045013222 total_time 725.0\n",
      "episode 7240, avg_reward 1210.1, memory_length 2000, epsilon 0.0267960711792771 total_time 723.0\n",
      "episode 7250, avg_reward 1109.6, memory_length 2000, epsilon 0.026662425216716083 total_time 720.0\n",
      "episode 7260, avg_reward 1093.0, memory_length 2000, epsilon 0.02652944581617416 total_time 725.0\n",
      "episode 7270, avg_reward 1011.6, memory_length 2000, epsilon 0.02639712965315939 total_time 720.0\n",
      "episode 7280, avg_reward 1129.1, memory_length 2000, epsilon 0.026265473419760797 total_time 722.0\n",
      "episode 7290, avg_reward 1260.0, memory_length 2000, epsilon 0.02613447382456568 total_time 721.0\n",
      "episode 7300, avg_reward 1151.0, memory_length 2000, epsilon 0.026004127592577364 total_time 721.0\n",
      "Saving model weights for episode:7300\n",
      "episode 7310, avg_reward 1202.3, memory_length 2000, epsilon 0.02587443146513324 total_time 724.0\n",
      "episode 7320, avg_reward 1240.9, memory_length 2000, epsilon 0.025745382199823377 total_time 724.0\n",
      "episode 7330, avg_reward 1200.6, memory_length 2000, epsilon 0.025616976570409414 total_time 726.0\n",
      "episode 7340, avg_reward 1274.2, memory_length 2000, epsilon 0.02548921136674392 total_time 721.0\n",
      "episode 7350, avg_reward 1314.8, memory_length 2000, epsilon 0.02536208339469017 total_time 721.0\n",
      "episode 7360, avg_reward 1205.3, memory_length 2000, epsilon 0.02523558947604223 total_time 720.0\n",
      "episode 7370, avg_reward 1442.9, memory_length 2000, epsilon 0.025109726448445543 total_time 720.0\n",
      "episode 7380, avg_reward 1204.2, memory_length 2000, epsilon 0.024984491165317858 total_time 726.0\n",
      "episode 7390, avg_reward 1216.9, memory_length 2000, epsilon 0.024859880495770592 total_time 720.0\n",
      "episode 7400, avg_reward 1142.4, memory_length 2000, epsilon 0.024735891324530506 total_time 720.0\n",
      "Saving model weights for episode:7400\n",
      "episode 7410, avg_reward 1187.1, memory_length 2000, epsilon 0.024612520551861864 total_time 720.0\n",
      "episode 7420, avg_reward 1095.3, memory_length 2000, epsilon 0.02448976509348891 total_time 721.0\n",
      "episode 7430, avg_reward 1111.5, memory_length 2000, epsilon 0.024367621880518817 total_time 721.0\n",
      "episode 7440, avg_reward 1081.6, memory_length 2000, epsilon 0.02424608785936488 total_time 724.0\n",
      "episode 7450, avg_reward 1085.0, memory_length 2000, epsilon 0.02412515999167025 total_time 720.0\n",
      "episode 7460, avg_reward 1201.6, memory_length 2000, epsilon 0.024004835254231915 total_time 720.0\n",
      "episode 7470, avg_reward 1239.4, memory_length 2000, epsilon 0.023885110638925205 total_time 721.0\n",
      "episode 7480, avg_reward 1312.0, memory_length 2000, epsilon 0.023765983152628484 total_time 723.0\n",
      "episode 7490, avg_reward 1167.9, memory_length 2000, epsilon 0.023647449817148388 total_time 720.0\n",
      "episode 7500, avg_reward 1271.7, memory_length 2000, epsilon 0.023529507669145352 total_time 726.0\n",
      "Saving model weights for episode:7500\n",
      "episode 7510, avg_reward 1434.3, memory_length 2000, epsilon 0.02341215376005955 total_time 721.0\n",
      "episode 7520, avg_reward 1416.6, memory_length 2000, epsilon 0.023295385156037136 total_time 727.0\n",
      "episode 7530, avg_reward 1383.8, memory_length 2000, epsilon 0.023179198937856922 total_time 723.0\n",
      "episode 7540, avg_reward 1349.8, memory_length 2000, epsilon 0.023063592200857398 total_time 720.0\n",
      "episode 7550, avg_reward 1327.0, memory_length 2000, epsilon 0.022948562054864136 total_time 727.0\n",
      "episode 7560, avg_reward 1366.5, memory_length 2000, epsilon 0.02283410562411748 total_time 728.0\n",
      "episode 7570, avg_reward 1282.1, memory_length 2000, epsilon 0.022720220047200708 total_time 721.0\n",
      "episode 7580, avg_reward 1335.2, memory_length 2000, epsilon 0.02260690247696846 total_time 725.0\n",
      "episode 7590, avg_reward 1231.2, memory_length 2000, epsilon 0.02249415008047557 total_time 720.0\n",
      "episode 7600, avg_reward 1180.2, memory_length 2000, epsilon 0.022381960038906273 total_time 721.0\n",
      "Saving model weights for episode:7600\n",
      "episode 7610, avg_reward 1121.5, memory_length 2000, epsilon 0.02227032954750368 total_time 722.0\n",
      "episode 7620, avg_reward 1127.0, memory_length 2000, epsilon 0.022159255815499684 total_time 720.0\n",
      "episode 7630, avg_reward 1273.0, memory_length 2000, epsilon 0.022048736066045196 total_time 721.0\n",
      "episode 7640, avg_reward 1303.9, memory_length 2000, epsilon 0.021938767536140744 total_time 721.0\n",
      "episode 7650, avg_reward 1218.7, memory_length 2000, epsilon 0.021829347476567337 total_time 724.0\n",
      "episode 7660, avg_reward 1212.0, memory_length 2000, epsilon 0.02172047315181779 total_time 722.0\n",
      "episode 7670, avg_reward 1320.7, memory_length 2000, epsilon 0.02161214184002831 total_time 724.0\n",
      "episode 7680, avg_reward 1275.4, memory_length 2000, epsilon 0.021504350832910468 total_time 720.0\n",
      "episode 7690, avg_reward 1192.4, memory_length 2000, epsilon 0.021397097435683472 total_time 724.0\n",
      "episode 7700, avg_reward 1189.6, memory_length 2000, epsilon 0.021290378967006796 total_time 728.0\n",
      "Saving model weights for episode:7700\n",
      "episode 7710, avg_reward 1201.1, memory_length 2000, epsilon 0.021184192758913166 total_time 720.0\n",
      "episode 7720, avg_reward 1144.3, memory_length 2000, epsilon 0.02107853615674186 total_time 724.0\n",
      "episode 7730, avg_reward 1207.7, memory_length 2000, epsilon 0.020973406519072313 total_time 722.0\n",
      "episode 7740, avg_reward 1198.7, memory_length 2000, epsilon 0.02086880121765811 total_time 721.0\n",
      "episode 7750, avg_reward 1152.1, memory_length 2000, epsilon 0.020764717637361256 total_time 738.0\n",
      "episode 7760, avg_reward 1172.8, memory_length 2000, epsilon 0.020661153176086845 total_time 720.0\n",
      "episode 7770, avg_reward 1118.9, memory_length 2000, epsilon 0.020558105244717934 total_time 723.0\n",
      "episode 7780, avg_reward 1164.9, memory_length 2000, epsilon 0.02045557126705088 total_time 723.0\n",
      "episode 7790, avg_reward 990.0, memory_length 2000, epsilon 0.020353548679730885 total_time 720.0\n",
      "episode 7800, avg_reward 1031.2, memory_length 2000, epsilon 0.020252034932187978 total_time 725.0\n",
      "Saving model weights for episode:7800\n",
      "episode 7810, avg_reward 1159.1, memory_length 2000, epsilon 0.020151027486573166 total_time 724.0\n",
      "episode 7820, avg_reward 1271.2, memory_length 2000, epsilon 0.020050523817695055 total_time 721.0\n",
      "episode 7830, avg_reward 1226.4, memory_length 2000, epsilon 0.019950521412956685 total_time 722.0\n",
      "episode 7840, avg_reward 1188.3, memory_length 2000, epsilon 0.019851017772292718 total_time 724.0\n",
      "episode 7850, avg_reward 1363.6, memory_length 2000, epsilon 0.01975201040810698 total_time 729.0\n",
      "episode 7860, avg_reward 1254.6, memory_length 2000, epsilon 0.01965349684521019 total_time 727.0\n",
      "episode 7870, avg_reward 1404.4, memory_length 2000, epsilon 0.019555474620758153 total_time 724.0\n",
      "episode 7880, avg_reward 1334.5, memory_length 2000, epsilon 0.019457941284190143 total_time 720.0\n",
      "episode 7890, avg_reward 1170.1, memory_length 2000, epsilon 0.01936089439716768 total_time 727.0\n",
      "episode 7900, avg_reward 1333.4, memory_length 2000, epsilon 0.019264331533513528 total_time 723.0\n",
      "Saving model weights for episode:7900\n",
      "episode 7910, avg_reward 1245.7, memory_length 2000, epsilon 0.019168250279151065 total_time 724.0\n",
      "episode 7920, avg_reward 947.5, memory_length 2000, epsilon 0.01907264823204392 total_time 722.0\n",
      "episode 7930, avg_reward 1027.3, memory_length 2000, epsilon 0.018977523002135954 total_time 728.0\n",
      "episode 7940, avg_reward 1296.5, memory_length 2000, epsilon 0.01888287221129145 total_time 720.0\n",
      "episode 7950, avg_reward 1367.6, memory_length 2000, epsilon 0.01878869349323572 total_time 729.0\n",
      "episode 7960, avg_reward 1318.9, memory_length 2000, epsilon 0.018694984493495884 total_time 722.0\n",
      "episode 7970, avg_reward 1401.8, memory_length 2000, epsilon 0.018601742869342092 total_time 721.0\n",
      "episode 7980, avg_reward 1336.1, memory_length 2000, epsilon 0.018508966289728878 total_time 725.0\n",
      "episode 7990, avg_reward 1308.0, memory_length 2000, epsilon 0.018416652435236915 total_time 724.0\n",
      "episode 8000, avg_reward 1108.9, memory_length 2000, epsilon 0.018324798998015028 total_time 722.0\n",
      "Saving model weights for episode:8000\n",
      "episode 8010, avg_reward 1265.6, memory_length 2000, epsilon 0.018233403681722514 total_time 725.0\n",
      "episode 8020, avg_reward 1285.1, memory_length 2000, epsilon 0.0181424642014717 total_time 720.0\n",
      "episode 8030, avg_reward 1294.3, memory_length 2000, epsilon 0.018051978283770847 total_time 721.0\n",
      "episode 8040, avg_reward 1289.3, memory_length 2000, epsilon 0.017961943666467296 total_time 721.0\n",
      "episode 8050, avg_reward 1355.4, memory_length 2000, epsilon 0.017872358098690925 total_time 721.0\n",
      "episode 8060, avg_reward 1248.0, memory_length 2000, epsilon 0.017783219340797858 total_time 725.0\n",
      "episode 8070, avg_reward 1206.6, memory_length 2000, epsilon 0.017694525164314533 total_time 724.0\n",
      "episode 8080, avg_reward 1306.7, memory_length 2000, epsilon 0.01760627335188191 total_time 725.0\n",
      "episode 8090, avg_reward 1358.1, memory_length 2000, epsilon 0.017518461697200078 total_time 720.0\n",
      "episode 8100, avg_reward 1287.0, memory_length 2000, epsilon 0.0174310880049731 total_time 720.0\n",
      "Saving model weights for episode:8100\n",
      "episode 8110, avg_reward 1169.5, memory_length 2000, epsilon 0.01734415009085411 total_time 722.0\n",
      "episode 8120, avg_reward 1189.9, memory_length 2000, epsilon 0.017257645781390735 total_time 727.0\n",
      "episode 8130, avg_reward 1289.0, memory_length 2000, epsilon 0.017171572913970735 total_time 720.0\n",
      "episode 8140, avg_reward 1306.7, memory_length 2000, epsilon 0.017085929336767933 total_time 722.0\n",
      "episode 8150, avg_reward 1349.1, memory_length 2000, epsilon 0.01700071290868843 total_time 730.0\n",
      "episode 8160, avg_reward 1201.5, memory_length 2000, epsilon 0.016915921499317116 total_time 722.0\n",
      "episode 8170, avg_reward 1233.1, memory_length 2000, epsilon 0.01683155298886432 total_time 726.0\n",
      "episode 8180, avg_reward 1440.0, memory_length 2000, epsilon 0.016747605268112892 total_time 725.0\n",
      "episode 8190, avg_reward 1348.0, memory_length 2000, epsilon 0.016664076238365435 total_time 722.0\n",
      "episode 8200, avg_reward 1502.3, memory_length 2000, epsilon 0.01658096381139186 total_time 730.0\n",
      "Saving model weights for episode:8200\n",
      "episode 8210, avg_reward 1482.8, memory_length 2000, epsilon 0.016498265909377165 total_time 720.0\n",
      "episode 8220, avg_reward 1309.5, memory_length 2000, epsilon 0.016415980464869487 total_time 724.0\n",
      "episode 8230, avg_reward 1279.5, memory_length 2000, epsilon 0.01633410542072842 total_time 724.0\n",
      "episode 8240, avg_reward 1205.0, memory_length 2000, epsilon 0.016252638730073616 total_time 723.0\n",
      "episode 8250, avg_reward 1460.3, memory_length 2000, epsilon 0.01617157835623356 total_time 720.0\n",
      "episode 8260, avg_reward 1318.3, memory_length 2000, epsilon 0.016090922272694676 total_time 730.0\n",
      "episode 8270, avg_reward 1293.1, memory_length 2000, epsilon 0.016010668463050683 total_time 720.0\n",
      "episode 8280, avg_reward 1211.4, memory_length 2000, epsilon 0.015930814920952153 total_time 722.0\n",
      "episode 8290, avg_reward 1300.3, memory_length 2000, epsilon 0.01585135965005638 total_time 729.0\n",
      "episode 8300, avg_reward 1314.0, memory_length 2000, epsilon 0.01577230066397745 total_time 727.0\n",
      "Saving model weights for episode:8300\n",
      "episode 8310, avg_reward 1404.6, memory_length 2000, epsilon 0.01569363598623658 total_time 726.0\n",
      "episode 8320, avg_reward 1127.7, memory_length 2000, epsilon 0.015615363650212755 total_time 723.0\n",
      "episode 8330, avg_reward 1216.1, memory_length 2000, epsilon 0.015537481699093489 total_time 724.0\n",
      "episode 8340, avg_reward 1297.2, memory_length 2000, epsilon 0.015459988185825944 total_time 726.0\n",
      "episode 8350, avg_reward 1287.8, memory_length 2000, epsilon 0.015382881173068253 total_time 722.0\n",
      "episode 8360, avg_reward 1221.7, memory_length 2000, epsilon 0.015306158733141081 total_time 723.0\n",
      "episode 8370, avg_reward 1129.6, memory_length 2000, epsilon 0.015229818947979435 total_time 723.0\n",
      "episode 8380, avg_reward 1184.0, memory_length 2000, epsilon 0.015153859909084707 total_time 723.0\n",
      "episode 8390, avg_reward 1331.5, memory_length 2000, epsilon 0.015078279717476972 total_time 720.0\n",
      "episode 8400, avg_reward 1146.5, memory_length 2000, epsilon 0.015003076483647487 total_time 722.0\n",
      "Saving model weights for episode:8400\n",
      "episode 8410, avg_reward 1321.8, memory_length 2000, epsilon 0.01492824832751152 total_time 727.0\n",
      "episode 8420, avg_reward 1181.6, memory_length 2000, epsilon 0.014853793378361251 total_time 723.0\n",
      "episode 8430, avg_reward 1363.3, memory_length 2000, epsilon 0.014779709774819079 total_time 726.0\n",
      "episode 8440, avg_reward 1211.7, memory_length 2000, epsilon 0.014705995664791053 total_time 723.0\n",
      "episode 8450, avg_reward 1315.7, memory_length 2000, epsilon 0.014632649205420586 total_time 732.0\n",
      "episode 8460, avg_reward 1210.4, memory_length 2000, epsilon 0.014559668563042371 total_time 726.0\n",
      "episode 8470, avg_reward 1178.4, memory_length 2000, epsilon 0.01448705191313655 total_time 720.0\n",
      "episode 8480, avg_reward 1250.8, memory_length 2000, epsilon 0.014414797440283079 total_time 725.0\n",
      "episode 8490, avg_reward 1079.7, memory_length 2000, epsilon 0.014342903338116398 total_time 721.0\n",
      "episode 8500, avg_reward 1167.0, memory_length 2000, epsilon 0.014271367809280198 total_time 734.0\n",
      "Saving model weights for episode:8500\n",
      "episode 8510, avg_reward 1203.9, memory_length 2000, epsilon 0.014200189065382531 total_time 727.0\n",
      "episode 8520, avg_reward 1279.2, memory_length 2000, epsilon 0.014129365326951093 total_time 720.0\n",
      "episode 8530, avg_reward 1242.6, memory_length 2000, epsilon 0.014058894823388731 total_time 724.0\n",
      "episode 8540, avg_reward 1201.2, memory_length 2000, epsilon 0.013988775792929191 total_time 725.0\n",
      "episode 8550, avg_reward 1148.4, memory_length 2000, epsilon 0.013919006482593057 total_time 728.0\n",
      "episode 8560, avg_reward 1311.7, memory_length 2000, epsilon 0.013849585148143924 total_time 721.0\n",
      "episode 8570, avg_reward 1302.4, memory_length 2000, epsilon 0.013780510054044839 total_time 722.0\n",
      "episode 8580, avg_reward 1164.2, memory_length 2000, epsilon 0.01371177947341484 total_time 720.0\n",
      "episode 8590, avg_reward 1310.5, memory_length 2000, epsilon 0.013643391687985834 total_time 720.0\n",
      "episode 8600, avg_reward 1230.6, memory_length 2000, epsilon 0.013575344988059622 total_time 720.0\n",
      "Saving model weights for episode:8600\n",
      "episode 8610, avg_reward 1191.0, memory_length 2000, epsilon 0.01350763767246516 total_time 724.0\n",
      "episode 8620, avg_reward 1206.6, memory_length 2000, epsilon 0.013440268048516033 total_time 726.0\n",
      "episode 8630, avg_reward 1256.7, memory_length 2000, epsilon 0.013373234431968133 total_time 725.0\n",
      "episode 8640, avg_reward 1104.7, memory_length 2000, epsilon 0.013306535146977556 total_time 731.0\n",
      "episode 8650, avg_reward 1312.7, memory_length 2000, epsilon 0.013240168526058691 total_time 722.0\n",
      "episode 8660, avg_reward 1375.8, memory_length 2000, epsilon 0.013174132910042583 total_time 724.0\n",
      "episode 8670, avg_reward 1293.0, memory_length 2000, epsilon 0.013108426648035377 total_time 720.0\n",
      "episode 8680, avg_reward 1286.4, memory_length 2000, epsilon 0.013043048097377104 total_time 729.0\n",
      "episode 8690, avg_reward 1330.8, memory_length 2000, epsilon 0.012977995623600592 total_time 727.0\n",
      "episode 8700, avg_reward 1345.9, memory_length 2000, epsilon 0.012913267600390608 total_time 722.0\n",
      "Saving model weights for episode:8700\n",
      "episode 8710, avg_reward 1225.7, memory_length 2000, epsilon 0.0128488624095432 total_time 720.0\n",
      "episode 8720, avg_reward 1310.3, memory_length 2000, epsilon 0.012784778440925243 total_time 731.0\n",
      "episode 8730, avg_reward 1245.2, memory_length 2000, epsilon 0.01272101409243417 total_time 720.0\n",
      "episode 8740, avg_reward 1272.3, memory_length 2000, epsilon 0.012657567769957976 total_time 725.0\n",
      "episode 8750, avg_reward 1244.3, memory_length 2000, epsilon 0.012594437887335278 total_time 724.0\n",
      "episode 8760, avg_reward 1299.6, memory_length 2000, epsilon 0.012531622866315725 total_time 723.0\n",
      "episode 8770, avg_reward 1305.8, memory_length 2000, epsilon 0.012469121136520519 total_time 720.0\n",
      "episode 8780, avg_reward 1255.0, memory_length 2000, epsilon 0.012406931135403159 total_time 729.0\n",
      "episode 8790, avg_reward 1297.1, memory_length 2000, epsilon 0.012345051308210378 total_time 720.0\n",
      "episode 8800, avg_reward 1223.5, memory_length 2000, epsilon 0.012283480107943277 total_time 720.0\n",
      "Saving model weights for episode:8800\n",
      "episode 8810, avg_reward 1270.5, memory_length 2000, epsilon 0.012222215995318626 total_time 723.0\n",
      "episode 8820, avg_reward 1210.4, memory_length 2000, epsilon 0.012161257438730446 total_time 727.0\n",
      "episode 8830, avg_reward 1049.2, memory_length 2000, epsilon 0.012100602914211632 total_time 720.0\n",
      "episode 8840, avg_reward 1311.9, memory_length 2000, epsilon 0.012040250905395913 total_time 720.0\n",
      "episode 8850, avg_reward 1266.6, memory_length 2000, epsilon 0.011980199903479927 total_time 724.0\n",
      "episode 8860, avg_reward 1362.1, memory_length 2000, epsilon 0.011920448407185497 total_time 726.0\n",
      "episode 8870, avg_reward 1276.0, memory_length 2000, epsilon 0.011860994922722103 total_time 724.0\n",
      "episode 8880, avg_reward 1197.2, memory_length 2000, epsilon 0.01180183796374954 total_time 720.0\n",
      "episode 8890, avg_reward 1062.1, memory_length 2000, epsilon 0.011742976051340748 total_time 722.0\n",
      "episode 8900, avg_reward 1410.6, memory_length 2000, epsilon 0.011684407713944843 total_time 720.0\n",
      "Saving model weights for episode:8900\n",
      "episode 8910, avg_reward 1394.1, memory_length 2000, epsilon 0.011626131487350361 total_time 730.0\n",
      "episode 8920, avg_reward 1416.5, memory_length 2000, epsilon 0.011568145914648591 total_time 721.0\n",
      "episode 8930, avg_reward 1322.9, memory_length 2000, epsilon 0.011510449546197196 total_time 724.0\n",
      "episode 8940, avg_reward 1260.9, memory_length 2000, epsilon 0.011453040939583957 total_time 720.0\n",
      "episode 8950, avg_reward 1375.3, memory_length 2000, epsilon 0.011395918659590722 total_time 722.0\n",
      "episode 8960, avg_reward 1259.3, memory_length 2000, epsilon 0.011339081278157515 total_time 723.0\n",
      "episode 8970, avg_reward 1398.4, memory_length 2000, epsilon 0.011282527374346839 total_time 721.0\n",
      "episode 8980, avg_reward 1276.7, memory_length 2000, epsilon 0.011226255534308144 total_time 722.0\n",
      "episode 8990, avg_reward 1216.0, memory_length 2000, epsilon 0.011170264351242519 total_time 721.0\n",
      "episode 9000, avg_reward 1312.7, memory_length 2000, epsilon 0.011114552425367458 total_time 723.0\n",
      "Saving model weights for episode:9000\n",
      "episode 9010, avg_reward 1294.9, memory_length 2000, epsilon 0.011059118363881916 total_time 720.0\n",
      "episode 9020, avg_reward 1192.0, memory_length 2000, epsilon 0.011003960780931467 total_time 722.0\n",
      "episode 9030, avg_reward 1252.2, memory_length 2000, epsilon 0.010949078297573665 total_time 723.0\n",
      "episode 9040, avg_reward 1143.9, memory_length 2000, epsilon 0.010894469541743567 total_time 726.0\n",
      "episode 9050, avg_reward 1278.4, memory_length 2000, epsilon 0.010840133148219433 total_time 720.0\n",
      "episode 9060, avg_reward 1184.5, memory_length 2000, epsilon 0.010786067758588586 total_time 720.0\n",
      "episode 9070, avg_reward 1206.0, memory_length 2000, epsilon 0.010732272021213489 total_time 726.0\n",
      "episode 9080, avg_reward 1234.7, memory_length 2000, epsilon 0.010678744591197893 total_time 727.0\n",
      "episode 9090, avg_reward 1123.6, memory_length 2000, epsilon 0.010625484130353264 total_time 720.0\n",
      "episode 9100, avg_reward 1107.5, memory_length 2000, epsilon 0.010572489307165304 total_time 726.0\n",
      "Saving model weights for episode:9100\n",
      "episode 9110, avg_reward 1212.1, memory_length 2000, epsilon 0.010519758796760675 total_time 724.0\n",
      "episode 9120, avg_reward 1177.2, memory_length 2000, epsilon 0.01046729128087387 total_time 725.0\n",
      "episode 9130, avg_reward 1254.4, memory_length 2000, epsilon 0.010415085447814257 total_time 727.0\n",
      "episode 9140, avg_reward 1114.5, memory_length 2000, epsilon 0.010363139992433292 total_time 720.0\n",
      "episode 9150, avg_reward 1236.4, memory_length 2000, epsilon 0.010311453616091877 total_time 720.0\n",
      "episode 9160, avg_reward 1288.2, memory_length 2000, epsilon 0.010260025026627928 total_time 725.0\n",
      "episode 9170, avg_reward 1273.6, memory_length 2000, epsilon 0.010208852938324024 total_time 722.0\n",
      "episode 9180, avg_reward 1276.1, memory_length 2000, epsilon 0.010157936071875286 total_time 724.0\n",
      "episode 9190, avg_reward 1073.1, memory_length 2000, epsilon 0.010107273154357407 total_time 722.0\n",
      "episode 9200, avg_reward 1222.8, memory_length 2000, epsilon 0.010056862919194807 total_time 725.0\n",
      "Saving model weights for episode:9200\n",
      "episode 9210, avg_reward 1254.0, memory_length 2000, epsilon 0.010006704106128982 total_time 720.0\n",
      "episode 9220, avg_reward 1243.9, memory_length 2000, epsilon 0.009956795461186994 total_time 727.0\n",
      "episode 9230, avg_reward 1113.3, memory_length 2000, epsilon 0.00990713573665011 total_time 721.0\n",
      "episode 9240, avg_reward 1293.3, memory_length 2000, epsilon 0.009857723691022648 total_time 722.0\n",
      "episode 9250, avg_reward 1155.9, memory_length 2000, epsilon 0.009808558089000885 total_time 725.0\n",
      "episode 9260, avg_reward 1328.7, memory_length 2000, epsilon 0.00975963770144221 total_time 724.0\n",
      "episode 9270, avg_reward 1178.9, memory_length 2000, epsilon 0.009710961305334385 total_time 722.0\n",
      "episode 9280, avg_reward 1218.3, memory_length 2000, epsilon 0.009662527683764974 total_time 720.0\n",
      "episode 9290, avg_reward 1190.9, memory_length 2000, epsilon 0.009614335625890914 total_time 723.0\n",
      "episode 9300, avg_reward 1187.5, memory_length 2000, epsilon 0.009566383926908248 total_time 720.0\n",
      "Saving model weights for episode:9300\n",
      "episode 9310, avg_reward 1212.6, memory_length 2000, epsilon 0.009518671388021996 total_time 724.0\n",
      "episode 9320, avg_reward 1121.5, memory_length 2000, epsilon 0.009471196816416219 total_time 722.0\n",
      "episode 9330, avg_reward 1329.1, memory_length 2000, epsilon 0.009423959025224143 total_time 725.0\n",
      "episode 9340, avg_reward 1154.7, memory_length 2000, epsilon 0.00937695683349853 total_time 726.0\n",
      "episode 9350, avg_reward 1247.4, memory_length 2000, epsilon 0.009330189066182135 total_time 725.0\n",
      "episode 9360, avg_reward 1259.9, memory_length 2000, epsilon 0.009283654554078346 total_time 720.0\n",
      "episode 9370, avg_reward 1345.7, memory_length 2000, epsilon 0.009237352133821932 total_time 722.0\n",
      "episode 9380, avg_reward 1244.7, memory_length 2000, epsilon 0.009191280647849975 total_time 720.0\n",
      "episode 9390, avg_reward 1351.3, memory_length 2000, epsilon 0.009145438944372928 total_time 721.0\n",
      "episode 9400, avg_reward 1384.2, memory_length 2000, epsilon 0.009099825877345808 total_time 725.0\n",
      "Saving model weights for episode:9400\n",
      "episode 9410, avg_reward 1250.8, memory_length 2000, epsilon 0.009054440306439577 total_time 722.0\n",
      "episode 9420, avg_reward 1202.9, memory_length 2000, epsilon 0.009009281097012595 total_time 728.0\n",
      "episode 9430, avg_reward 1234.4, memory_length 2000, epsilon 0.008964347120082273 total_time 723.0\n",
      "episode 9440, avg_reward 1251.5, memory_length 2000, epsilon 0.008919637252296844 total_time 725.0\n",
      "episode 9450, avg_reward 1361.5, memory_length 2000, epsilon 0.00887515037590729 total_time 722.0\n",
      "episode 9460, avg_reward 1292.9, memory_length 2000, epsilon 0.008830885378739379 total_time 720.0\n",
      "episode 9470, avg_reward 1145.8, memory_length 2000, epsilon 0.00878684115416588 total_time 720.0\n",
      "episode 9480, avg_reward 1149.3, memory_length 2000, epsilon 0.008743016601078875 total_time 725.0\n",
      "episode 9490, avg_reward 1287.8, memory_length 2000, epsilon 0.008699410623862272 total_time 724.0\n",
      "episode 9500, avg_reward 1305.1, memory_length 2000, epsilon 0.008656022132364358 total_time 722.0\n",
      "Saving model weights for episode:9500\n",
      "episode 9510, avg_reward 1119.6, memory_length 2000, epsilon 0.00861285004187059 total_time 726.0\n",
      "episode 9520, avg_reward 1200.7, memory_length 2000, epsilon 0.008569893273076454 total_time 722.0\n",
      "episode 9530, avg_reward 1284.7, memory_length 2000, epsilon 0.008527150752060494 total_time 722.0\n",
      "episode 9540, avg_reward 1190.7, memory_length 2000, epsilon 0.00848462141025746 total_time 724.0\n",
      "episode 9550, avg_reward 1352.5, memory_length 2000, epsilon 0.008442304184431587 total_time 722.0\n",
      "episode 9560, avg_reward 1190.1, memory_length 2000, epsilon 0.008400198016650023 total_time 720.0\n",
      "episode 9570, avg_reward 1193.2, memory_length 2000, epsilon 0.008358301854256393 total_time 720.0\n",
      "episode 9580, avg_reward 1254.9, memory_length 2000, epsilon 0.008316614649844449 total_time 720.0\n",
      "episode 9590, avg_reward 1307.5, memory_length 2000, epsilon 0.008275135361231907 total_time 723.0\n",
      "episode 9600, avg_reward 1396.5, memory_length 2000, epsilon 0.008233862951434393 total_time 720.0\n",
      "Saving model weights for episode:9600\n",
      "episode 9610, avg_reward 1289.2, memory_length 2000, epsilon 0.008192796388639513 total_time 722.0\n",
      "episode 9620, avg_reward 1339.9, memory_length 2000, epsilon 0.008151934646181058 total_time 725.0\n",
      "episode 9630, avg_reward 1223.2, memory_length 2000, epsilon 0.008111276702513336 total_time 720.0\n",
      "episode 9640, avg_reward 1222.9, memory_length 2000, epsilon 0.00807082154118564 total_time 724.0\n",
      "episode 9650, avg_reward 1250.0, memory_length 2000, epsilon 0.008030568150816823 total_time 722.0\n",
      "episode 9660, avg_reward 1438.4, memory_length 2000, epsilon 0.007990515525070043 total_time 722.0\n",
      "episode 9670, avg_reward 1294.4, memory_length 2000, epsilon 0.007950662662627561 total_time 724.0\n",
      "episode 9680, avg_reward 1420.2, memory_length 2000, epsilon 0.007911008567165744 total_time 722.0\n",
      "episode 9690, avg_reward 1220.5, memory_length 2000, epsilon 0.007871552247330136 total_time 723.0\n",
      "episode 9700, avg_reward 1094.1, memory_length 2000, epsilon 0.00783229271671069 total_time 725.0\n",
      "Saving model weights for episode:9700\n",
      "episode 9710, avg_reward 1194.3, memory_length 2000, epsilon 0.007793228993817094 total_time 723.0\n",
      "episode 9720, avg_reward 1196.8, memory_length 2000, epsilon 0.00775436010205424 total_time 725.0\n",
      "episode 9730, avg_reward 1349.3, memory_length 2000, epsilon 0.007715685069697805 total_time 725.0\n",
      "episode 9740, avg_reward 1272.3, memory_length 2000, epsilon 0.007677202929869977 total_time 724.0\n",
      "episode 9750, avg_reward 1196.3, memory_length 2000, epsilon 0.007638912720515251 total_time 723.0\n",
      "episode 9760, avg_reward 1200.2, memory_length 2000, epsilon 0.007600813484376398 total_time 727.0\n",
      "episode 9770, avg_reward 1112.8, memory_length 2000, epsilon 0.007562904268970531 total_time 724.0\n",
      "episode 9780, avg_reward 1310.1, memory_length 2000, epsilon 0.007525184126565289 total_time 725.0\n",
      "episode 9790, avg_reward 1306.0, memory_length 2000, epsilon 0.007487652114155149 total_time 720.0\n",
      "episode 9800, avg_reward 1300.1, memory_length 2000, epsilon 0.007450307293437845 total_time 720.0\n",
      "Saving model weights for episode:9800\n",
      "episode 9810, avg_reward 1379.0, memory_length 2000, epsilon 0.0074131487307909074 total_time 726.0\n",
      "episode 9820, avg_reward 1226.1, memory_length 2000, epsilon 0.007376175497248348 total_time 727.0\n",
      "episode 9830, avg_reward 1213.5, memory_length 2000, epsilon 0.007339386668477396 total_time 724.0\n",
      "episode 9840, avg_reward 1281.5, memory_length 2000, epsilon 0.007302781324755415 total_time 721.0\n",
      "episode 9850, avg_reward 1147.1, memory_length 2000, epsilon 0.0072663585509469076 total_time 720.0\n",
      "episode 9860, avg_reward 1142.8, memory_length 2000, epsilon 0.00723011743648063 total_time 721.0\n",
      "episode 9870, avg_reward 1206.4, memory_length 2000, epsilon 0.007194057075326833 total_time 720.0\n",
      "episode 9880, avg_reward 1125.3, memory_length 2000, epsilon 0.00715817656597461 total_time 720.0\n",
      "episode 9890, avg_reward 1210.2, memory_length 2000, epsilon 0.007122475011409358 total_time 727.0\n",
      "episode 9900, avg_reward 1109.2, memory_length 2000, epsilon 0.007086951519090348 total_time 721.0\n",
      "Saving model weights for episode:9900\n",
      "episode 9910, avg_reward 1172.9, memory_length 2000, epsilon 0.007051605200928434 total_time 728.0\n",
      "episode 9920, avg_reward 1195.1, memory_length 2000, epsilon 0.007016435173263815 total_time 723.0\n",
      "episode 9930, avg_reward 1297.7, memory_length 2000, epsilon 0.006981440556843967 total_time 723.0\n",
      "episode 9940, avg_reward 1298.2, memory_length 2000, epsilon 0.006946620476801657 total_time 722.0\n",
      "episode 9950, avg_reward 1354.9, memory_length 2000, epsilon 0.00691197406263307 total_time 720.0\n",
      "episode 9960, avg_reward 1315.3, memory_length 2000, epsilon 0.006877500448176048 total_time 723.0\n",
      "episode 9970, avg_reward 1272.4, memory_length 2000, epsilon 0.006843198771588434 total_time 724.0\n",
      "episode 9980, avg_reward 1288.7, memory_length 2000, epsilon 0.00680906817532652 total_time 725.0\n",
      "episode 9990, avg_reward 1205.8, memory_length 2000, epsilon 0.0067751078061236356 total_time 723.0\n",
      "episode 10000, avg_reward 1365.3, memory_length 2000, epsilon 0.0067413168149687746 total_time 720.0\n",
      "Saving model weights for episode:10000\n",
      "episode 10010, avg_reward 1323.4, memory_length 2000, epsilon 0.006707694357085398 total_time 724.0\n",
      "episode 10020, avg_reward 1415.6, memory_length 2000, epsilon 0.0066742395919103074 total_time 723.0\n",
      "episode 10030, avg_reward 1369.1, memory_length 2000, epsilon 0.006640951683072632 total_time 720.0\n",
      "episode 10040, avg_reward 1365.5, memory_length 2000, epsilon 0.006607829798372916 total_time 723.0\n",
      "episode 10050, avg_reward 1358.0, memory_length 2000, epsilon 0.006574873109762318 total_time 722.0\n",
      "episode 10060, avg_reward 1334.5, memory_length 2000, epsilon 0.0065420807933219 total_time 723.0\n",
      "episode 10070, avg_reward 1247.6, memory_length 2000, epsilon 0.006509452029242055 total_time 722.0\n",
      "episode 10080, avg_reward 1327.3, memory_length 2000, epsilon 0.006476986001801974 total_time 720.0\n",
      "episode 10090, avg_reward 1206.2, memory_length 2000, epsilon 0.006444681899349282 total_time 723.0\n",
      "episode 10100, avg_reward 894.7, memory_length 2000, epsilon 0.006412538914279735 total_time 726.0\n",
      "Saving model weights for episode:10100\n",
      "episode 10110, avg_reward 1136.2, memory_length 2000, epsilon 0.006380556243017031 total_time 722.0\n",
      "episode 10120, avg_reward 1195.6, memory_length 2000, epsilon 0.006348733085992723 total_time 731.0\n",
      "episode 10130, avg_reward 1381.6, memory_length 2000, epsilon 0.006317068647626229 total_time 720.0\n",
      "episode 10140, avg_reward 1162.1, memory_length 2000, epsilon 0.006285562136304939 total_time 722.0\n",
      "episode 10150, avg_reward 1209.1, memory_length 2000, epsilon 0.006254212764364425 total_time 725.0\n",
      "episode 10160, avg_reward 1506.2, memory_length 2000, epsilon 0.006223019748068767 total_time 720.0\n",
      "episode 10170, avg_reward 1300.7, memory_length 2000, epsilon 0.006191982307590925 total_time 723.0\n",
      "episode 10180, avg_reward 1333.0, memory_length 2000, epsilon 0.006161099666993274 total_time 725.0\n",
      "episode 10190, avg_reward 1403.6, memory_length 2000, epsilon 0.006130371054208187 total_time 722.0\n",
      "episode 10200, avg_reward 1369.2, memory_length 2000, epsilon 0.006099795701018746 total_time 720.0\n",
      "Saving model weights for episode:10200\n",
      "episode 10210, avg_reward 1336.6, memory_length 2000, epsilon 0.006069372843039529 total_time 721.0\n",
      "episode 10220, avg_reward 1235.7, memory_length 2000, epsilon 0.006039101719697501 total_time 721.0\n",
      "episode 10230, avg_reward 1364.0, memory_length 2000, epsilon 0.0060089815742129975 total_time 722.0\n",
      "episode 10240, avg_reward 1380.4, memory_length 2000, epsilon 0.005979011653580822 total_time 722.0\n",
      "episode 10250, avg_reward 1145.6, memory_length 2000, epsilon 0.0059491912085513935 total_time 720.0\n",
      "episode 10260, avg_reward 1259.4, memory_length 2000, epsilon 0.005919519493612032 total_time 723.0\n",
      "episode 10270, avg_reward 1300.8, memory_length 2000, epsilon 0.005889995766968321 total_time 721.0\n",
      "episode 10280, avg_reward 1226.1, memory_length 2000, epsilon 0.005860619290525553 total_time 721.0\n",
      "episode 10290, avg_reward 1188.9, memory_length 2000, epsilon 0.00583138932987029 total_time 724.0\n",
      "episode 10300, avg_reward 1260.9, memory_length 2000, epsilon 0.005802305154251993 total_time 720.0\n",
      "Saving model weights for episode:10300\n",
      "episode 10310, avg_reward 1323.0, memory_length 2000, epsilon 0.00577336603656475 total_time 723.0\n",
      "episode 10320, avg_reward 1259.9, memory_length 2000, epsilon 0.005744571253329122 total_time 724.0\n",
      "episode 10330, avg_reward 1260.7, memory_length 2000, epsilon 0.005715920084674025 total_time 734.0\n",
      "episode 10340, avg_reward 1289.4, memory_length 2000, epsilon 0.005687411814318749 total_time 724.0\n",
      "episode 10350, avg_reward 1178.4, memory_length 2000, epsilon 0.00565904572955505 total_time 726.0\n",
      "episode 10360, avg_reward 1261.5, memory_length 2000, epsilon 0.005630821121229332 total_time 721.0\n",
      "episode 10370, avg_reward 1200.0, memory_length 2000, epsilon 0.005602737283724918 total_time 727.0\n",
      "episode 10380, avg_reward 1266.1, memory_length 2000, epsilon 0.005574793514944406 total_time 725.0\n",
      "episode 10390, avg_reward 1340.4, memory_length 2000, epsilon 0.005546989116292121 total_time 720.0\n",
      "episode 10400, avg_reward 1196.2, memory_length 2000, epsilon 0.0055193233926566455 total_time 729.0\n",
      "Saving model weights for episode:10400\n",
      "episode 10410, avg_reward 1092.6, memory_length 2000, epsilon 0.005491795652393456 total_time 724.0\n",
      "episode 10420, avg_reward 1165.6, memory_length 2000, epsilon 0.005464405207307607 total_time 720.0\n",
      "episode 10430, avg_reward 1174.5, memory_length 2000, epsilon 0.005437151372636546 total_time 721.0\n",
      "episode 10440, avg_reward 1131.6, memory_length 2000, epsilon 0.005410033467032986 total_time 726.0\n",
      "episode 10450, avg_reward 1119.8, memory_length 2000, epsilon 0.005383050812547875 total_time 725.0\n",
      "episode 10460, avg_reward 1181.0, memory_length 2000, epsilon 0.005356202734613444 total_time 720.0\n",
      "episode 10470, avg_reward 1195.2, memory_length 2000, epsilon 0.0053294885620263485 total_time 724.0\n",
      "episode 10480, avg_reward 1342.3, memory_length 2000, epsilon 0.005302907626930876 total_time 722.0\n",
      "episode 10490, avg_reward 1218.4, memory_length 2000, epsilon 0.005276459264802275 total_time 724.0\n",
      "episode 10500, avg_reward 1261.3, memory_length 2000, epsilon 0.00525014281443011 total_time 723.0\n",
      "Saving model weights for episode:10500\n",
      "episode 10510, avg_reward 1287.5, memory_length 2000, epsilon 0.005223957617901751 total_time 722.0\n",
      "episode 10520, avg_reward 1315.6, memory_length 2000, epsilon 0.005197903020585921 total_time 720.0\n",
      "episode 10530, avg_reward 1250.4, memory_length 2000, epsilon 0.00517197837111633 total_time 721.0\n",
      "episode 10540, avg_reward 1352.0, memory_length 2000, epsilon 0.0051461830213753905 total_time 721.0\n",
      "episode 10550, avg_reward 1413.6, memory_length 2000, epsilon 0.005120516326478017 total_time 724.0\n",
      "episode 10560, avg_reward 1273.8, memory_length 2000, epsilon 0.0050949776447554935 total_time 723.0\n",
      "episode 10570, avg_reward 1196.1, memory_length 2000, epsilon 0.005069566337739457 total_time 721.0\n",
      "episode 10580, avg_reward 1306.8, memory_length 2000, epsilon 0.005044281770145906 total_time 728.0\n",
      "episode 10590, avg_reward 1362.2, memory_length 2000, epsilon 0.0050191233098593295 total_time 723.0\n",
      "episode 10600, avg_reward 1434.8, memory_length 2000, epsilon 0.004994090327916914 total_time 725.0\n",
      "Saving model weights for episode:10600\n",
      "episode 10610, avg_reward 1244.4, memory_length 2000, epsilon 0.004969182198492805 total_time 731.0\n",
      "episode 10620, avg_reward 1121.8, memory_length 2000, epsilon 0.00494439829888247 total_time 727.0\n",
      "episode 10630, avg_reward 1263.7, memory_length 2000, epsilon 0.004919738009487128 total_time 722.0\n",
      "episode 10640, avg_reward 1336.7, memory_length 2000, epsilon 0.00489520071379826 total_time 720.0\n",
      "episode 10650, avg_reward 1380.0, memory_length 2000, epsilon 0.00487078579838219 total_time 720.0\n",
      "episode 10660, avg_reward 1246.6, memory_length 2000, epsilon 0.004846492652864773 total_time 723.0\n",
      "episode 10670, avg_reward 1270.9, memory_length 2000, epsilon 0.004822320669916097 total_time 723.0\n",
      "episode 10680, avg_reward 1267.9, memory_length 2000, epsilon 0.004798269245235333 total_time 722.0\n",
      "episode 10690, avg_reward 1251.0, memory_length 2000, epsilon 0.00477433777753561 total_time 722.0\n",
      "episode 10700, avg_reward 1352.3, memory_length 2000, epsilon 0.0047505256685289885 total_time 724.0\n",
      "Saving model weights for episode:10700\n",
      "episode 10710, avg_reward 1350.3, memory_length 2000, epsilon 0.004726832322911504 total_time 729.0\n",
      "episode 10720, avg_reward 1202.3, memory_length 2000, epsilon 0.004703257148348282 total_time 727.0\n",
      "episode 10730, avg_reward 1175.3, memory_length 2000, epsilon 0.004679799555458725 total_time 724.0\n",
      "episode 10740, avg_reward 1179.9, memory_length 2000, epsilon 0.004656458957801799 total_time 723.0\n",
      "episode 10750, avg_reward 1149.8, memory_length 2000, epsilon 0.004633234771861343 total_time 722.0\n",
      "episode 10760, avg_reward 1115.9, memory_length 2000, epsilon 0.004610126417031497 total_time 723.0\n",
      "episode 10770, avg_reward 1254.6, memory_length 2000, epsilon 0.004587133315602188 total_time 726.0\n",
      "episode 10780, avg_reward 1256.9, memory_length 2000, epsilon 0.004564254892744682 total_time 720.0\n",
      "episode 10790, avg_reward 1288.8, memory_length 2000, epsilon 0.004541490576497217 total_time 730.0\n",
      "episode 10800, avg_reward 1298.6, memory_length 2000, epsilon 0.004518839797750701 total_time 720.0\n",
      "Saving model weights for episode:10800\n",
      "episode 10810, avg_reward 1473.6, memory_length 2000, epsilon 0.00449630199023448 total_time 721.0\n",
      "episode 10820, avg_reward 1357.6, memory_length 2000, epsilon 0.004473876590502203 total_time 724.0\n",
      "episode 10830, avg_reward 1266.1, memory_length 2000, epsilon 0.0044515630379177015 total_time 721.0\n",
      "episode 10840, avg_reward 1327.0, memory_length 2000, epsilon 0.004429360774641001 total_time 724.0\n",
      "episode 10850, avg_reward 1534.0, memory_length 2000, epsilon 0.004407269245614362 total_time 726.0\n",
      "episode 10860, avg_reward 1247.6, memory_length 2000, epsilon 0.00438528789854841 total_time 723.0\n",
      "episode 10870, avg_reward 1275.6, memory_length 2000, epsilon 0.004363416183908322 total_time 725.0\n",
      "episode 10880, avg_reward 1272.2, memory_length 2000, epsilon 0.004341653554900093 total_time 727.0\n",
      "episode 10890, avg_reward 1359.8, memory_length 2000, epsilon 0.004319999467456865 total_time 724.0\n",
      "episode 10900, avg_reward 1298.4, memory_length 2000, epsilon 0.004298453380225319 total_time 729.0\n",
      "Saving model weights for episode:10900\n",
      "episode 10910, avg_reward 1317.1, memory_length 2000, epsilon 0.00427701475455216 total_time 722.0\n",
      "episode 10920, avg_reward 1058.7, memory_length 2000, epsilon 0.004255683054470627 total_time 720.0\n",
      "episode 10930, avg_reward 940.5, memory_length 2000, epsilon 0.004234457746687106 total_time 726.0\n",
      "episode 10940, avg_reward 1153.4, memory_length 2000, epsilon 0.004213338300567797 total_time 720.0\n",
      "episode 10950, avg_reward 1190.3, memory_length 2000, epsilon 0.004192324188125447 total_time 721.0\n",
      "episode 10960, avg_reward 1076.0, memory_length 2000, epsilon 0.00417141488400615 total_time 724.0\n",
      "episode 10970, avg_reward 1297.8, memory_length 2000, epsilon 0.004150609865476215 total_time 729.0\n",
      "episode 10980, avg_reward 1328.4, memory_length 2000, epsilon 0.004129908612409092 total_time 721.0\n",
      "episode 10990, avg_reward 1299.1, memory_length 2000, epsilon 0.004109310607272381 total_time 726.0\n",
      "episode 11000, avg_reward 1300.3, memory_length 2000, epsilon 0.004088815335114879 total_time 722.0\n",
      "Saving model weights for episode:11000\n",
      "episode 11010, avg_reward 1365.2, memory_length 2000, epsilon 0.004068422283553715 total_time 726.0\n",
      "episode 11020, avg_reward 1361.7, memory_length 2000, epsilon 0.004048130942761536 total_time 722.0\n",
      "episode 11030, avg_reward 1432.8, memory_length 2000, epsilon 0.004027940805453767 total_time 722.0\n",
      "episode 11040, avg_reward 1376.8, memory_length 2000, epsilon 0.004007851366875923 total_time 721.0\n",
      "episode 11050, avg_reward 1487.2, memory_length 2000, epsilon 0.003987862124790993 total_time 726.0\n",
      "episode 11060, avg_reward 1341.0, memory_length 2000, epsilon 0.00396797257946688 total_time 723.0\n",
      "episode 11070, avg_reward 1307.3, memory_length 2000, epsilon 0.003948182233663924 total_time 728.0\n",
      "episode 11080, avg_reward 1226.1, memory_length 2000, epsilon 0.003928490592622443 total_time 737.0\n",
      "episode 11090, avg_reward 1292.8, memory_length 2000, epsilon 0.003908897164050388 total_time 720.0\n",
      "episode 11100, avg_reward 1210.5, memory_length 2000, epsilon 0.0038894014581110216 total_time 725.0\n",
      "Saving model weights for episode:11100\n",
      "episode 11110, avg_reward 1227.2, memory_length 2000, epsilon 0.0038700029874106814 total_time 727.0\n",
      "episode 11120, avg_reward 1240.3, memory_length 2000, epsilon 0.0038507012669865893 total_time 729.0\n",
      "episode 11130, avg_reward 1338.0, memory_length 2000, epsilon 0.0038314958142947296 total_time 725.0\n",
      "episode 11140, avg_reward 1405.3, memory_length 2000, epsilon 0.0038123861491977844 total_time 721.0\n",
      "episode 11150, avg_reward 1371.4, memory_length 2000, epsilon 0.003793371793953128 total_time 727.0\n",
      "episode 11160, avg_reward 1336.9, memory_length 2000, epsilon 0.003774452273200895 total_time 720.0\n",
      "episode 11170, avg_reward 1371.7, memory_length 2000, epsilon 0.0037556271139520783 total_time 722.0\n",
      "episode 11180, avg_reward 1278.0, memory_length 2000, epsilon 0.0037368958455767163 total_time 734.0\n",
      "episode 11190, avg_reward 1424.8, memory_length 2000, epsilon 0.003718257999792124 total_time 725.0\n",
      "episode 11200, avg_reward 1433.2, memory_length 2000, epsilon 0.0036997131106511856 total_time 720.0\n",
      "Saving model weights for episode:11200\n",
      "episode 11210, avg_reward 1305.8, memory_length 2000, epsilon 0.003681260714530707 total_time 725.0\n",
      "episode 11220, avg_reward 1339.3, memory_length 2000, epsilon 0.0036629003501198244 total_time 722.0\n",
      "episode 11230, avg_reward 1357.4, memory_length 2000, epsilon 0.0036446315584084676 total_time 724.0\n",
      "episode 11240, avg_reward 1273.5, memory_length 2000, epsilon 0.0036264538826758994 total_time 726.0\n",
      "episode 11250, avg_reward 1282.9, memory_length 2000, epsilon 0.0036083668684792755 total_time 724.0\n",
      "episode 11260, avg_reward 1298.1, memory_length 2000, epsilon 0.0035903700636422996 total_time 720.0\n",
      "episode 11270, avg_reward 1352.0, memory_length 2000, epsilon 0.003572463018243914 total_time 728.0\n",
      "episode 11280, avg_reward 1271.8, memory_length 2000, epsilon 0.0035546452846070497 total_time 724.0\n",
      "episode 11290, avg_reward 1283.4, memory_length 2000, epsilon 0.0035369164172874387 total_time 721.0\n",
      "episode 11300, avg_reward 1171.1, memory_length 2000, epsilon 0.0035192759730624744 total_time 720.0\n",
      "Saving model weights for episode:11300\n",
      "episode 11310, avg_reward 1199.6, memory_length 2000, epsilon 0.003501723510920129 total_time 721.0\n",
      "episode 11320, avg_reward 1258.7, memory_length 2000, epsilon 0.003484258592047942 total_time 721.0\n",
      "episode 11330, avg_reward 1120.4, memory_length 2000, epsilon 0.003466880779822028 total_time 725.0\n",
      "episode 11340, avg_reward 1311.6, memory_length 2000, epsilon 0.003449589639796176 total_time 724.0\n",
      "episode 11350, avg_reward 1082.1, memory_length 2000, epsilon 0.0034323847396909857 total_time 725.0\n",
      "episode 11360, avg_reward 1259.7, memory_length 2000, epsilon 0.0034152656493830575 total_time 720.0\n",
      "episode 11370, avg_reward 1284.2, memory_length 2000, epsilon 0.0033982319408942425 total_time 725.0\n",
      "episode 11380, avg_reward 1383.8, memory_length 2000, epsilon 0.003381283188380941 total_time 728.0\n",
      "episode 11390, avg_reward 1160.1, memory_length 2000, epsilon 0.003364418968123455 total_time 724.0\n",
      "episode 11400, avg_reward 1079.0, memory_length 2000, epsilon 0.0033476388585154055 total_time 725.0\n",
      "Saving model weights for episode:11400\n",
      "episode 11410, avg_reward 1044.8, memory_length 2000, epsilon 0.003330942440053175 total_time 723.0\n",
      "episode 11420, avg_reward 1202.8, memory_length 2000, epsilon 0.0033143292953254325 total_time 726.0\n",
      "episode 11430, avg_reward 1116.4, memory_length 2000, epsilon 0.0032977990090026946 total_time 730.0\n",
      "episode 11440, avg_reward 1221.3, memory_length 2000, epsilon 0.0032813511678269425 total_time 725.0\n",
      "episode 11450, avg_reward 1153.6, memory_length 2000, epsilon 0.00326498536060129 total_time 721.0\n",
      "episode 11460, avg_reward 1126.1, memory_length 2000, epsilon 0.0032487011781797037 total_time 727.0\n",
      "episode 11470, avg_reward 1165.2, memory_length 2000, epsilon 0.003232498213456775 total_time 720.0\n",
      "episode 11480, avg_reward 1190.2, memory_length 2000, epsilon 0.0032163760613575397 total_time 727.0\n",
      "episode 11490, avg_reward 1152.9, memory_length 2000, epsilon 0.0032003343188273608 total_time 727.0\n",
      "episode 11500, avg_reward 1080.5, memory_length 2000, epsilon 0.0031843725848218368 total_time 720.0\n",
      "Saving model weights for episode:11500\n",
      "episode 11510, avg_reward 1143.8, memory_length 2000, epsilon 0.0031684904602967863 total_time 721.0\n",
      "episode 11520, avg_reward 1152.7, memory_length 2000, epsilon 0.003152687548198269 total_time 724.0\n",
      "episode 11530, avg_reward 1145.9, memory_length 2000, epsilon 0.0031369634534526587 total_time 721.0\n",
      "episode 11540, avg_reward 1043.1, memory_length 2000, epsilon 0.003121317782956769 total_time 721.0\n",
      "episode 11550, avg_reward 1067.5, memory_length 2000, epsilon 0.0031057501455680216 total_time 722.0\n",
      "episode 11560, avg_reward 1059.5, memory_length 2000, epsilon 0.003090260152094669 total_time 720.0\n",
      "episode 11570, avg_reward 1126.2, memory_length 2000, epsilon 0.0030748474152860724 total_time 725.0\n",
      "episode 11580, avg_reward 1063.7, memory_length 2000, epsilon 0.003059511549823006 total_time 722.0\n",
      "episode 11590, avg_reward 1023.3, memory_length 2000, epsilon 0.0030442521723080354 total_time 720.0\n",
      "episode 11600, avg_reward 1153.2, memory_length 2000, epsilon 0.0030290689012559273 total_time 724.0\n",
      "Saving model weights for episode:11600\n",
      "episode 11610, avg_reward 1099.7, memory_length 2000, epsilon 0.0030139613570841148 total_time 721.0\n",
      "episode 11620, avg_reward 1189.6, memory_length 2000, epsilon 0.0029989291621032067 total_time 723.0\n",
      "episode 11630, avg_reward 1209.9, memory_length 2000, epsilon 0.0029839719405075457 total_time 725.0\n",
      "episode 11640, avg_reward 1150.3, memory_length 2000, epsilon 0.0029690893183658097 total_time 723.0\n",
      "episode 11650, avg_reward 1217.3, memory_length 2000, epsilon 0.002954280923611676 total_time 724.0\n",
      "episode 11660, avg_reward 1301.1, memory_length 2000, epsilon 0.002939546386034501 total_time 722.0\n",
      "episode 11670, avg_reward 1307.5, memory_length 2000, epsilon 0.002924885337270079 total_time 721.0\n",
      "episode 11680, avg_reward 1257.3, memory_length 2000, epsilon 0.0029102974107914265 total_time 722.0\n",
      "episode 11690, avg_reward 1367.1, memory_length 2000, epsilon 0.002895782241899622 total_time 720.0\n",
      "episode 11700, avg_reward 1157.0, memory_length 2000, epsilon 0.0028813394677146865 total_time 727.0\n",
      "Saving model weights for episode:11700\n",
      "episode 11710, avg_reward 1240.2, memory_length 2000, epsilon 0.002866968727166514 total_time 724.0\n",
      "episode 11720, avg_reward 1149.0, memory_length 2000, epsilon 0.0028526696609858424 total_time 721.0\n",
      "episode 11730, avg_reward 1163.7, memory_length 2000, epsilon 0.0028384419116952693 total_time 720.0\n",
      "episode 11740, avg_reward 1341.9, memory_length 2000, epsilon 0.0028242851236003268 total_time 720.0\n",
      "episode 11750, avg_reward 1299.9, memory_length 2000, epsilon 0.0028101989427805727 total_time 721.0\n",
      "episode 11760, avg_reward 1292.3, memory_length 2000, epsilon 0.002796183017080753 total_time 721.0\n",
      "episode 11770, avg_reward 1137.5, memory_length 2000, epsilon 0.0027822369961019943 total_time 720.0\n",
      "episode 11780, avg_reward 1214.5, memory_length 2000, epsilon 0.002768360531193047 total_time 720.0\n",
      "episode 11790, avg_reward 1249.0, memory_length 2000, epsilon 0.002754553275441565 total_time 725.0\n",
      "episode 11800, avg_reward 1318.3, memory_length 2000, epsilon 0.0027408148836654354 total_time 722.0\n",
      "Saving model weights for episode:11800\n",
      "episode 11810, avg_reward 1192.6, memory_length 2000, epsilon 0.0027271450124041457 total_time 724.0\n",
      "episode 11820, avg_reward 1254.4, memory_length 2000, epsilon 0.002713543319910208 total_time 720.0\n",
      "episode 11830, avg_reward 1123.2, memory_length 2000, epsilon 0.002700009466140598 total_time 731.0\n",
      "episode 11840, avg_reward 1175.9, memory_length 2000, epsilon 0.0026865431127482673 total_time 724.0\n",
      "episode 11850, avg_reward 1233.6, memory_length 2000, epsilon 0.0026731439230736798 total_time 723.0\n",
      "episode 11860, avg_reward 1184.7, memory_length 2000, epsilon 0.002659811562136395 total_time 720.0\n",
      "episode 11870, avg_reward 1131.9, memory_length 2000, epsilon 0.002646545696626696 total_time 734.0\n",
      "episode 11880, avg_reward 1276.2, memory_length 2000, epsilon 0.0026333459948972536 total_time 724.0\n",
      "episode 11890, avg_reward 1135.6, memory_length 2000, epsilon 0.002620212126954835 total_time 722.0\n",
      "episode 11900, avg_reward 1255.8, memory_length 2000, epsilon 0.0026071437644520617 total_time 720.0\n",
      "Saving model weights for episode:11900\n",
      "episode 11910, avg_reward 1311.4, memory_length 2000, epsilon 0.0025941405806791887 total_time 720.0\n",
      "episode 11920, avg_reward 1299.3, memory_length 2000, epsilon 0.0025812022505559444 total_time 723.0\n",
      "episode 11930, avg_reward 1247.2, memory_length 2000, epsilon 0.002568328450623402 total_time 725.0\n",
      "episode 11940, avg_reward 1335.5, memory_length 2000, epsilon 0.0025555188590358916 total_time 721.0\n",
      "episode 11950, avg_reward 1319.3, memory_length 2000, epsilon 0.0025427731555529576 total_time 722.0\n",
      "episode 11960, avg_reward 1399.3, memory_length 2000, epsilon 0.0025300910215313487 total_time 725.0\n",
      "episode 11970, avg_reward 1473.1, memory_length 2000, epsilon 0.002517472139917054 total_time 725.0\n",
      "episode 11980, avg_reward 1474.9, memory_length 2000, epsilon 0.002504916195237373 total_time 727.0\n",
      "episode 11990, avg_reward 1360.0, memory_length 2000, epsilon 0.0024924228735930397 total_time 721.0\n",
      "episode 12000, avg_reward 1362.9, memory_length 2000, epsilon 0.00247999186265036 total_time 724.0\n",
      "Saving model weights for episode:12000\n",
      "episode 12010, avg_reward 1325.7, memory_length 2000, epsilon 0.002467622851633413 total_time 723.0\n",
      "episode 12020, avg_reward 1391.6, memory_length 2000, epsilon 0.002455315531316279 total_time 724.0\n",
      "episode 12030, avg_reward 1423.5, memory_length 2000, epsilon 0.002443069594015309 total_time 727.0\n",
      "episode 12040, avg_reward 1309.8, memory_length 2000, epsilon 0.002430884733581433 total_time 728.0\n",
      "episode 12050, avg_reward 1453.4, memory_length 2000, epsilon 0.002418760645392505 total_time 720.0\n",
      "episode 12060, avg_reward 1407.2, memory_length 2000, epsilon 0.0024066970263456867 total_time 724.0\n",
      "episode 12070, avg_reward 1326.1, memory_length 2000, epsilon 0.0023946935748498785 total_time 726.0\n",
      "episode 12080, avg_reward 1265.7, memory_length 2000, epsilon 0.002382749990818165 total_time 724.0\n",
      "episode 12090, avg_reward 1440.8, memory_length 2000, epsilon 0.0023708659756603236 total_time 720.0\n",
      "episode 12100, avg_reward 1335.7, memory_length 2000, epsilon 0.002359041232275356 total_time 726.0\n",
      "Saving model weights for episode:12100\n",
      "episode 12110, avg_reward 1496.9, memory_length 2000, epsilon 0.002347275465044063 total_time 724.0\n",
      "episode 12120, avg_reward 1370.4, memory_length 2000, epsilon 0.0023355683798216495 total_time 724.0\n",
      "episode 12130, avg_reward 1332.7, memory_length 2000, epsilon 0.002323919683930376 total_time 725.0\n",
      "episode 12140, avg_reward 1472.9, memory_length 2000, epsilon 0.0023123290861522365 total_time 720.0\n",
      "episode 12150, avg_reward 1432.3, memory_length 2000, epsilon 0.002300796296721686 total_time 724.0\n",
      "episode 12160, avg_reward 1347.0, memory_length 2000, epsilon 0.0022893210273183875 total_time 720.0\n",
      "episode 12170, avg_reward 1278.8, memory_length 2000, epsilon 0.0022779029910600075 total_time 727.0\n",
      "episode 12180, avg_reward 1285.4, memory_length 2000, epsilon 0.002266541902495045 total_time 724.0\n",
      "episode 12190, avg_reward 1352.6, memory_length 2000, epsilon 0.0022552374775956937 total_time 727.0\n",
      "episode 12200, avg_reward 1312.6, memory_length 2000, epsilon 0.002243989433750743 total_time 722.0\n",
      "Saving model weights for episode:12200\n",
      "episode 12210, avg_reward 1292.9, memory_length 2000, epsilon 0.00223279748975851 total_time 720.0\n",
      "episode 12220, avg_reward 1336.0, memory_length 2000, epsilon 0.002221661365819813 total_time 724.0\n",
      "episode 12230, avg_reward 1431.5, memory_length 2000, epsilon 0.002210580783530971 total_time 724.0\n",
      "episode 12240, avg_reward 1338.6, memory_length 2000, epsilon 0.002199555465876854 total_time 724.0\n",
      "episode 12250, avg_reward 1375.0, memory_length 2000, epsilon 0.0021885851372239443 total_time 726.0\n",
      "episode 12260, avg_reward 1307.4, memory_length 2000, epsilon 0.002177669523313454 total_time 722.0\n",
      "episode 12270, avg_reward 1405.1, memory_length 2000, epsilon 0.0021668083512544666 total_time 723.0\n",
      "episode 12280, avg_reward 1263.5, memory_length 2000, epsilon 0.002156001349517115 total_time 726.0\n",
      "episode 12290, avg_reward 1289.3, memory_length 2000, epsilon 0.002145248247925794 total_time 722.0\n",
      "episode 12300, avg_reward 1259.9, memory_length 2000, epsilon 0.0021345487776524025 total_time 722.0\n",
      "Saving model weights for episode:12300\n",
      "episode 12310, avg_reward 1324.7, memory_length 2000, epsilon 0.0021239026712096248 total_time 722.0\n",
      "episode 12320, avg_reward 1244.6, memory_length 2000, epsilon 0.0021133096624442495 total_time 725.0\n",
      "episode 12330, avg_reward 1356.2, memory_length 2000, epsilon 0.0021027694865305034 total_time 721.0\n",
      "episode 12340, avg_reward 1254.1, memory_length 2000, epsilon 0.0020922818799634403 total_time 723.0\n",
      "episode 12350, avg_reward 1288.1, memory_length 2000, epsilon 0.002081846580552349 total_time 728.0\n",
      "episode 12360, avg_reward 1247.9, memory_length 2000, epsilon 0.002071463327414201 total_time 722.0\n",
      "episode 12370, avg_reward 1259.5, memory_length 2000, epsilon 0.0020611318609671274 total_time 721.0\n",
      "episode 12380, avg_reward 1314.0, memory_length 2000, epsilon 0.002050851922923929 total_time 724.0\n",
      "episode 12390, avg_reward 1275.2, memory_length 2000, epsilon 0.0020406232562856165 total_time 722.0\n",
      "episode 12400, avg_reward 1218.2, memory_length 2000, epsilon 0.0020304456053349958 total_time 723.0\n",
      "Saving model weights for episode:12400\n",
      "episode 12410, avg_reward 1340.5, memory_length 2000, epsilon 0.002020318715630261 total_time 724.0\n",
      "episode 12420, avg_reward 1434.4, memory_length 2000, epsilon 0.002010242333998642 total_time 726.0\n",
      "episode 12430, avg_reward 1387.5, memory_length 2000, epsilon 0.002000216208530073 total_time 725.0\n",
      "episode 12440, avg_reward 1426.5, memory_length 2000, epsilon 0.0019902400885708957 total_time 727.0\n",
      "episode 12450, avg_reward 1303.0, memory_length 2000, epsilon 0.0019803137247175907 total_time 729.0\n",
      "episode 12460, avg_reward 1258.3, memory_length 2000, epsilon 0.001970436868810545 total_time 725.0\n",
      "episode 12470, avg_reward 1269.5, memory_length 2000, epsilon 0.001960609273927847 total_time 722.0\n",
      "episode 12480, avg_reward 1268.0, memory_length 2000, epsilon 0.0019508306943791102 total_time 722.0\n",
      "episode 12490, avg_reward 1437.5, memory_length 2000, epsilon 0.0019411008856993405 total_time 722.0\n",
      "episode 12500, avg_reward 1331.1, memory_length 2000, epsilon 0.0019314196046428124 total_time 725.0\n",
      "Saving model weights for episode:12500\n",
      "episode 12510, avg_reward 1062.9, memory_length 2000, epsilon 0.0019217866091769953 total_time 724.0\n",
      "episode 12520, avg_reward 1201.8, memory_length 2000, epsilon 0.0019122016584765006 total_time 720.0\n",
      "episode 12530, avg_reward 1338.0, memory_length 2000, epsilon 0.001902664512917062 total_time 730.0\n",
      "episode 12540, avg_reward 1374.9, memory_length 2000, epsilon 0.0018931749340695434 total_time 728.0\n",
      "episode 12550, avg_reward 1357.3, memory_length 2000, epsilon 0.0018837326846939794 total_time 726.0\n",
      "episode 12560, avg_reward 1382.9, memory_length 2000, epsilon 0.0018743375287336424 total_time 720.0\n",
      "episode 12570, avg_reward 1360.0, memory_length 2000, epsilon 0.001864989231309147 total_time 721.0\n",
      "episode 12580, avg_reward 1418.1, memory_length 2000, epsilon 0.0018556875587125694 total_time 731.0\n",
      "episode 12590, avg_reward 1273.2, memory_length 2000, epsilon 0.0018464322784016102 total_time 722.0\n",
      "episode 12600, avg_reward 1360.3, memory_length 2000, epsilon 0.0018372231589937793 total_time 721.0\n",
      "Saving model weights for episode:12600\n",
      "episode 12610, avg_reward 1280.7, memory_length 2000, epsilon 0.0018280599702606122 total_time 728.0\n",
      "episode 12620, avg_reward 1272.0, memory_length 2000, epsilon 0.001818942483121913 total_time 720.0\n",
      "episode 12630, avg_reward 1405.5, memory_length 2000, epsilon 0.0018098704696400286 total_time 724.0\n",
      "episode 12640, avg_reward 1373.7, memory_length 2000, epsilon 0.0018008437030141477 total_time 722.0\n",
      "episode 12650, avg_reward 1334.6, memory_length 2000, epsilon 0.0017918619575746377 total_time 727.0\n",
      "episode 12660, avg_reward 1356.9, memory_length 2000, epsilon 0.0017829250087773934 total_time 730.0\n",
      "episode 12670, avg_reward 1297.4, memory_length 2000, epsilon 0.0017740326331982293 total_time 725.0\n",
      "episode 12680, avg_reward 1323.6, memory_length 2000, epsilon 0.0017651846085272927 total_time 720.0\n",
      "episode 12690, avg_reward 1482.4, memory_length 2000, epsilon 0.0017563807135635061 total_time 721.0\n",
      "episode 12700, avg_reward 1262.8, memory_length 2000, epsilon 0.0017476207282090368 total_time 720.0\n",
      "Saving model weights for episode:12700\n",
      "episode 12710, avg_reward 1476.2, memory_length 2000, epsilon 0.0017389044334637947 total_time 722.0\n",
      "episode 12720, avg_reward 1275.0, memory_length 2000, epsilon 0.0017302316114199572 total_time 724.0\n",
      "episode 12730, avg_reward 1330.6, memory_length 2000, epsilon 0.0017216020452565201 total_time 721.0\n",
      "episode 12740, avg_reward 1188.2, memory_length 2000, epsilon 0.0017130155192338827 total_time 720.0\n",
      "episode 12750, avg_reward 1295.4, memory_length 2000, epsilon 0.0017044718186884457 total_time 720.0\n",
      "episode 12760, avg_reward 1328.5, memory_length 2000, epsilon 0.0016959707300272507 total_time 720.0\n",
      "episode 12770, avg_reward 1551.4, memory_length 2000, epsilon 0.0016875120407226383 total_time 722.0\n",
      "episode 12780, avg_reward 1253.3, memory_length 2000, epsilon 0.001679095539306935 total_time 720.0\n",
      "episode 12790, avg_reward 1399.7, memory_length 2000, epsilon 0.0016707210153671676 total_time 721.0\n",
      "episode 12800, avg_reward 1452.6, memory_length 2000, epsilon 0.0016623882595398012 total_time 720.0\n",
      "Saving model weights for episode:12800\n",
      "episode 12810, avg_reward 1262.3, memory_length 2000, epsilon 0.0016540970635055044 total_time 728.0\n",
      "episode 12820, avg_reward 1138.5, memory_length 2000, epsilon 0.001645847219983948 total_time 721.0\n",
      "episode 12830, avg_reward 1428.9, memory_length 2000, epsilon 0.0016376385227286122 total_time 723.0\n",
      "episode 12840, avg_reward 1150.6, memory_length 2000, epsilon 0.0016294707665216383 total_time 726.0\n",
      "episode 12850, avg_reward 1276.5, memory_length 2000, epsilon 0.001621343747168696 total_time 723.0\n",
      "episode 12860, avg_reward 1157.9, memory_length 2000, epsilon 0.0016132572614938779 total_time 734.0\n",
      "episode 12870, avg_reward 1455.3, memory_length 2000, epsilon 0.0016052111073346208 total_time 730.0\n",
      "episode 12880, avg_reward 1455.4, memory_length 2000, epsilon 0.0015972050835366522 total_time 724.0\n",
      "episode 12890, avg_reward 1472.8, memory_length 2000, epsilon 0.0015892389899489583 total_time 726.0\n",
      "episode 12900, avg_reward 1425.5, memory_length 2000, epsilon 0.0015813126274187876 total_time 720.0\n",
      "Saving model weights for episode:12900\n",
      "episode 12910, avg_reward 1320.0, memory_length 2000, epsilon 0.0015734257977866623 total_time 721.0\n",
      "episode 12920, avg_reward 1370.0, memory_length 2000, epsilon 0.001565578303881431 total_time 722.0\n",
      "episode 12930, avg_reward 1305.3, memory_length 2000, epsilon 0.0015577699495153375 total_time 722.0\n",
      "episode 12940, avg_reward 1317.4, memory_length 2000, epsilon 0.0015500005394791156 total_time 720.0\n",
      "episode 12950, avg_reward 1376.5, memory_length 2000, epsilon 0.00154226987953711 total_time 726.0\n",
      "episode 12960, avg_reward 1198.6, memory_length 2000, epsilon 0.0015345777764224195 total_time 721.0\n",
      "episode 12970, avg_reward 1192.7, memory_length 2000, epsilon 0.0015269240378320656 total_time 722.0\n",
      "episode 12980, avg_reward 1424.2, memory_length 2000, epsilon 0.0015193084724221834 total_time 722.0\n",
      "episode 12990, avg_reward 1385.7, memory_length 2000, epsilon 0.0015117308898032439 total_time 726.0\n",
      "episode 13000, avg_reward 1316.1, memory_length 2000, epsilon 0.0015041911005352856 total_time 721.0\n",
      "Saving model weights for episode:13000\n",
      "episode 13010, avg_reward 1301.6, memory_length 2000, epsilon 0.0014966889161231837 total_time 724.0\n",
      "episode 13020, avg_reward 1437.6, memory_length 2000, epsilon 0.001489224149011938 total_time 723.0\n",
      "episode 13030, avg_reward 1489.7, memory_length 2000, epsilon 0.001481796612581981 total_time 721.0\n",
      "episode 13040, avg_reward 1478.5, memory_length 2000, epsilon 0.0014744061211445156 total_time 722.0\n",
      "episode 13050, avg_reward 1403.5, memory_length 2000, epsilon 0.001467052489936871 total_time 721.0\n",
      "episode 13060, avg_reward 1346.1, memory_length 2000, epsilon 0.0014597355351178825 total_time 720.0\n",
      "episode 13070, avg_reward 1391.5, memory_length 2000, epsilon 0.001452455073763301 total_time 724.0\n",
      "episode 13080, avg_reward 1361.8, memory_length 2000, epsilon 0.0014452109238612127 total_time 726.0\n",
      "episode 13090, avg_reward 1444.5, memory_length 2000, epsilon 0.0014380029043074922 total_time 721.0\n",
      "episode 13100, avg_reward 1292.1, memory_length 2000, epsilon 0.0014308308349012756 total_time 729.0\n",
      "Saving model weights for episode:13100\n",
      "episode 13110, avg_reward 1270.3, memory_length 2000, epsilon 0.001423694536340454 total_time 726.0\n",
      "episode 13120, avg_reward 1307.0, memory_length 2000, epsilon 0.0014165938302171915 total_time 720.0\n",
      "episode 13130, avg_reward 1337.3, memory_length 2000, epsilon 0.0014095285390134655 total_time 728.0\n",
      "episode 13140, avg_reward 1366.3, memory_length 2000, epsilon 0.0014024984860966266 total_time 727.0\n",
      "episode 13150, avg_reward 1438.6, memory_length 2000, epsilon 0.0013955034957149883 total_time 721.0\n",
      "episode 13160, avg_reward 1246.8, memory_length 2000, epsilon 0.0013885433929934255 total_time 723.0\n",
      "episode 13170, avg_reward 1344.4, memory_length 2000, epsilon 0.0013816180039290073 total_time 729.0\n",
      "episode 13180, avg_reward 1372.7, memory_length 2000, epsilon 0.0013747271553866468 total_time 722.0\n",
      "episode 13190, avg_reward 1274.7, memory_length 2000, epsilon 0.0013678706750947715 total_time 722.0\n",
      "episode 13200, avg_reward 1277.1, memory_length 2000, epsilon 0.0013610483916410166 total_time 721.0\n",
      "Saving model weights for episode:13200\n",
      "episode 13210, avg_reward 1199.4, memory_length 2000, epsilon 0.001354260134467941 total_time 720.0\n",
      "episode 13220, avg_reward 1315.0, memory_length 2000, epsilon 0.0013475057338687615 total_time 722.0\n",
      "episode 13230, avg_reward 1359.4, memory_length 2000, epsilon 0.0013407850209831102 total_time 721.0\n",
      "episode 13240, avg_reward 1441.6, memory_length 2000, epsilon 0.0013340978277928172 total_time 723.0\n",
      "episode 13250, avg_reward 1297.3, memory_length 2000, epsilon 0.001327443987117703 total_time 720.0\n",
      "episode 13260, avg_reward 1221.2, memory_length 2000, epsilon 0.001320823332611405 total_time 720.0\n",
      "episode 13270, avg_reward 1396.6, memory_length 2000, epsilon 0.001314235698757215 total_time 720.0\n",
      "episode 13280, avg_reward 1439.2, memory_length 2000, epsilon 0.001307680920863944 total_time 720.0\n",
      "episode 13290, avg_reward 1211.0, memory_length 2000, epsilon 0.0013011588350618029 total_time 720.0\n",
      "episode 13300, avg_reward 1170.1, memory_length 2000, epsilon 0.001294669278298307 total_time 726.0\n",
      "Saving model weights for episode:13300\n",
      "episode 13310, avg_reward 1280.2, memory_length 2000, epsilon 0.0012882120883341982 total_time 725.0\n",
      "episode 13320, avg_reward 1307.8, memory_length 2000, epsilon 0.0012817871037393933 total_time 720.0\n",
      "episode 13330, avg_reward 1253.4, memory_length 2000, epsilon 0.0012753941638889417 total_time 722.0\n",
      "episode 13340, avg_reward 1334.0, memory_length 2000, epsilon 0.0012690331089590143 total_time 722.0\n",
      "episode 13350, avg_reward 1085.8, memory_length 2000, epsilon 0.0012627037799229062 total_time 724.0\n",
      "episode 13360, avg_reward 1309.7, memory_length 2000, epsilon 0.0012564060185470622 total_time 725.0\n",
      "episode 13370, avg_reward 1366.9, memory_length 2000, epsilon 0.0012501396673871197 total_time 721.0\n",
      "episode 13380, avg_reward 1328.7, memory_length 2000, epsilon 0.0012439045697839734 total_time 720.0\n",
      "episode 13390, avg_reward 1200.7, memory_length 2000, epsilon 0.0012377005698598572 total_time 724.0\n",
      "episode 13400, avg_reward 1247.3, memory_length 2000, epsilon 0.0012315275125144524 total_time 721.0\n",
      "Saving model weights for episode:13400\n",
      "episode 13410, avg_reward 1317.0, memory_length 2000, epsilon 0.0012253852434210026 total_time 721.0\n",
      "episode 13420, avg_reward 1292.9, memory_length 2000, epsilon 0.0012192736090224604 total_time 723.0\n",
      "episode 13430, avg_reward 1216.7, memory_length 2000, epsilon 0.0012131924565276476 total_time 720.0\n",
      "episode 13440, avg_reward 1201.8, memory_length 2000, epsilon 0.0012071416339074353 total_time 728.0\n",
      "episode 13450, avg_reward 1267.9, memory_length 2000, epsilon 0.0012011209898909426 total_time 720.0\n",
      "episode 13460, avg_reward 1180.6, memory_length 2000, epsilon 0.0011951303739617556 total_time 722.0\n",
      "episode 13470, avg_reward 1280.0, memory_length 2000, epsilon 0.0011891696363541641 total_time 722.0\n",
      "episode 13480, avg_reward 1421.9, memory_length 2000, epsilon 0.0011832386280494164 total_time 722.0\n",
      "episode 13490, avg_reward 1236.2, memory_length 2000, epsilon 0.001177337200771998 total_time 721.0\n",
      "episode 13500, avg_reward 1142.8, memory_length 2000, epsilon 0.0011714652069859186 total_time 722.0\n",
      "Saving model weights for episode:13500\n",
      "episode 13510, avg_reward 1066.3, memory_length 2000, epsilon 0.0011656224998910277 total_time 726.0\n",
      "episode 13520, avg_reward 1095.7, memory_length 2000, epsilon 0.0011598089334193438 total_time 725.0\n",
      "episode 13530, avg_reward 1168.9, memory_length 2000, epsilon 0.001154024362231402 total_time 720.0\n",
      "episode 13540, avg_reward 1042.1, memory_length 2000, epsilon 0.0011482686417126214 total_time 720.0\n",
      "episode 13550, avg_reward 1328.0, memory_length 2000, epsilon 0.0011425416279696894 total_time 725.0\n",
      "episode 13560, avg_reward 1266.1, memory_length 2000, epsilon 0.001136843177826963 total_time 726.0\n",
      "episode 13570, avg_reward 1324.4, memory_length 2000, epsilon 0.0011311731488228942 total_time 721.0\n",
      "episode 13580, avg_reward 1330.6, memory_length 2000, epsilon 0.001125531399206461 total_time 730.0\n",
      "episode 13590, avg_reward 1300.2, memory_length 2000, epsilon 0.0011199177879336294 total_time 720.0\n",
      "episode 13600, avg_reward 1294.8, memory_length 2000, epsilon 0.0011143321746638255 total_time 728.0\n",
      "Saving model weights for episode:13600\n",
      "episode 13610, avg_reward 1223.8, memory_length 2000, epsilon 0.001108774419756426 total_time 722.0\n",
      "episode 13620, avg_reward 1233.6, memory_length 2000, epsilon 0.0011032443842672694 total_time 722.0\n",
      "episode 13630, avg_reward 1279.2, memory_length 2000, epsilon 0.0010977419299451803 total_time 723.0\n",
      "episode 13640, avg_reward 1171.3, memory_length 2000, epsilon 0.0010922669192285127 total_time 720.0\n",
      "episode 13650, avg_reward 1159.0, memory_length 2000, epsilon 0.0010868192152417159 total_time 720.0\n",
      "episode 13660, avg_reward 1268.0, memory_length 2000, epsilon 0.0010813986817919052 total_time 724.0\n",
      "episode 13670, avg_reward 1378.2, memory_length 2000, epsilon 0.0010760051833654622 total_time 726.0\n",
      "episode 13680, avg_reward 1302.8, memory_length 2000, epsilon 0.0010706385851246454 total_time 721.0\n",
      "episode 13690, avg_reward 1254.8, memory_length 2000, epsilon 0.0010652987529042191 total_time 725.0\n",
      "episode 13700, avg_reward 1225.4, memory_length 2000, epsilon 0.0010599855532081 total_time 725.0\n",
      "Saving model weights for episode:13700\n",
      "episode 13710, avg_reward 1263.4, memory_length 2000, epsilon 0.0010546988532060186 total_time 726.0\n",
      "episode 13720, avg_reward 1372.6, memory_length 2000, epsilon 0.0010494385207301996 total_time 722.0\n",
      "episode 13730, avg_reward 1086.6, memory_length 2000, epsilon 0.001044204424272056 total_time 722.0\n",
      "episode 13740, avg_reward 1195.9, memory_length 2000, epsilon 0.0010389964329789061 total_time 720.0\n",
      "episode 13750, avg_reward 1317.7, memory_length 2000, epsilon 0.0010338144166506952 total_time 722.0\n",
      "episode 13760, avg_reward 1145.8, memory_length 2000, epsilon 0.001028658245736745 total_time 725.0\n",
      "episode 13770, avg_reward 1268.1, memory_length 2000, epsilon 0.001023527791332514 total_time 726.0\n",
      "episode 13780, avg_reward 1334.6, memory_length 2000, epsilon 0.0010184229251763753 total_time 720.0\n",
      "episode 13790, avg_reward 1408.2, memory_length 2000, epsilon 0.001013343519646409 total_time 725.0\n",
      "episode 13800, avg_reward 1307.4, memory_length 2000, epsilon 0.001008289447757212 total_time 722.0\n",
      "Saving model weights for episode:13800\n",
      "episode 13810, avg_reward 1336.3, memory_length 2000, epsilon 0.0010032605831567234 total_time 720.0\n",
      "episode 13820, avg_reward 1446.4, memory_length 2000, epsilon 0.0009982568001230675 total_time 723.0\n",
      "episode 13830, avg_reward 1306.3, memory_length 2000, epsilon 0.0009932779735614076 total_time 728.0\n",
      "episode 13840, avg_reward 1342.5, memory_length 2000, epsilon 0.0009883239790008198 total_time 723.0\n",
      "episode 13850, avg_reward 1379.0, memory_length 2000, epsilon 0.0009833946925911822 total_time 720.0\n",
      "episode 13860, avg_reward 1443.4, memory_length 2000, epsilon 0.000978489991100078 total_time 721.0\n",
      "episode 13870, avg_reward 1361.4, memory_length 2000, epsilon 0.0009736097519097145 total_time 724.0\n",
      "episode 13880, avg_reward 1385.1, memory_length 2000, epsilon 0.0009687538530138574 total_time 723.0\n",
      "episode 13890, avg_reward 1453.1, memory_length 2000, epsilon 0.0009639221730147809 total_time 732.0\n",
      "episode 13900, avg_reward 1279.3, memory_length 2000, epsilon 0.0009591145911202349 total_time 725.0\n",
      "Saving model weights for episode:13900\n",
      "episode 13910, avg_reward 1184.0, memory_length 2000, epsilon 0.0009543309871404208 total_time 720.0\n",
      "episode 13920, avg_reward 1350.6, memory_length 2000, epsilon 0.0009495712414849901 total_time 724.0\n",
      "episode 13930, avg_reward 1344.0, memory_length 2000, epsilon 0.0009448352351600533 total_time 726.0\n",
      "episode 13940, avg_reward 1413.4, memory_length 2000, epsilon 0.0009401228497652057 total_time 720.0\n",
      "episode 13950, avg_reward 1176.7, memory_length 2000, epsilon 0.0009354339674905671 total_time 721.0\n",
      "episode 13960, avg_reward 1232.2, memory_length 2000, epsilon 0.0009307684711138362 total_time 726.0\n",
      "episode 13970, avg_reward 1205.0, memory_length 2000, epsilon 0.0009261262439973608 total_time 724.0\n",
      "episode 13980, avg_reward 1282.5, memory_length 2000, epsilon 0.0009215071700852202 total_time 723.0\n",
      "episode 13990, avg_reward 1240.8, memory_length 2000, epsilon 0.0009169111339003279 total_time 720.0\n",
      "episode 14000, avg_reward 1153.9, memory_length 2000, epsilon 0.0009123380205415388 total_time 720.0\n",
      "Saving model weights for episode:14000\n",
      "episode 14010, avg_reward 1184.7, memory_length 2000, epsilon 0.000907787715680781 total_time 720.0\n",
      "episode 14020, avg_reward 1114.2, memory_length 2000, epsilon 0.0009032601055601958 total_time 723.0\n",
      "episode 14030, avg_reward 1204.1, memory_length 2000, epsilon 0.0008987550769892944 total_time 722.0\n",
      "episode 14040, avg_reward 1134.2, memory_length 2000, epsilon 0.000894272517342128 total_time 720.0\n",
      "episode 14050, avg_reward 1211.5, memory_length 2000, epsilon 0.0008898123145544719 total_time 725.0\n",
      "episode 14060, avg_reward 1333.3, memory_length 2000, epsilon 0.0008853743571210233 total_time 722.0\n",
      "episode 14070, avg_reward 1467.3, memory_length 2000, epsilon 0.0008809585340926167 total_time 724.0\n",
      "episode 14080, avg_reward 1259.6, memory_length 2000, epsilon 0.0008765647350734458 total_time 723.0\n",
      "episode 14090, avg_reward 1337.5, memory_length 2000, epsilon 0.0008721928502183062 total_time 720.0\n",
      "episode 14100, avg_reward 1219.6, memory_length 2000, epsilon 0.0008678427702298487 total_time 720.0\n",
      "Saving model weights for episode:14100\n",
      "episode 14110, avg_reward 1164.2, memory_length 2000, epsilon 0.0008635143863558472 total_time 727.0\n",
      "episode 14120, avg_reward 1120.8, memory_length 2000, epsilon 0.0008592075903864794 total_time 722.0\n",
      "episode 14130, avg_reward 1088.1, memory_length 2000, epsilon 0.0008549222746516216 total_time 724.0\n",
      "episode 14140, avg_reward 1347.4, memory_length 2000, epsilon 0.0008506583320181566 total_time 724.0\n",
      "episode 14150, avg_reward 1255.4, memory_length 2000, epsilon 0.000846415655887298 total_time 724.0\n",
      "episode 14160, avg_reward 1191.3, memory_length 2000, epsilon 0.0008421941401919207 total_time 724.0\n",
      "episode 14170, avg_reward 1292.8, memory_length 2000, epsilon 0.0008379936793939125 total_time 729.0\n",
      "episode 14180, avg_reward 1095.7, memory_length 2000, epsilon 0.0008338141684815348 total_time 724.0\n",
      "episode 14190, avg_reward 1288.9, memory_length 2000, epsilon 0.000829655502966797 total_time 722.0\n",
      "episode 14200, avg_reward 1269.7, memory_length 2000, epsilon 0.0008255175788828446 total_time 725.0\n",
      "Saving model weights for episode:14200\n",
      "episode 14210, avg_reward 1152.0, memory_length 2000, epsilon 0.0008214002927813599 total_time 723.0\n",
      "episode 14220, avg_reward 1223.8, memory_length 2000, epsilon 0.0008173035417299761 total_time 726.0\n",
      "episode 14230, avg_reward 1071.6, memory_length 2000, epsilon 0.0008132272233097028 total_time 724.0\n",
      "episode 14240, avg_reward 1219.8, memory_length 2000, epsilon 0.0008091712356123685 total_time 720.0\n",
      "episode 14250, avg_reward 1070.2, memory_length 2000, epsilon 0.000805135477238069 total_time 723.0\n",
      "episode 14260, avg_reward 1046.8, memory_length 2000, epsilon 0.0008011198472926346 total_time 720.0\n",
      "episode 14270, avg_reward 1205.5, memory_length 2000, epsilon 0.0007971242453851075 total_time 727.0\n",
      "episode 14280, avg_reward 1119.8, memory_length 2000, epsilon 0.0007931485716252319 total_time 722.0\n",
      "episode 14290, avg_reward 1141.1, memory_length 2000, epsilon 0.0007891927266209568 total_time 725.0\n",
      "episode 14300, avg_reward 1160.9, memory_length 2000, epsilon 0.0007852566114759512 total_time 721.0\n",
      "Saving model weights for episode:14300\n",
      "episode 14310, avg_reward 1178.8, memory_length 2000, epsilon 0.0007813401277871304 total_time 723.0\n",
      "episode 14320, avg_reward 1323.8, memory_length 2000, epsilon 0.0007774431776421999 total_time 733.0\n",
      "episode 14330, avg_reward 1200.6, memory_length 2000, epsilon 0.0007735656636172024 total_time 722.0\n",
      "episode 14340, avg_reward 1224.8, memory_length 2000, epsilon 0.0007697074887740851 total_time 720.0\n",
      "episode 14350, avg_reward 1192.8, memory_length 2000, epsilon 0.0007658685566582762 total_time 728.0\n",
      "episode 14360, avg_reward 1263.8, memory_length 2000, epsilon 0.0007620487712962728 total_time 720.0\n",
      "episode 14370, avg_reward 1114.7, memory_length 2000, epsilon 0.0007582480371932419 total_time 722.0\n",
      "episode 14380, avg_reward 1376.1, memory_length 2000, epsilon 0.0007544662593306328 total_time 721.0\n",
      "episode 14390, avg_reward 1397.8, memory_length 2000, epsilon 0.0007507033431638015 total_time 723.0\n",
      "episode 14400, avg_reward 1395.8, memory_length 2000, epsilon 0.0007469591946196492 total_time 729.0\n",
      "Saving model weights for episode:14400\n",
      "episode 14410, avg_reward 1305.2, memory_length 2000, epsilon 0.0007432337200942666 total_time 720.0\n",
      "episode 14420, avg_reward 1346.9, memory_length 2000, epsilon 0.0007395268264505963 total_time 725.0\n",
      "episode 14430, avg_reward 1273.7, memory_length 2000, epsilon 0.0007358384210161044 total_time 721.0\n",
      "episode 14440, avg_reward 1457.9, memory_length 2000, epsilon 0.0007321684115804629 total_time 720.0\n",
      "episode 14450, avg_reward 1325.0, memory_length 2000, epsilon 0.0007285167063932447 total_time 721.0\n",
      "episode 14460, avg_reward 1450.9, memory_length 2000, epsilon 0.0007248832141616299 total_time 720.0\n",
      "episode 14470, avg_reward 1311.3, memory_length 2000, epsilon 0.0007212678440481235 total_time 721.0\n",
      "episode 14480, avg_reward 1251.6, memory_length 2000, epsilon 0.0007176705056682838 total_time 729.0\n",
      "episode 14490, avg_reward 1252.0, memory_length 2000, epsilon 0.0007140911090884651 total_time 724.0\n",
      "episode 14500, avg_reward 1295.3, memory_length 2000, epsilon 0.0007105295648235658 total_time 721.0\n",
      "Saving model weights for episode:14500\n",
      "episode 14510, avg_reward 1316.7, memory_length 2000, epsilon 0.0007069857838347941 total_time 722.0\n",
      "episode 14520, avg_reward 1366.1, memory_length 2000, epsilon 0.0007034596775274403 total_time 722.0\n",
      "episode 14530, avg_reward 1387.6, memory_length 2000, epsilon 0.0006999511577486633 total_time 725.0\n",
      "episode 14540, avg_reward 1324.8, memory_length 2000, epsilon 0.0006964601367852858 total_time 723.0\n",
      "episode 14550, avg_reward 1233.1, memory_length 2000, epsilon 0.0006929865273616019 total_time 720.0\n",
      "episode 14560, avg_reward 1376.3, memory_length 2000, epsilon 0.0006895302426371946 total_time 722.0\n",
      "episode 14570, avg_reward 1306.7, memory_length 2000, epsilon 0.0006860911962047668 total_time 721.0\n",
      "episode 14580, avg_reward 1419.7, memory_length 2000, epsilon 0.0006826693020879781 total_time 721.0\n",
      "episode 14590, avg_reward 1285.9, memory_length 2000, epsilon 0.0006792644747392974 total_time 721.0\n",
      "episode 14600, avg_reward 1377.5, memory_length 2000, epsilon 0.0006758766290378635 total_time 729.0\n",
      "Saving model weights for episode:14600\n",
      "episode 14610, avg_reward 1292.4, memory_length 2000, epsilon 0.0006725056802873575 total_time 726.0\n",
      "episode 14620, avg_reward 1374.4, memory_length 2000, epsilon 0.000669151544213885 total_time 720.0\n",
      "episode 14630, avg_reward 1213.8, memory_length 2000, epsilon 0.0006658141369638695 total_time 726.0\n",
      "episode 14640, avg_reward 1477.3, memory_length 2000, epsilon 0.0006624933751019555 total_time 723.0\n",
      "episode 14650, avg_reward 1209.5, memory_length 2000, epsilon 0.0006591891756089244 total_time 721.0\n",
      "episode 14660, avg_reward 1177.5, memory_length 2000, epsilon 0.0006559014558796164 total_time 727.0\n",
      "episode 14670, avg_reward 1229.4, memory_length 2000, epsilon 0.0006526301337208669 total_time 724.0\n",
      "episode 14680, avg_reward 1218.8, memory_length 2000, epsilon 0.0006493751273494516 total_time 724.0\n",
      "episode 14690, avg_reward 951.7, memory_length 2000, epsilon 0.0006461363553900418 total_time 727.0\n",
      "episode 14700, avg_reward 922.3, memory_length 2000, epsilon 0.0006429137368731695 total_time 720.0\n",
      "Saving model weights for episode:14700\n",
      "episode 14710, avg_reward 1261.4, memory_length 2000, epsilon 0.0006397071912332043 total_time 721.0\n",
      "episode 14720, avg_reward 1306.4, memory_length 2000, epsilon 0.0006365166383063374 total_time 727.0\n",
      "episode 14730, avg_reward 1241.9, memory_length 2000, epsilon 0.0006333419983285808 total_time 727.0\n",
      "episode 14740, avg_reward 1177.4, memory_length 2000, epsilon 0.0006301831919337689 total_time 721.0\n",
      "episode 14750, avg_reward 1199.7, memory_length 2000, epsilon 0.0006270401401515775 total_time 725.0\n",
      "episode 14760, avg_reward 1217.9, memory_length 2000, epsilon 0.0006239127644055481 total_time 727.0\n",
      "episode 14770, avg_reward 1333.4, memory_length 2000, epsilon 0.0006208009865111243 total_time 721.0\n",
      "episode 14780, avg_reward 1383.8, memory_length 2000, epsilon 0.0006177047286736969 total_time 720.0\n",
      "episode 14790, avg_reward 1416.3, memory_length 2000, epsilon 0.0006146239134866583 total_time 720.0\n",
      "episode 14800, avg_reward 1348.9, memory_length 2000, epsilon 0.0006115584639294686 total_time 726.0\n",
      "Saving model weights for episode:14800\n",
      "episode 14810, avg_reward 1347.5, memory_length 2000, epsilon 0.0006085083033657287 total_time 730.0\n",
      "episode 14820, avg_reward 1414.6, memory_length 2000, epsilon 0.0006054733555412666 total_time 720.0\n",
      "episode 14830, avg_reward 1408.2, memory_length 2000, epsilon 0.0006024535445822281 total_time 724.0\n",
      "episode 14840, avg_reward 1339.9, memory_length 2000, epsilon 0.0005994487949931821 total_time 730.0\n",
      "episode 14850, avg_reward 1282.1, memory_length 2000, epsilon 0.0005964590316552322 total_time 722.0\n",
      "episode 14860, avg_reward 1326.7, memory_length 2000, epsilon 0.0005934841798241391 total_time 730.0\n",
      "episode 14870, avg_reward 1321.0, memory_length 2000, epsilon 0.0005905241651284525 total_time 728.0\n",
      "episode 14880, avg_reward 1381.4, memory_length 2000, epsilon 0.0005875789135676504 total_time 729.0\n",
      "episode 14890, avg_reward 1322.8, memory_length 2000, epsilon 0.0005846483515102903 total_time 720.0\n",
      "episode 14900, avg_reward 1355.3, memory_length 2000, epsilon 0.0005817324056921687 total_time 721.0\n",
      "Saving model weights for episode:14900\n",
      "episode 14910, avg_reward 1346.5, memory_length 2000, epsilon 0.0005788310032144882 total_time 721.0\n",
      "episode 14920, avg_reward 1294.0, memory_length 2000, epsilon 0.0005759440715420353 total_time 727.0\n",
      "episode 14930, avg_reward 1342.8, memory_length 2000, epsilon 0.0005730715385013683 total_time 721.0\n",
      "episode 14940, avg_reward 1198.6, memory_length 2000, epsilon 0.0005702133322790111 total_time 723.0\n",
      "episode 14950, avg_reward 1306.8, memory_length 2000, epsilon 0.0005673693814196597 total_time 722.0\n",
      "episode 14960, avg_reward 1309.9, memory_length 2000, epsilon 0.0005645396148243943 total_time 723.0\n",
      "episode 14970, avg_reward 1323.5, memory_length 2000, epsilon 0.0005617239617489021 total_time 721.0\n",
      "episode 14980, avg_reward 1300.5, memory_length 2000, epsilon 0.0005589223518017105 total_time 723.0\n",
      "episode 14990, avg_reward 1336.4, memory_length 2000, epsilon 0.0005561347149424246 total_time 720.0\n",
      "episode 15000, avg_reward 1255.9, memory_length 2000, epsilon 0.0005533609814799776 total_time 721.0\n",
      "Saving model weights for episode:15000\n",
      "episode 15010, avg_reward 1268.7, memory_length 2000, epsilon 0.0005506010820708886 total_time 723.0\n",
      "episode 15020, avg_reward 1328.6, memory_length 2000, epsilon 0.0005478549477175284 total_time 720.0\n",
      "episode 15030, avg_reward 1350.6, memory_length 2000, epsilon 0.0005451225097663954 total_time 727.0\n",
      "episode 15040, avg_reward 1232.2, memory_length 2000, epsilon 0.0005424036999063984 total_time 720.0\n",
      "episode 15050, avg_reward 1315.1, memory_length 2000, epsilon 0.0005396984501671491 total_time 722.0\n",
      "episode 15060, avg_reward 1397.9, memory_length 2000, epsilon 0.0005370066929172629 total_time 725.0\n",
      "episode 15070, avg_reward 1396.8, memory_length 2000, epsilon 0.0005343283608626693 total_time 720.0\n",
      "episode 15080, avg_reward 1248.0, memory_length 2000, epsilon 0.0005316633870449269 total_time 726.0\n",
      "episode 15090, avg_reward 1316.5, memory_length 2000, epsilon 0.0005290117048395513 total_time 724.0\n",
      "episode 15100, avg_reward 1277.6, memory_length 2000, epsilon 0.0005263732479543494 total_time 720.0\n",
      "Saving model weights for episode:15100\n",
      "episode 15110, avg_reward 1276.4, memory_length 2000, epsilon 0.0005237479504277619 total_time 727.0\n",
      "episode 15120, avg_reward 1385.7, memory_length 2000, epsilon 0.0005211357466272134 total_time 724.0\n",
      "episode 15130, avg_reward 1345.0, memory_length 2000, epsilon 0.0005185365712474733 total_time 733.0\n",
      "episode 15140, avg_reward 1371.7, memory_length 2000, epsilon 0.0005159503593090208 total_time 720.0\n",
      "episode 15150, avg_reward 1390.9, memory_length 2000, epsilon 0.0005133770461564241 total_time 721.0\n",
      "episode 15160, avg_reward 1393.8, memory_length 2000, epsilon 0.0005108165674567197 total_time 720.0\n",
      "episode 15170, avg_reward 1309.5, memory_length 2000, epsilon 0.0005082688591978068 total_time 730.0\n",
      "episode 15180, avg_reward 1216.1, memory_length 2000, epsilon 0.0005057338576868462 total_time 721.0\n",
      "episode 15190, avg_reward 1228.2, memory_length 2000, epsilon 0.0005032114995486681 total_time 720.0\n",
      "episode 15200, avg_reward 1263.3, memory_length 2000, epsilon 0.0005007017217241876 total_time 725.0\n",
      "Saving model weights for episode:15200\n",
      "episode 15210, avg_reward 1125.5, memory_length 2000, epsilon 0.0004982044614688285 total_time 728.0\n",
      "episode 15220, avg_reward 1176.5, memory_length 2000, epsilon 0.0004957196563509538 total_time 726.0\n",
      "episode 15230, avg_reward 1251.6, memory_length 2000, epsilon 0.0004932472442503071 total_time 723.0\n",
      "episode 15240, avg_reward 1283.8, memory_length 2000, epsilon 0.0004907871633564566 total_time 724.0\n",
      "episode 15250, avg_reward 1292.4, memory_length 2000, epsilon 0.0004883393521672519 total_time 721.0\n",
      "episode 15260, avg_reward 1268.4, memory_length 2000, epsilon 0.00048590374948728575 total_time 720.0\n",
      "episode 15270, avg_reward 1234.2, memory_length 2000, epsilon 0.0004834802944263643 total_time 726.0\n",
      "episode 15280, avg_reward 1331.3, memory_length 2000, epsilon 0.00048106892639798477 total_time 722.0\n",
      "episode 15290, avg_reward 1272.6, memory_length 2000, epsilon 0.00047866958511782093 total_time 720.0\n",
      "episode 15300, avg_reward 1303.3, memory_length 2000, epsilon 0.0004762822106022157 total_time 730.0\n",
      "Saving model weights for episode:15300\n",
      "episode 15310, avg_reward 1298.9, memory_length 2000, epsilon 0.0004739067431666815 total_time 724.0\n",
      "episode 15320, avg_reward 1371.1, memory_length 2000, epsilon 0.0004715431234244096 total_time 725.0\n",
      "episode 15330, avg_reward 1320.3, memory_length 2000, epsilon 0.00046919129228478284 total_time 724.0\n",
      "episode 15340, avg_reward 1379.9, memory_length 2000, epsilon 0.00046685119095190024 total_time 724.0\n",
      "episode 15350, avg_reward 1361.7, memory_length 2000, epsilon 0.0004645227609231066 total_time 722.0\n",
      "episode 15360, avg_reward 1295.5, memory_length 2000, epsilon 0.00046220594398753005 total_time 725.0\n",
      "episode 15370, avg_reward 1196.5, memory_length 2000, epsilon 0.00045990068222462636 total_time 720.0\n",
      "episode 15380, avg_reward 1221.4, memory_length 2000, epsilon 0.0004576069180027315 total_time 724.0\n",
      "episode 15390, avg_reward 1260.8, memory_length 2000, epsilon 0.00045532459397762 total_time 720.0\n",
      "episode 15400, avg_reward 1378.3, memory_length 2000, epsilon 0.00045305365309107317 total_time 720.0\n",
      "Saving model weights for episode:15400\n",
      "episode 15410, avg_reward 1383.5, memory_length 2000, epsilon 0.0004507940385694502 total_time 721.0\n",
      "episode 15420, avg_reward 1248.3, memory_length 2000, epsilon 0.00044854569392227035 total_time 724.0\n",
      "episode 15430, avg_reward 1254.9, memory_length 2000, epsilon 0.00044630856294080034 total_time 724.0\n",
      "episode 15440, avg_reward 1345.6, memory_length 2000, epsilon 0.0004440825896966491 total_time 720.0\n",
      "episode 15450, avg_reward 1434.4, memory_length 2000, epsilon 0.0004418677185403696 total_time 728.0\n",
      "episode 15460, avg_reward 1294.2, memory_length 2000, epsilon 0.00043966389410006755 total_time 726.0\n",
      "episode 15470, avg_reward 1272.4, memory_length 2000, epsilon 0.0004374710612800168 total_time 722.0\n",
      "episode 15480, avg_reward 1105.7, memory_length 2000, epsilon 0.0004352891652592834 total_time 722.0\n",
      "episode 15490, avg_reward 1206.9, memory_length 2000, epsilon 0.00043311815149035284 total_time 720.0\n",
      "episode 15500, avg_reward 1276.4, memory_length 2000, epsilon 0.0004309579656977678 total_time 720.0\n",
      "Saving model weights for episode:15500\n",
      "episode 15510, avg_reward 1229.4, memory_length 2000, epsilon 0.00042880855387677087 total_time 723.0\n",
      "episode 15520, avg_reward 1101.6, memory_length 2000, epsilon 0.0004266698622919547 total_time 724.0\n",
      "episode 15530, avg_reward 1279.9, memory_length 2000, epsilon 0.00042454183747591827 total_time 723.0\n",
      "episode 15540, avg_reward 1315.9, memory_length 2000, epsilon 0.0004224244262279303 total_time 723.0\n",
      "episode 15550, avg_reward 1286.4, memory_length 2000, epsilon 0.00042031757561259933 total_time 729.0\n",
      "episode 15560, avg_reward 1286.8, memory_length 2000, epsilon 0.00041822123295854984 total_time 721.0\n",
      "episode 15570, avg_reward 1107.7, memory_length 2000, epsilon 0.00041613534585710705 total_time 726.0\n",
      "episode 15580, avg_reward 1220.2, memory_length 2000, epsilon 0.0004140598621609844 total_time 723.0\n",
      "episode 15590, avg_reward 1299.0, memory_length 2000, epsilon 0.00041199472998298145 total_time 722.0\n",
      "episode 15600, avg_reward 1427.0, memory_length 2000, epsilon 0.00040993989769468617 total_time 725.0\n",
      "Saving model weights for episode:15600\n",
      "episode 15610, avg_reward 1346.6, memory_length 2000, epsilon 0.00040789531392518427 total_time 721.0\n",
      "episode 15620, avg_reward 1392.4, memory_length 2000, epsilon 0.00040586092755977505 total_time 722.0\n",
      "episode 15630, avg_reward 1188.1, memory_length 2000, epsilon 0.0004038366877386935 total_time 720.0\n",
      "episode 15640, avg_reward 1191.5, memory_length 2000, epsilon 0.00040182254385583816 total_time 726.0\n",
      "episode 15650, avg_reward 1254.2, memory_length 2000, epsilon 0.0003998184455575079 total_time 724.0\n",
      "episode 15660, avg_reward 1243.5, memory_length 2000, epsilon 0.00039782434274114045 total_time 724.0\n",
      "episode 15670, avg_reward 1305.4, memory_length 2000, epsilon 0.0003958401855540616 total_time 720.0\n",
      "episode 15680, avg_reward 1305.8, memory_length 2000, epsilon 0.00039386592439223824 total_time 726.0\n",
      "episode 15690, avg_reward 1309.4, memory_length 2000, epsilon 0.0003919015098990386 total_time 727.0\n",
      "episode 15700, avg_reward 1279.4, memory_length 2000, epsilon 0.00038994689296399795 total_time 721.0\n",
      "Saving model weights for episode:15700\n",
      "episode 15710, avg_reward 1372.4, memory_length 2000, epsilon 0.0003880020247215912 total_time 720.0\n",
      "episode 15720, avg_reward 1402.0, memory_length 2000, epsilon 0.00038606685655001057 total_time 720.0\n",
      "episode 15730, avg_reward 1259.1, memory_length 2000, epsilon 0.00038414134006995164 total_time 723.0\n",
      "episode 15740, avg_reward 1241.8, memory_length 2000, epsilon 0.0003822254271434019 total_time 723.0\n",
      "episode 15750, avg_reward 1284.0, memory_length 2000, epsilon 0.00038031906987243835 total_time 723.0\n",
      "episode 15760, avg_reward 1279.1, memory_length 2000, epsilon 0.0003784222205980299 total_time 721.0\n",
      "episode 15770, avg_reward 1349.6, memory_length 2000, epsilon 0.0003765348318988459 total_time 724.0\n",
      "episode 15780, avg_reward 1414.5, memory_length 2000, epsilon 0.0003746568565900706 total_time 722.0\n",
      "episode 15790, avg_reward 1385.1, memory_length 2000, epsilon 0.0003727882477222234 total_time 725.0\n",
      "episode 15800, avg_reward 1339.5, memory_length 2000, epsilon 0.0003709289585799854 total_time 723.0\n",
      "Saving model weights for episode:15800\n",
      "episode 15810, avg_reward 1426.4, memory_length 2000, epsilon 0.00036907894268103073 total_time 722.0\n",
      "episode 15820, avg_reward 1372.3, memory_length 2000, epsilon 0.0003672381537748663 total_time 730.0\n",
      "episode 15830, avg_reward 1404.0, memory_length 2000, epsilon 0.00036540654584167327 total_time 726.0\n",
      "episode 15840, avg_reward 1426.9, memory_length 2000, epsilon 0.0003635840730911579 total_time 720.0\n",
      "episode 15850, avg_reward 1506.6, memory_length 2000, epsilon 0.00036177068996140645 total_time 729.0\n",
      "episode 15860, avg_reward 1419.9, memory_length 2000, epsilon 0.0003599663511177463 total_time 729.0\n",
      "episode 15870, avg_reward 1337.7, memory_length 2000, epsilon 0.00035817101145161235 total_time 723.0\n",
      "episode 15880, avg_reward 1273.3, memory_length 2000, epsilon 0.00035638462607941944 total_time 721.0\n",
      "episode 15890, avg_reward 1371.0, memory_length 2000, epsilon 0.00035460715034143993 total_time 720.0\n",
      "episode 15900, avg_reward 1271.3, memory_length 2000, epsilon 0.0003528385398006884 total_time 721.0\n",
      "Saving model weights for episode:15900\n",
      "episode 15910, avg_reward 1278.9, memory_length 2000, epsilon 0.00035107875024180895 total_time 725.0\n",
      "episode 15920, avg_reward 1217.8, memory_length 2000, epsilon 0.00034932773766997087 total_time 722.0\n",
      "episode 15930, avg_reward 1272.4, memory_length 2000, epsilon 0.0003475854583097687 total_time 725.0\n",
      "episode 15940, avg_reward 1201.0, memory_length 2000, epsilon 0.0003458518686041277 total_time 725.0\n",
      "episode 15950, avg_reward 1280.1, memory_length 2000, epsilon 0.00034412692521321495 total_time 728.0\n",
      "episode 15960, avg_reward 1272.1, memory_length 2000, epsilon 0.0003424105850133558 total_time 722.0\n",
      "episode 15970, avg_reward 1196.6, memory_length 2000, epsilon 0.00034070280509595563 total_time 726.0\n",
      "episode 15980, avg_reward 1191.1, memory_length 2000, epsilon 0.0003390035427664281 total_time 725.0\n",
      "episode 15990, avg_reward 1213.3, memory_length 2000, epsilon 0.00033731275554312615 total_time 720.0\n",
      "episode 16000, avg_reward 1165.0, memory_length 2000, epsilon 0.00033563040115628116 total_time 720.0\n",
      "Saving model weights for episode:16000\n",
      "episode 16010, avg_reward 1294.1, memory_length 2000, epsilon 0.0003339564375469459 total_time 721.0\n",
      "episode 16020, avg_reward 1263.2, memory_length 2000, epsilon 0.00033229082286594254 total_time 720.0\n",
      "episode 16030, avg_reward 1183.7, memory_length 2000, epsilon 0.00033063351547281823 total_time 727.0\n",
      "episode 16040, avg_reward 1148.5, memory_length 2000, epsilon 0.00032898447393480074 total_time 724.0\n",
      "episode 16050, avg_reward 1308.6, memory_length 2000, epsilon 0.00032734365702576676 total_time 720.0\n",
      "episode 16060, avg_reward 1255.7, memory_length 2000, epsilon 0.00032571102372520703 total_time 720.0\n",
      "episode 16070, avg_reward 1267.7, memory_length 2000, epsilon 0.00032408653321720516 total_time 729.0\n",
      "episode 16080, avg_reward 1245.5, memory_length 2000, epsilon 0.00032247014488941264 total_time 729.0\n",
      "episode 16090, avg_reward 1253.8, memory_length 2000, epsilon 0.00032086181833203823 total_time 721.0\n",
      "episode 16100, avg_reward 1190.8, memory_length 2000, epsilon 0.00031926151333683316 total_time 720.0\n",
      "Saving model weights for episode:16100\n",
      "episode 16110, avg_reward 1116.5, memory_length 2000, epsilon 0.0003176691898960897 total_time 723.0\n",
      "episode 16120, avg_reward 1205.9, memory_length 2000, epsilon 0.00031608480820163957 total_time 720.0\n",
      "episode 16130, avg_reward 1263.0, memory_length 2000, epsilon 0.0003145083286438566 total_time 722.0\n",
      "episode 16140, avg_reward 1269.8, memory_length 2000, epsilon 0.000312939711810671 total_time 721.0\n",
      "episode 16150, avg_reward 1184.8, memory_length 2000, epsilon 0.000311378918486579 total_time 728.0\n",
      "episode 16160, avg_reward 1212.8, memory_length 2000, epsilon 0.0003098259096516674 total_time 720.0\n",
      "episode 16170, avg_reward 1200.1, memory_length 2000, epsilon 0.00030828064648063337 total_time 722.0\n",
      "episode 16180, avg_reward 1208.1, memory_length 2000, epsilon 0.00030674309034181763 total_time 720.0\n",
      "episode 16190, avg_reward 1219.7, memory_length 2000, epsilon 0.0003052132027962372 total_time 720.0\n",
      "episode 16200, avg_reward 1254.6, memory_length 2000, epsilon 0.0003036909455966226 total_time 726.0\n",
      "Saving model weights for episode:16200\n",
      "episode 16210, avg_reward 1283.2, memory_length 2000, epsilon 0.0003021762806864658 total_time 721.0\n",
      "episode 16220, avg_reward 1243.4, memory_length 2000, epsilon 0.0003006691701990639 total_time 726.0\n",
      "episode 16230, avg_reward 1289.4, memory_length 2000, epsilon 0.00029916957645657747 total_time 727.0\n",
      "episode 16240, avg_reward 1176.1, memory_length 2000, epsilon 0.00029767746196908364 total_time 724.0\n",
      "episode 16250, avg_reward 1198.3, memory_length 2000, epsilon 0.0002961927894336436 total_time 723.0\n",
      "episode 16260, avg_reward 1267.9, memory_length 2000, epsilon 0.00029471552173336563 total_time 727.0\n",
      "episode 16270, avg_reward 1244.3, memory_length 2000, epsilon 0.00029324562193648075 total_time 720.0\n",
      "episode 16280, avg_reward 1298.5, memory_length 2000, epsilon 0.000291783053295418 total_time 729.0\n",
      "episode 16290, avg_reward 1276.4, memory_length 2000, epsilon 0.0002903277792458842 total_time 720.0\n",
      "episode 16300, avg_reward 1272.3, memory_length 2000, epsilon 0.0002888797634059533 total_time 723.0\n",
      "Saving model weights for episode:16300\n",
      "episode 16310, avg_reward 1184.4, memory_length 2000, epsilon 0.00028743896957515285 total_time 723.0\n",
      "episode 16320, avg_reward 1180.4, memory_length 2000, epsilon 0.0002860053617335631 total_time 720.0\n",
      "episode 16330, avg_reward 1279.2, memory_length 2000, epsilon 0.00028457890404091235 total_time 727.0\n",
      "episode 16340, avg_reward 1129.4, memory_length 2000, epsilon 0.0002831595608356849 total_time 723.0\n",
      "episode 16350, avg_reward 1227.7, memory_length 2000, epsilon 0.00028174729663422575 total_time 724.0\n",
      "episode 16360, avg_reward 1245.2, memory_length 2000, epsilon 0.00028034207612985686 total_time 720.0\n",
      "episode 16370, avg_reward 1312.3, memory_length 2000, epsilon 0.0002789438641919928 total_time 722.0\n",
      "episode 16380, avg_reward 1216.9, memory_length 2000, epsilon 0.00027755262586526145 total_time 722.0\n",
      "episode 16390, avg_reward 1248.0, memory_length 2000, epsilon 0.00027616832636863305 total_time 726.0\n",
      "episode 16400, avg_reward 1336.1, memory_length 2000, epsilon 0.0002747909310945472 total_time 726.0\n",
      "Saving model weights for episode:16400\n",
      "episode 16410, avg_reward 1054.8, memory_length 2000, epsilon 0.0002734204056080512 total_time 721.0\n",
      "episode 16420, avg_reward 1067.6, memory_length 2000, epsilon 0.0002720567156459356 total_time 722.0\n",
      "episode 16430, avg_reward 1052.3, memory_length 2000, epsilon 0.0002706998271158808 total_time 728.0\n",
      "episode 16440, avg_reward 1070.3, memory_length 2000, epsilon 0.00026934970609560326 total_time 724.0\n",
      "episode 16450, avg_reward 1237.6, memory_length 2000, epsilon 0.00026800631883200636 total_time 720.0\n",
      "episode 16460, avg_reward 1300.5, memory_length 2000, epsilon 0.0002666696317403394 total_time 725.0\n",
      "episode 16470, avg_reward 1215.0, memory_length 2000, epsilon 0.00026533961140335445 total_time 720.0\n",
      "episode 16480, avg_reward 1157.2, memory_length 2000, epsilon 0.0002640162245704749 total_time 723.0\n",
      "episode 16490, avg_reward 1101.1, memory_length 2000, epsilon 0.00026269943815696 total_time 724.0\n",
      "episode 16500, avg_reward 1169.1, memory_length 2000, epsilon 0.00026138921924308175 total_time 723.0\n",
      "Saving model weights for episode:16500\n",
      "episode 16510, avg_reward 1304.2, memory_length 2000, epsilon 0.0002600855350732982 total_time 723.0\n",
      "episode 16520, avg_reward 1268.5, memory_length 2000, epsilon 0.0002587883530554375 total_time 725.0\n",
      "episode 16530, avg_reward 1284.4, memory_length 2000, epsilon 0.0002574976407598823 total_time 722.0\n",
      "episode 16540, avg_reward 1234.6, memory_length 2000, epsilon 0.000256213365918757 total_time 725.0\n",
      "episode 16550, avg_reward 1364.1, memory_length 2000, epsilon 0.00025493549642512456 total_time 721.0\n",
      "episode 16560, avg_reward 1227.0, memory_length 2000, epsilon 0.00025366400033218015 total_time 721.0\n",
      "episode 16570, avg_reward 1274.7, memory_length 2000, epsilon 0.00025239884585245625 total_time 723.0\n",
      "episode 16580, avg_reward 1226.8, memory_length 2000, epsilon 0.00025114000135702393 total_time 722.0\n",
      "episode 16590, avg_reward 1371.9, memory_length 2000, epsilon 0.00024988743537470625 total_time 720.0\n",
      "episode 16600, avg_reward 1282.3, memory_length 2000, epsilon 0.0002486411165912874 total_time 720.0\n",
      "Saving model weights for episode:16600\n",
      "episode 16610, avg_reward 1291.2, memory_length 2000, epsilon 0.0002474010138487335 total_time 726.0\n",
      "episode 16620, avg_reward 1352.5, memory_length 2000, epsilon 0.0002461670961444117 total_time 730.0\n",
      "episode 16630, avg_reward 1343.1, memory_length 2000, epsilon 0.00024493933263031425 total_time 722.0\n",
      "episode 16640, avg_reward 1284.1, memory_length 2000, epsilon 0.0002437176926122903 total_time 720.0\n",
      "episode 16650, avg_reward 1291.9, memory_length 2000, epsilon 0.00024250214554927488 total_time 722.0\n",
      "episode 16660, avg_reward 1220.6, memory_length 2000, epsilon 0.00024129266105252893 total_time 725.0\n",
      "episode 16670, avg_reward 1292.3, memory_length 2000, epsilon 0.0002400892088848762 total_time 720.0\n",
      "episode 16680, avg_reward 1229.7, memory_length 2000, epsilon 0.00023889175895995025 total_time 721.0\n",
      "episode 16690, avg_reward 1420.7, memory_length 2000, epsilon 0.000237700281341441 total_time 730.0\n",
      "episode 16700, avg_reward 1404.2, memory_length 2000, epsilon 0.00023651474624234513 total_time 725.0\n",
      "Saving model weights for episode:16700\n",
      "episode 16710, avg_reward 1220.1, memory_length 2000, epsilon 0.0002353351240242242 total_time 720.0\n",
      "episode 16720, avg_reward 1360.4, memory_length 2000, epsilon 0.0002341613851964605 total_time 720.0\n",
      "episode 16730, avg_reward 1339.5, memory_length 2000, epsilon 0.00023299350041552306 total_time 720.0\n",
      "episode 16740, avg_reward 1350.3, memory_length 2000, epsilon 0.00023183144048423068 total_time 722.0\n",
      "episode 16750, avg_reward 1369.7, memory_length 2000, epsilon 0.00023067517635102537 total_time 720.0\n",
      "episode 16760, avg_reward 1318.7, memory_length 2000, epsilon 0.00022952467910924276 total_time 727.0\n",
      "episode 16770, avg_reward 1399.5, memory_length 2000, epsilon 0.00022837991999639234 total_time 723.0\n",
      "episode 16780, avg_reward 1402.2, memory_length 2000, epsilon 0.000227240870393437 total_time 720.0\n",
      "episode 16790, avg_reward 1339.9, memory_length 2000, epsilon 0.0002261075018240766 total_time 725.0\n",
      "episode 16800, avg_reward 1298.8, memory_length 2000, epsilon 0.0002249797859540386 total_time 722.0\n",
      "Saving model weights for episode:16800\n",
      "episode 16810, avg_reward 1312.9, memory_length 2000, epsilon 0.00022385769459036677 total_time 731.0\n",
      "episode 16820, avg_reward 1350.5, memory_length 2000, epsilon 0.00022274119968071937 total_time 722.0\n",
      "episode 16830, avg_reward 1448.9, memory_length 2000, epsilon 0.00022163027331266471 total_time 726.0\n",
      "episode 16840, avg_reward 1453.1, memory_length 2000, epsilon 0.0002205248877129865 total_time 734.0\n",
      "episode 16850, avg_reward 1476.1, memory_length 2000, epsilon 0.00021942501524698643 total_time 721.0\n",
      "episode 16860, avg_reward 1352.9, memory_length 2000, epsilon 0.00021833062841779591 total_time 720.0\n",
      "episode 16870, avg_reward 1288.5, memory_length 2000, epsilon 0.0002172416998656876 total_time 722.0\n",
      "episode 16880, avg_reward 1227.8, memory_length 2000, epsilon 0.00021615820236739027 total_time 727.0\n",
      "episode 16890, avg_reward 1281.7, memory_length 2000, epsilon 0.00021508010883541074 total_time 728.0\n",
      "episode 16900, avg_reward 1295.1, memory_length 2000, epsilon 0.00021400739231735381 total_time 721.0\n",
      "Saving model weights for episode:16900\n",
      "episode 16910, avg_reward 1328.0, memory_length 2000, epsilon 0.00021294002599525142 total_time 721.0\n",
      "episode 16920, avg_reward 1244.1, memory_length 2000, epsilon 0.00021187798318488915 total_time 728.0\n",
      "episode 16930, avg_reward 1312.6, memory_length 2000, epsilon 0.00021082123733514186 total_time 721.0\n",
      "episode 16940, avg_reward 1346.9, memory_length 2000, epsilon 0.00020976976202730858 total_time 721.0\n",
      "episode 16950, avg_reward 1299.4, memory_length 2000, epsilon 0.00020872353097445113 total_time 728.0\n",
      "episode 16960, avg_reward 1318.3, memory_length 2000, epsilon 0.00020768251802073944 total_time 724.0\n",
      "episode 16970, avg_reward 1366.5, memory_length 2000, epsilon 0.00020664669714079472 total_time 720.0\n",
      "episode 16980, avg_reward 1439.3, memory_length 2000, epsilon 0.00020561604243904175 total_time 722.0\n",
      "episode 16990, avg_reward 1398.0, memory_length 2000, epsilon 0.00020459052814905855 total_time 720.0\n",
      "episode 17000, avg_reward 1508.1, memory_length 2000, epsilon 0.0002035701286329352 total_time 720.0\n",
      "Saving model weights for episode:17000\n",
      "episode 17010, avg_reward 1553.5, memory_length 2000, epsilon 0.00020255481838062995 total_time 722.0\n",
      "episode 17020, avg_reward 1469.8, memory_length 2000, epsilon 0.00020154457200933395 total_time 722.0\n",
      "episode 17030, avg_reward 1465.3, memory_length 2000, epsilon 0.00020053936426283565 total_time 725.0\n",
      "episode 17040, avg_reward 1429.6, memory_length 2000, epsilon 0.00019953917001088833 total_time 724.0\n",
      "episode 17050, avg_reward 1296.3, memory_length 2000, epsilon 0.00019854396424858433 total_time 732.0\n",
      "episode 17060, avg_reward 1397.3, memory_length 2000, epsilon 0.00019755372209572701 total_time 723.0\n",
      "episode 17070, avg_reward 1407.4, memory_length 2000, epsilon 0.00019656841879621173 total_time 727.0\n",
      "episode 17080, avg_reward 1565.6, memory_length 2000, epsilon 0.00019558802971740392 total_time 722.0\n",
      "episode 17090, avg_reward 1327.3, memory_length 2000, epsilon 0.0001946125303495263 total_time 723.0\n",
      "episode 17100, avg_reward 1378.3, memory_length 2000, epsilon 0.00019364189630504314 total_time 721.0\n",
      "Saving model weights for episode:17100\n",
      "episode 17110, avg_reward 1222.5, memory_length 2000, epsilon 0.00019267610331805314 total_time 720.0\n",
      "episode 17120, avg_reward 1311.5, memory_length 2000, epsilon 0.00019171512724368165 total_time 721.0\n",
      "episode 17130, avg_reward 1433.9, memory_length 2000, epsilon 0.00019075894405747607 total_time 724.0\n",
      "episode 17140, avg_reward 1325.1, memory_length 2000, epsilon 0.00018980752985480763 total_time 727.0\n",
      "episode 17150, avg_reward 1380.5, memory_length 2000, epsilon 0.00018886086085027106 total_time 721.0\n",
      "episode 17160, avg_reward 1318.9, memory_length 2000, epsilon 0.0001879189133770926 total_time 721.0\n",
      "episode 17170, avg_reward 1331.0, memory_length 2000, epsilon 0.0001869816638865357 total_time 724.0\n",
      "episode 17180, avg_reward 1370.9, memory_length 2000, epsilon 0.00018604908894731455 total_time 725.0\n",
      "episode 17190, avg_reward 1304.7, memory_length 2000, epsilon 0.00018512116524500752 total_time 720.0\n",
      "episode 17200, avg_reward 1280.2, memory_length 2000, epsilon 0.000184197869581473 total_time 724.0\n",
      "Saving model weights for episode:17200\n",
      "episode 17210, avg_reward 1261.6, memory_length 2000, epsilon 0.00018327917887427198 total_time 721.0\n",
      "episode 17220, avg_reward 1283.4, memory_length 2000, epsilon 0.0001823650701560883 total_time 720.0\n",
      "episode 17230, avg_reward 1500.1, memory_length 2000, epsilon 0.00018145552057415704 total_time 721.0\n",
      "episode 17240, avg_reward 1373.4, memory_length 2000, epsilon 0.00018055050738969064 total_time 724.0\n",
      "episode 17250, avg_reward 1253.4, memory_length 2000, epsilon 0.00017965000797731296 total_time 722.0\n",
      "episode 17260, avg_reward 1186.3, memory_length 2000, epsilon 0.00017875399982449119 total_time 727.0\n",
      "episode 17270, avg_reward 1207.2, memory_length 2000, epsilon 0.00017786246053097516 total_time 720.0\n",
      "episode 17280, avg_reward 1346.4, memory_length 2000, epsilon 0.00017697536780823637 total_time 723.0\n",
      "episode 17290, avg_reward 1425.5, memory_length 2000, epsilon 0.00017609269947890994 total_time 721.0\n",
      "episode 17300, avg_reward 1440.0, memory_length 2000, epsilon 0.00017521443347624232 total_time 720.0\n",
      "Saving model weights for episode:17300\n",
      "episode 17310, avg_reward 1310.4, memory_length 2000, epsilon 0.00017434054784353704 total_time 724.0\n",
      "episode 17320, avg_reward 1315.1, memory_length 2000, epsilon 0.0001734710207336084 total_time 724.0\n",
      "episode 17330, avg_reward 1351.1, memory_length 2000, epsilon 0.00017260583040823276 total_time 721.0\n",
      "episode 17340, avg_reward 1206.7, memory_length 2000, epsilon 0.00017174495523760753 total_time 722.0\n",
      "episode 17350, avg_reward 1318.2, memory_length 2000, epsilon 0.00017088837369980798 total_time 728.0\n",
      "episode 17360, avg_reward 1323.4, memory_length 2000, epsilon 0.00017003606438025138 total_time 725.0\n",
      "episode 17370, avg_reward 1390.1, memory_length 2000, epsilon 0.00016918800597116063 total_time 722.0\n",
      "episode 17380, avg_reward 1272.7, memory_length 2000, epsilon 0.00016834417727103074 total_time 725.0\n",
      "episode 17390, avg_reward 1127.8, memory_length 2000, epsilon 0.00016750455718410087 total_time 721.0\n",
      "episode 17400, avg_reward 1102.2, memory_length 2000, epsilon 0.0001666691247198245 total_time 726.0\n",
      "Saving model weights for episode:17400\n",
      "episode 17410, avg_reward 1152.2, memory_length 2000, epsilon 0.00016583785899234712 total_time 721.0\n",
      "episode 17420, avg_reward 1225.9, memory_length 2000, epsilon 0.00016501073921998163 total_time 725.0\n",
      "episode 17430, avg_reward 1149.0, memory_length 2000, epsilon 0.00016418774472469098 total_time 720.0\n",
      "episode 17440, avg_reward 1322.0, memory_length 2000, epsilon 0.00016336885493157017 total_time 720.0\n",
      "episode 17450, avg_reward 1294.8, memory_length 2000, epsilon 0.00016255404936833118 total_time 722.0\n",
      "episode 17460, avg_reward 1281.7, memory_length 2000, epsilon 0.00016174330766479303 total_time 720.0\n",
      "episode 17470, avg_reward 1354.9, memory_length 2000, epsilon 0.00016093660955237036 total_time 722.0\n",
      "episode 17480, avg_reward 1298.9, memory_length 2000, epsilon 0.00016013393486356893 total_time 722.0\n",
      "episode 17490, avg_reward 1399.4, memory_length 2000, epsilon 0.0001593352635314791 total_time 721.0\n",
      "episode 17500, avg_reward 1311.3, memory_length 2000, epsilon 0.00015854057558927656 total_time 723.0\n",
      "Saving model weights for episode:17500\n",
      "episode 17510, avg_reward 1282.6, memory_length 2000, epsilon 0.0001577498511697208 total_time 727.0\n",
      "episode 17520, avg_reward 1395.0, memory_length 2000, epsilon 0.0001569630705046604 total_time 729.0\n",
      "episode 17530, avg_reward 1236.5, memory_length 2000, epsilon 0.00015618021392453812 total_time 729.0\n",
      "episode 17540, avg_reward 1262.4, memory_length 2000, epsilon 0.00015540126185789802 total_time 720.0\n",
      "episode 17550, avg_reward 1253.6, memory_length 2000, epsilon 0.00015462619483089848 total_time 726.0\n",
      "episode 17560, avg_reward 1329.7, memory_length 2000, epsilon 0.0001538549934668229 total_time 721.0\n",
      "episode 17570, avg_reward 1390.7, memory_length 2000, epsilon 0.00015308763848559756 total_time 720.0\n",
      "episode 17580, avg_reward 1387.3, memory_length 2000, epsilon 0.00015232411070330737 total_time 726.0\n",
      "episode 17590, avg_reward 1139.8, memory_length 2000, epsilon 0.00015156439103171862 total_time 720.0\n",
      "episode 17600, avg_reward 1265.0, memory_length 2000, epsilon 0.0001508084604777994 total_time 724.0\n",
      "Saving model weights for episode:17600\n",
      "episode 17610, avg_reward 1271.0, memory_length 2000, epsilon 0.00015005630014324668 total_time 727.0\n",
      "episode 17620, avg_reward 1225.9, memory_length 2000, epsilon 0.0001493078912240133 total_time 728.0\n",
      "episode 17630, avg_reward 1321.2, memory_length 2000, epsilon 0.00014856321500983667 total_time 722.0\n",
      "episode 17640, avg_reward 1362.9, memory_length 2000, epsilon 0.00014782225288377326 total_time 733.0\n",
      "episode 17650, avg_reward 1526.4, memory_length 2000, epsilon 0.00014708498632173074 total_time 721.0\n",
      "episode 17660, avg_reward 1461.0, memory_length 2000, epsilon 0.0001463513968920072 total_time 720.0\n",
      "episode 17670, avg_reward 1431.0, memory_length 2000, epsilon 0.0001456214662548282 total_time 724.0\n",
      "episode 17680, avg_reward 1376.0, memory_length 2000, epsilon 0.00014489517616189004 total_time 724.0\n",
      "episode 17690, avg_reward 1415.8, memory_length 2000, epsilon 0.0001441725084559028 total_time 727.0\n",
      "episode 17700, avg_reward 1271.0, memory_length 2000, epsilon 0.00014345344507013568 total_time 727.0\n",
      "Saving model weights for episode:17700\n",
      "episode 17710, avg_reward 1202.5, memory_length 2000, epsilon 0.0001427379680279671 total_time 723.0\n",
      "episode 17720, avg_reward 1257.6, memory_length 2000, epsilon 0.00014202605944243327 total_time 723.0\n",
      "episode 17730, avg_reward 1430.1, memory_length 2000, epsilon 0.00014131770151578293 total_time 723.0\n",
      "episode 17740, avg_reward 1321.3, memory_length 2000, epsilon 0.00014061287653903054 total_time 720.0\n",
      "episode 17750, avg_reward 1330.1, memory_length 2000, epsilon 0.00013991156689151547 total_time 725.0\n",
      "episode 17760, avg_reward 1303.1, memory_length 2000, epsilon 0.0001392137550404595 total_time 723.0\n",
      "episode 17770, avg_reward 1290.5, memory_length 2000, epsilon 0.0001385194235405303 total_time 729.0\n",
      "episode 17780, avg_reward 1285.4, memory_length 2000, epsilon 0.00013782855503340438 total_time 721.0\n",
      "episode 17790, avg_reward 1386.7, memory_length 2000, epsilon 0.00013714113224733263 total_time 722.0\n",
      "episode 17800, avg_reward 1467.2, memory_length 2000, epsilon 0.0001364571379967101 total_time 722.0\n",
      "Saving model weights for episode:17800\n",
      "episode 17810, avg_reward 1353.0, memory_length 2000, epsilon 0.00013577655518164436 total_time 728.0\n",
      "episode 17820, avg_reward 1441.3, memory_length 2000, epsilon 0.00013509936678753013 total_time 725.0\n",
      "episode 17830, avg_reward 1313.1, memory_length 2000, epsilon 0.00013442555588462178 total_time 720.0\n",
      "episode 17840, avg_reward 1437.2, memory_length 2000, epsilon 0.00013375510562761212 total_time 720.0\n",
      "episode 17850, avg_reward 1392.1, memory_length 2000, epsilon 0.00013308799925520933 total_time 722.0\n",
      "episode 17860, avg_reward 1476.8, memory_length 2000, epsilon 0.00013242422008971958 total_time 724.0\n",
      "episode 17870, avg_reward 1133.7, memory_length 2000, epsilon 0.00013176375153662942 total_time 721.0\n",
      "episode 17880, avg_reward 1227.3, memory_length 2000, epsilon 0.00013110657708419014 total_time 720.0\n",
      "episode 17890, avg_reward 1177.3, memory_length 2000, epsilon 0.00013045268030300667 total_time 721.0\n",
      "episode 17900, avg_reward 1183.3, memory_length 2000, epsilon 0.00012980204484562495 total_time 727.0\n",
      "Saving model weights for episode:17900\n",
      "episode 17910, avg_reward 1140.7, memory_length 2000, epsilon 0.00012915465444612514 total_time 724.0\n",
      "episode 17920, avg_reward 1122.7, memory_length 2000, epsilon 0.00012851049291971308 total_time 721.0\n",
      "episode 17930, avg_reward 1369.0, memory_length 2000, epsilon 0.00012786954416231725 total_time 728.0\n",
      "episode 17940, avg_reward 1438.9, memory_length 2000, epsilon 0.0001272317921501856 total_time 723.0\n",
      "episode 17950, avg_reward 1341.5, memory_length 2000, epsilon 0.00012659722093948412 total_time 724.0\n",
      "episode 17960, avg_reward 1182.9, memory_length 2000, epsilon 0.00012596581466589997 total_time 720.0\n",
      "episode 17970, avg_reward 1248.3, memory_length 2000, epsilon 0.00012533755754424297 total_time 726.0\n",
      "episode 17980, avg_reward 1329.7, memory_length 2000, epsilon 0.0001247124338680528 total_time 721.0\n",
      "episode 17990, avg_reward 1246.3, memory_length 2000, epsilon 0.0001240904280092046 total_time 725.0\n",
      "episode 18000, avg_reward 1340.3, memory_length 2000, epsilon 0.00012347152441751982 total_time 726.0\n",
      "Saving model weights for episode:18000\n",
      "episode 18010, avg_reward 1363.0, memory_length 2000, epsilon 0.00012285570762037612 total_time 722.0\n",
      "episode 18020, avg_reward 1305.6, memory_length 2000, epsilon 0.00012224296222232168 total_time 723.0\n",
      "episode 18030, avg_reward 1370.5, memory_length 2000, epsilon 0.0001216332729046898 total_time 724.0\n",
      "episode 18040, avg_reward 1426.7, memory_length 2000, epsilon 0.00012102662442521542 total_time 720.0\n",
      "episode 18050, avg_reward 1305.1, memory_length 2000, epsilon 0.00012042300161765536 total_time 720.0\n",
      "episode 18060, avg_reward 1295.1, memory_length 2000, epsilon 0.00011982238939140755 total_time 722.0\n",
      "episode 18070, avg_reward 1323.5, memory_length 2000, epsilon 0.0001192247727311355 total_time 727.0\n",
      "episode 18080, avg_reward 1401.1, memory_length 2000, epsilon 0.00011863013669639115 total_time 722.0\n",
      "episode 18090, avg_reward 1379.2, memory_length 2000, epsilon 0.00011803846642124308 total_time 729.0\n",
      "episode 18100, avg_reward 1247.3, memory_length 2000, epsilon 0.00011744974711390317 total_time 730.0\n",
      "Saving model weights for episode:18100\n",
      "episode 18110, avg_reward 1338.7, memory_length 2000, epsilon 0.00011686396405635829 total_time 722.0\n",
      "episode 18120, avg_reward 1363.3, memory_length 2000, epsilon 0.00011628110260400169 total_time 722.0\n",
      "episode 18130, avg_reward 1293.2, memory_length 2000, epsilon 0.0001157011481852663 total_time 724.0\n",
      "episode 18140, avg_reward 1185.6, memory_length 2000, epsilon 0.00011512408630126185 total_time 720.0\n",
      "episode 18150, avg_reward 1151.7, memory_length 2000, epsilon 0.00011454990252541077 total_time 724.0\n",
      "episode 18160, avg_reward 1219.8, memory_length 2000, epsilon 0.00011397858250308919 total_time 725.0\n",
      "episode 18170, avg_reward 1260.9, memory_length 2000, epsilon 0.00011341011195126637 total_time 721.0\n",
      "episode 18180, avg_reward 1255.1, memory_length 2000, epsilon 0.00011284447665814911 total_time 726.0\n",
      "episode 18190, avg_reward 1220.7, memory_length 2000, epsilon 0.00011228166248282583 total_time 728.0\n",
      "episode 18200, avg_reward 1158.4, memory_length 2000, epsilon 0.00011172165535491242 total_time 721.0\n",
      "Saving model weights for episode:18200\n",
      "episode 18210, avg_reward 1240.5, memory_length 2000, epsilon 0.00011116444127420193 total_time 721.0\n",
      "episode 18220, avg_reward 1183.3, memory_length 2000, epsilon 0.00011061000631031292 total_time 724.0\n",
      "episode 18230, avg_reward 1285.7, memory_length 2000, epsilon 0.00011005833660234281 total_time 725.0\n",
      "episode 18240, avg_reward 1277.6, memory_length 2000, epsilon 0.00010950941835851975 total_time 723.0\n",
      "episode 18250, avg_reward 1190.4, memory_length 2000, epsilon 0.0001089632378558595 total_time 725.0\n",
      "episode 18260, avg_reward 1287.9, memory_length 2000, epsilon 0.0001084197814398206 total_time 725.0\n",
      "episode 18270, avg_reward 1347.5, memory_length 2000, epsilon 0.00010787903552396458 total_time 726.0\n",
      "episode 18280, avg_reward 1233.9, memory_length 2000, epsilon 0.00010734098658961554 total_time 720.0\n",
      "episode 18290, avg_reward 1314.7, memory_length 2000, epsilon 0.00010680562118552175 total_time 722.0\n",
      "episode 18300, avg_reward 1282.5, memory_length 2000, epsilon 0.00010627292592752057 total_time 720.0\n",
      "Saving model weights for episode:18300\n",
      "episode 18310, avg_reward 1368.9, memory_length 2000, epsilon 0.00010574288749820246 total_time 726.0\n",
      "episode 18320, avg_reward 1357.8, memory_length 2000, epsilon 0.00010521549264657942 total_time 721.0\n",
      "episode 18330, avg_reward 1312.6, memory_length 2000, epsilon 0.00010469072818775234 total_time 722.0\n",
      "episode 18340, avg_reward 1362.5, memory_length 2000, epsilon 0.00010416858100258279 total_time 730.0\n",
      "episode 18350, avg_reward 1268.0, memory_length 2000, epsilon 0.00010364903803736357 total_time 722.0\n",
      "episode 18360, avg_reward 1391.2, memory_length 2000, epsilon 0.00010313208630349368 total_time 720.0\n",
      "episode 18370, avg_reward 1326.7, memory_length 2000, epsilon 0.00010261771287715302 total_time 722.0\n",
      "episode 18380, avg_reward 1439.3, memory_length 2000, epsilon 0.00010210590489897879 total_time 720.0\n",
      "episode 18390, avg_reward 1294.1, memory_length 2000, epsilon 0.00010159664957374522 total_time 722.0\n",
      "episode 18400, avg_reward 1289.5, memory_length 2000, epsilon 0.00010108993417004233 total_time 723.0\n",
      "Saving model weights for episode:18400\n",
      "episode 18410, avg_reward 1545.6, memory_length 2000, epsilon 0.00010058574601995897 total_time 728.0\n",
      "episode 18420, avg_reward 1433.3, memory_length 2000, epsilon 0.00010008407251876477 total_time 720.0\n",
      "episode 18430, avg_reward 1399.1, memory_length 2000, epsilon 9.958490112459626e-05 total_time 726.0\n",
      "episode 18440, avg_reward 1300.7, memory_length 2000, epsilon 9.908821935814277e-05 total_time 723.0\n",
      "episode 18450, avg_reward 1372.0, memory_length 2000, epsilon 9.859401480233389e-05 total_time 720.0\n",
      "episode 18460, avg_reward 1499.4, memory_length 2000, epsilon 9.810227510203037e-05 total_time 721.0\n",
      "episode 18470, avg_reward 1281.9, memory_length 2000, epsilon 9.76129879637137e-05 total_time 721.0\n",
      "episode 18480, avg_reward 1314.7, memory_length 2000, epsilon 9.712614115518032e-05 total_time 721.0\n",
      "episode 18490, avg_reward 1274.6, memory_length 2000, epsilon 9.664172250523432e-05 total_time 723.0\n",
      "episode 18500, avg_reward 1246.8, memory_length 2000, epsilon 9.615971990338453e-05 total_time 720.0\n",
      "Saving model weights for episode:18500\n",
      "episode 18510, avg_reward 1420.4, memory_length 2000, epsilon 9.568012129954049e-05 total_time 725.0\n",
      "episode 18520, avg_reward 1355.0, memory_length 2000, epsilon 9.520291470371227e-05 total_time 725.0\n",
      "episode 18530, avg_reward 1287.3, memory_length 2000, epsilon 9.472808818571031e-05 total_time 723.0\n",
      "episode 18540, avg_reward 1221.7, memory_length 2000, epsilon 9.425562987484659e-05 total_time 723.0\n",
      "episode 18550, avg_reward 1368.1, memory_length 2000, epsilon 9.378552795963904e-05 total_time 724.0\n",
      "episode 18560, avg_reward 1242.6, memory_length 2000, epsilon 9.3317770687515e-05 total_time 721.0\n",
      "episode 18570, avg_reward 1308.4, memory_length 2000, epsilon 9.285234636451861e-05 total_time 723.0\n",
      "episode 18580, avg_reward 1454.1, memory_length 2000, epsilon 9.238924335501723e-05 total_time 726.0\n",
      "episode 18590, avg_reward 1425.3, memory_length 2000, epsilon 9.192845008141184e-05 total_time 720.0\n",
      "episode 18600, avg_reward 1427.7, memory_length 2000, epsilon 9.146995502384626e-05 total_time 729.0\n",
      "Saving model weights for episode:18600\n",
      "episode 18610, avg_reward 1505.1, memory_length 2000, epsilon 9.101374671992034e-05 total_time 724.0\n",
      "episode 18620, avg_reward 1256.7, memory_length 2000, epsilon 9.055981376440289e-05 total_time 726.0\n",
      "episode 18630, avg_reward 1333.8, memory_length 2000, epsilon 9.010814480894604e-05 total_time 726.0\n",
      "episode 18640, avg_reward 1191.7, memory_length 2000, epsilon 8.965872856180271e-05 total_time 724.0\n",
      "episode 18650, avg_reward 1338.8, memory_length 2000, epsilon 8.921155378754301e-05 total_time 721.0\n",
      "episode 18660, avg_reward 1403.8, memory_length 2000, epsilon 8.876660930677459e-05 total_time 727.0\n",
      "episode 18670, avg_reward 1577.5, memory_length 2000, epsilon 8.832388399586195e-05 total_time 720.0\n",
      "episode 18680, avg_reward 1482.4, memory_length 2000, epsilon 8.788336678664939e-05 total_time 724.0\n",
      "episode 18690, avg_reward 1477.4, memory_length 2000, epsilon 8.744504666618394e-05 total_time 720.0\n",
      "episode 18700, avg_reward 1412.7, memory_length 2000, epsilon 8.70089126764394e-05 total_time 723.0\n",
      "Saving model weights for episode:18700\n",
      "episode 18710, avg_reward 1339.0, memory_length 2000, epsilon 8.657495391404366e-05 total_time 727.0\n",
      "episode 18720, avg_reward 1382.5, memory_length 2000, epsilon 8.614315953000473e-05 total_time 723.0\n",
      "episode 18730, avg_reward 1271.0, memory_length 2000, epsilon 8.571351872944084e-05 total_time 725.0\n",
      "episode 18740, avg_reward 1316.0, memory_length 2000, epsilon 8.528602077130927e-05 total_time 723.0\n",
      "episode 18750, avg_reward 1382.8, memory_length 2000, epsilon 8.486065496813912e-05 total_time 725.0\n",
      "episode 18760, avg_reward 1382.4, memory_length 2000, epsilon 8.443741068576285e-05 total_time 720.0\n",
      "episode 18770, avg_reward 1409.6, memory_length 2000, epsilon 8.401627734305152e-05 total_time 721.0\n",
      "episode 18780, avg_reward 1495.5, memory_length 2000, epsilon 8.359724441164975e-05 total_time 720.0\n",
      "episode 18790, avg_reward 1351.3, memory_length 2000, epsilon 8.318030141571217e-05 total_time 724.0\n",
      "episode 18800, avg_reward 1335.2, memory_length 2000, epsilon 8.276543793164243e-05 total_time 734.0\n",
      "Saving model weights for episode:18800\n",
      "episode 18810, avg_reward 1274.8, memory_length 2000, epsilon 8.235264358783153e-05 total_time 720.0\n",
      "episode 18820, avg_reward 1317.9, memory_length 2000, epsilon 8.19419080643997e-05 total_time 727.0\n",
      "episode 18830, avg_reward 1215.5, memory_length 2000, epsilon 8.153322109293712e-05 total_time 729.0\n",
      "episode 18840, avg_reward 1259.4, memory_length 2000, epsilon 8.112657245624855e-05 total_time 728.0\n",
      "episode 18850, avg_reward 1198.8, memory_length 2000, epsilon 8.072195198809658e-05 total_time 721.0\n",
      "episode 18860, avg_reward 1211.8, memory_length 2000, epsilon 8.031934957294858e-05 total_time 728.0\n",
      "episode 18870, avg_reward 1226.1, memory_length 2000, epsilon 7.991875514572335e-05 total_time 726.0\n",
      "episode 18880, avg_reward 1234.8, memory_length 2000, epsilon 7.952015869153907e-05 total_time 720.0\n",
      "episode 18890, avg_reward 1375.1, memory_length 2000, epsilon 7.912355024546389e-05 total_time 725.0\n",
      "episode 18900, avg_reward 1307.1, memory_length 2000, epsilon 7.872891989226574e-05 total_time 720.0\n",
      "Saving model weights for episode:18900\n",
      "episode 18910, avg_reward 1341.8, memory_length 2000, epsilon 7.833625776616549e-05 total_time 728.0\n",
      "episode 18920, avg_reward 1341.6, memory_length 2000, epsilon 7.794555405058928e-05 total_time 723.0\n",
      "episode 18930, avg_reward 1348.7, memory_length 2000, epsilon 7.7556798977924e-05 total_time 721.0\n",
      "episode 18940, avg_reward 1367.4, memory_length 2000, epsilon 7.716998282927273e-05 total_time 727.0\n",
      "episode 18950, avg_reward 1480.4, memory_length 2000, epsilon 7.678509593421132e-05 total_time 720.0\n",
      "episode 18960, avg_reward 1422.4, memory_length 2000, epsilon 7.640212867054764e-05 total_time 721.0\n",
      "episode 18970, avg_reward 1367.7, memory_length 2000, epsilon 7.602107146407985e-05 total_time 720.0\n",
      "episode 18980, avg_reward 1349.7, memory_length 2000, epsilon 7.564191478835824e-05 total_time 724.0\n",
      "episode 18990, avg_reward 1340.7, memory_length 2000, epsilon 7.52646491644459e-05 total_time 732.0\n",
      "episode 19000, avg_reward 1204.5, memory_length 2000, epsilon 7.488926516068283e-05 total_time 721.0\n",
      "Saving model weights for episode:19000\n",
      "episode 19010, avg_reward 1326.5, memory_length 2000, epsilon 7.451575339244914e-05 total_time 728.0\n",
      "episode 19020, avg_reward 1379.2, memory_length 2000, epsilon 7.414410452193129e-05 total_time 722.0\n",
      "episode 19030, avg_reward 1284.2, memory_length 2000, epsilon 7.377430925788829e-05 total_time 726.0\n",
      "episode 19040, avg_reward 1298.9, memory_length 2000, epsilon 7.340635835541902e-05 total_time 725.0\n",
      "episode 19050, avg_reward 1416.1, memory_length 2000, epsilon 7.304024261573202e-05 total_time 724.0\n",
      "episode 19060, avg_reward 1304.4, memory_length 2000, epsilon 7.267595288591448e-05 total_time 726.0\n",
      "episode 19070, avg_reward 1318.2, memory_length 2000, epsilon 7.23134800587044e-05 total_time 720.0\n",
      "episode 19080, avg_reward 1251.3, memory_length 2000, epsilon 7.195281507226201e-05 total_time 727.0\n",
      "episode 19090, avg_reward 1246.3, memory_length 2000, epsilon 7.159394890994409e-05 total_time 720.0\n",
      "episode 19100, avg_reward 1268.6, memory_length 2000, epsilon 7.123687260007765e-05 total_time 723.0\n",
      "Saving model weights for episode:19100\n",
      "episode 19110, avg_reward 1225.5, memory_length 2000, epsilon 7.088157721573647e-05 total_time 720.0\n",
      "episode 19120, avg_reward 1310.6, memory_length 2000, epsilon 7.052805387451756e-05 total_time 731.0\n",
      "episode 19130, avg_reward 1201.9, memory_length 2000, epsilon 7.017629373831873e-05 total_time 720.0\n",
      "episode 19140, avg_reward 1350.2, memory_length 2000, epsilon 6.982628801311849e-05 total_time 726.0\n",
      "episode 19150, avg_reward 1233.1, memory_length 2000, epsilon 6.947802794875525e-05 total_time 727.0\n",
      "episode 19160, avg_reward 1250.0, memory_length 2000, epsilon 6.91315048387095e-05 total_time 723.0\n",
      "episode 19170, avg_reward 1356.2, memory_length 2000, epsilon 6.87867100198852e-05 total_time 723.0\n",
      "episode 19180, avg_reward 1280.4, memory_length 2000, epsilon 6.844363487239404e-05 total_time 720.0\n",
      "episode 19190, avg_reward 1384.2, memory_length 2000, epsilon 6.810227081933959e-05 total_time 731.0\n",
      "episode 19200, avg_reward 1356.2, memory_length 2000, epsilon 6.77626093266025e-05 total_time 723.0\n",
      "Saving model weights for episode:19200\n",
      "episode 19210, avg_reward 1376.3, memory_length 2000, epsilon 6.7424641902628e-05 total_time 728.0\n",
      "episode 19220, avg_reward 1534.4, memory_length 2000, epsilon 6.708836009821265e-05 total_time 727.0\n",
      "episode 19230, avg_reward 1473.6, memory_length 2000, epsilon 6.675375550629406e-05 total_time 721.0\n",
      "episode 19240, avg_reward 1336.9, memory_length 2000, epsilon 6.642081976173976e-05 total_time 727.0\n",
      "episode 19250, avg_reward 1271.0, memory_length 2000, epsilon 6.608954454113905e-05 total_time 729.0\n",
      "episode 19260, avg_reward 1257.9, memory_length 2000, epsilon 6.575992156259391e-05 total_time 720.0\n",
      "episode 19270, avg_reward 1353.3, memory_length 2000, epsilon 6.543194258551285e-05 total_time 721.0\n",
      "episode 19280, avg_reward 1215.8, memory_length 2000, epsilon 6.510559941040446e-05 total_time 723.0\n",
      "episode 19290, avg_reward 1218.0, memory_length 2000, epsilon 6.478088387867212e-05 total_time 724.0\n",
      "episode 19300, avg_reward 1334.6, memory_length 2000, epsilon 6.445778787241088e-05 total_time 722.0\n",
      "Saving model weights for episode:19300\n",
      "episode 19310, avg_reward 1377.6, memory_length 2000, epsilon 6.41363033142035e-05 total_time 724.0\n",
      "episode 19320, avg_reward 1389.2, memory_length 2000, epsilon 6.381642216691955e-05 total_time 720.0\n",
      "episode 19330, avg_reward 1376.3, memory_length 2000, epsilon 6.349813643351342e-05 total_time 722.0\n",
      "episode 19340, avg_reward 1226.9, memory_length 2000, epsilon 6.318143815682543e-05 total_time 724.0\n",
      "episode 19350, avg_reward 1326.3, memory_length 2000, epsilon 6.286631941938195e-05 total_time 720.0\n",
      "episode 19360, avg_reward 1382.0, memory_length 2000, epsilon 6.255277234319826e-05 total_time 724.0\n",
      "episode 19370, avg_reward 1313.6, memory_length 2000, epsilon 6.224078908958122e-05 total_time 724.0\n",
      "episode 19380, avg_reward 1353.3, memory_length 2000, epsilon 6.193036185893301e-05 total_time 722.0\n",
      "episode 19390, avg_reward 1369.2, memory_length 2000, epsilon 6.162148289055693e-05 total_time 725.0\n",
      "episode 19400, avg_reward 1250.8, memory_length 2000, epsilon 6.131414446246246e-05 total_time 721.0\n",
      "Saving model weights for episode:19400\n",
      "episode 19410, avg_reward 1275.5, memory_length 2000, epsilon 6.100833889117311e-05 total_time 723.0\n",
      "episode 19420, avg_reward 1273.1, memory_length 2000, epsilon 6.070405853153344e-05 total_time 720.0\n",
      "episode 19430, avg_reward 1342.8, memory_length 2000, epsilon 6.040129577651874e-05 total_time 722.0\n",
      "episode 19440, avg_reward 1308.8, memory_length 2000, epsilon 6.0100043057044465e-05 total_time 726.0\n",
      "episode 19450, avg_reward 1226.6, memory_length 2000, epsilon 5.9800292841776724e-05 total_time 726.0\n",
      "episode 19460, avg_reward 1273.2, memory_length 2000, epsilon 5.950203763694473e-05 total_time 721.0\n",
      "episode 19470, avg_reward 1304.2, memory_length 2000, epsilon 5.920526998615262e-05 total_time 720.0\n",
      "episode 19480, avg_reward 1291.2, memory_length 2000, epsilon 5.890998247019388e-05 total_time 722.0\n",
      "episode 19490, avg_reward 1377.4, memory_length 2000, epsilon 5.861616770686502e-05 total_time 722.0\n",
      "episode 19500, avg_reward 1387.4, memory_length 2000, epsilon 5.832381835078187e-05 total_time 722.0\n",
      "Saving model weights for episode:19500\n",
      "episode 19510, avg_reward 1244.9, memory_length 2000, epsilon 5.803292709319508e-05 total_time 736.0\n",
      "episode 19520, avg_reward 1184.8, memory_length 2000, epsilon 5.7743486661808175e-05 total_time 720.0\n",
      "episode 19530, avg_reward 1291.0, memory_length 2000, epsilon 5.745548982059539e-05 total_time 724.0\n",
      "episode 19540, avg_reward 1282.4, memory_length 2000, epsilon 5.71689293696205e-05 total_time 721.0\n",
      "episode 19550, avg_reward 1319.0, memory_length 2000, epsilon 5.68837981448575e-05 total_time 721.0\n",
      "episode 19560, avg_reward 1460.2, memory_length 2000, epsilon 5.6600089018010716e-05 total_time 722.0\n",
      "episode 19570, avg_reward 1320.5, memory_length 2000, epsilon 5.6317794896337415e-05 total_time 724.0\n",
      "episode 19580, avg_reward 1316.2, memory_length 2000, epsilon 5.603690872246964e-05 total_time 727.0\n",
      "episode 19590, avg_reward 1261.1, memory_length 2000, epsilon 5.5757423474238614e-05 total_time 720.0\n",
      "episode 19600, avg_reward 1308.4, memory_length 2000, epsilon 5.547933216449838e-05 total_time 722.0\n",
      "Saving model weights for episode:19600\n",
      "episode 19610, avg_reward 1276.2, memory_length 2000, epsilon 5.520262784095181e-05 total_time 726.0\n",
      "episode 19620, avg_reward 1372.5, memory_length 2000, epsilon 5.4927303585976505e-05 total_time 721.0\n",
      "episode 19630, avg_reward 1234.9, memory_length 2000, epsilon 5.465335251645155e-05 total_time 725.0\n",
      "episode 19640, avg_reward 1321.7, memory_length 2000, epsilon 5.438076778358613e-05 total_time 720.0\n",
      "episode 19650, avg_reward 1399.9, memory_length 2000, epsilon 5.410954257274754e-05 total_time 720.0\n",
      "episode 19660, avg_reward 1324.0, memory_length 2000, epsilon 5.383967010329158e-05 total_time 720.0\n",
      "episode 19670, avg_reward 1242.3, memory_length 2000, epsilon 5.3571143628392246e-05 total_time 726.0\n",
      "episode 19680, avg_reward 1373.7, memory_length 2000, epsilon 5.330395643487379e-05 total_time 724.0\n",
      "episode 19690, avg_reward 1282.7, memory_length 2000, epsilon 5.303810184304256e-05 total_time 723.0\n",
      "episode 19700, avg_reward 1299.3, memory_length 2000, epsilon 5.277357320651971e-05 total_time 723.0\n",
      "Saving model weights for episode:19700\n",
      "episode 19710, avg_reward 1303.7, memory_length 2000, epsilon 5.251036391207574e-05 total_time 720.0\n",
      "episode 19720, avg_reward 1378.4, memory_length 2000, epsilon 5.22484673794644e-05 total_time 720.0\n",
      "episode 19730, avg_reward 1372.7, memory_length 2000, epsilon 5.1987877061258915e-05 total_time 722.0\n",
      "episode 19740, avg_reward 1371.5, memory_length 2000, epsilon 5.1728586442687567e-05 total_time 720.0\n",
      "episode 19750, avg_reward 1334.3, memory_length 2000, epsilon 5.1470589041471585e-05 total_time 722.0\n",
      "episode 19760, avg_reward 1253.5, memory_length 2000, epsilon 5.121387840766231e-05 total_time 728.0\n",
      "episode 19770, avg_reward 1326.1, memory_length 2000, epsilon 5.095844812348061e-05 total_time 720.0\n",
      "episode 19780, avg_reward 1214.9, memory_length 2000, epsilon 5.0704291803156184e-05 total_time 722.0\n",
      "episode 19790, avg_reward 1222.3, memory_length 2000, epsilon 5.0451403092767595e-05 total_time 725.0\n",
      "episode 19800, avg_reward 1327.8, memory_length 2000, epsilon 5.01997756700841e-05 total_time 722.0\n",
      "Saving model weights for episode:19800\n",
      "episode 19810, avg_reward 1419.7, memory_length 2000, epsilon 4.994940324440683e-05 total_time 725.0\n",
      "episode 19820, avg_reward 1246.1, memory_length 2000, epsilon 4.97002795564123e-05 total_time 724.0\n",
      "episode 19830, avg_reward 1228.3, memory_length 2000, epsilon 4.945239837799515e-05 total_time 721.0\n",
      "episode 19840, avg_reward 1274.6, memory_length 2000, epsilon 4.92057535121131e-05 total_time 721.0\n",
      "episode 19850, avg_reward 1385.5, memory_length 2000, epsilon 4.896033879263174e-05 total_time 721.0\n",
      "episode 19860, avg_reward 1404.3, memory_length 2000, epsilon 4.871614808417012e-05 total_time 721.0\n",
      "episode 19870, avg_reward 1342.5, memory_length 2000, epsilon 4.8473175281948e-05 total_time 725.0\n",
      "episode 19880, avg_reward 1308.7, memory_length 2000, epsilon 4.823141431163249e-05 total_time 721.0\n",
      "episode 19890, avg_reward 1270.8, memory_length 2000, epsilon 4.799085912918691e-05 total_time 722.0\n",
      "episode 19900, avg_reward 1350.6, memory_length 2000, epsilon 4.7751503720719e-05 total_time 721.0\n",
      "Saving model weights for episode:19900\n",
      "episode 19910, avg_reward 1436.0, memory_length 2000, epsilon 4.7513342102331256e-05 total_time 725.0\n",
      "episode 19920, avg_reward 1390.9, memory_length 2000, epsilon 4.727636831997064e-05 total_time 720.0\n",
      "episode 19930, avg_reward 1289.2, memory_length 2000, epsilon 4.7040576449280335e-05 total_time 720.0\n",
      "episode 19940, avg_reward 1358.0, memory_length 2000, epsilon 4.680596059545138e-05 total_time 726.0\n",
      "episode 19950, avg_reward 1338.4, memory_length 2000, epsilon 4.6572514893075036e-05 total_time 720.0\n",
      "episode 19960, avg_reward 1367.7, memory_length 2000, epsilon 4.634023350599676e-05 total_time 723.0\n",
      "episode 19970, avg_reward 1192.6, memory_length 2000, epsilon 4.6109110627169604e-05 total_time 729.0\n",
      "episode 19980, avg_reward 1354.1, memory_length 2000, epsilon 4.587914047850973e-05 total_time 730.0\n",
      "episode 19990, avg_reward 1264.9, memory_length 2000, epsilon 4.5650317310751274e-05 total_time 723.0\n",
      "episode 20000, avg_reward 1342.2, memory_length 2000, epsilon 4.542263540330329e-05 total_time 724.0\n",
      "Saving model weights for episode:20000\n"
     ]
    }
   ],
   "source": [
    "#start_time = time.time()\n",
    "agent = DQNAgent(action_size=21, state_size=36)\n",
    "for episode in range(Episodes):\n",
    "    \n",
    "    #print('episode:',episode)\n",
    "\n",
    "    # Write code here\n",
    "    terminal_state = False\n",
    "    # Call the environment\n",
    "    env = CabDriver()\n",
    "    # Call all the initialised variables of the environment\n",
    "    action_space, state_space, state = env.reset()\n",
    "    \n",
    "    episode_time = 0\n",
    "    \n",
    "    ride_count = 0\n",
    "    \n",
    "    while not terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        #possible_actions_indices, actions = env.requests(state)\n",
    "        action_idx = agent.get_action(state)\n",
    "        action = env.action_space[action_idx]\n",
    "        \n",
    "        #if (state == [0,9,1]):\n",
    "        #    print('Saw reqd condn')\n",
    "            \n",
    "        # 2. Evaluate your reward and next state\n",
    "        next_state, reward, step_time = env.step(state, action, Time_matrix)\n",
    "        #print(f'pres_state:{state} and next_state:{next_state} and step_time:{step_time}')\n",
    "        \n",
    "        episode_time = episode_time + step_time\n",
    "        \n",
    "        #print('episode_time:',episode_time)\n",
    "        \n",
    "        if(episode_time >= max_episode_time):\n",
    "            terminal_state = True\n",
    "        # 3. Append the experience to the memory\n",
    "        agent.append_sample(state, action_idx, reward, next_state, terminal_state)\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        agent.train_model()\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        #print('reward:',reward)\n",
    "        total_reward = total_reward + reward\n",
    "        \n",
    "        #print('reward_per_episode:',reward_per_episode)\n",
    "        \n",
    "        ride_count = ride_count + 1\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    #print('ride_count:',ride_count)    \n",
    "    \n",
    "    \n",
    "    # epsilon decay\n",
    "    #agent.epsilon = (1 - 0.00001) * np.exp(-1*agent.epsilon_decay * episode)\n",
    "    agent.epsilon = 1 * np.exp(-1*agent.epsilon_decay * episode)\n",
    "    \n",
    "    if ((episode + 1) % 10 == 0):\n",
    "        print(\"episode {0}, avg_reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode+1,\n",
    "                                                                         total_reward/10,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon, episode_time))\n",
    "        track_state = np.array(env.state_encod_arch2([0,9,1])).reshape(1, 36)\n",
    "        q_value = agent.model.predict(track_state)\n",
    "        track_state_0_9_1_act_0_2.append(q_value[0][env.action_space.index([0,2])])\n",
    "        \n",
    "        track_state = np.array(env.state_encod_arch2([4,1,4])).reshape(1, 36)\n",
    "        q_value = agent.model.predict(track_state)\n",
    "        track_state_4_1_4_act_3_0.append(q_value[0][env.action_space.index([3,0])])\n",
    "        \n",
    "        track_state = np.array(env.state_encod_arch2([1,14,6])).reshape(1, 36)\n",
    "        q_value = agent.model.predict(track_state)\n",
    "        track_state_1_14_6_act_4_1.append(q_value[0][env.action_space.index([4,1])])\n",
    "        \n",
    "        states_tracked['0_9_1_0_2'] = track_state_0_9_1_act_0_2\n",
    "        states_tracked['4_1_4_3_0'] = track_state_4_1_4_act_3_0\n",
    "        states_tracked['1_14_6_4_1'] = track_state_1_14_6_act_4_1\n",
    "        \n",
    "        save_obj(states_tracked, 'states_tracked')\n",
    "        \n",
    "        avg_rewards_arr.append(total_reward/10)\n",
    "        #clear it so that again avg rewards can be calculated\n",
    "        total_reward = 0\n",
    "\n",
    "        save_obj(avg_rewards_arr, 'avg_rewards_tracked')\n",
    "        \n",
    "    if((episode + 1) % 100 == 0):\n",
    "        print('Saving model weights for episode:{}'.format(episode+1))\n",
    "        agent.save('model_weights_{}.h5'.format(episode+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f364817d0f0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(track_state_0_9_1_act_0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(track_state_4_1_4_act_3_0)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f363bb986d8>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXl4FEX6x79vbnIAAcIZIBwB5JArIqIgCCKHiseqoOu9srrq6qq7grf4Y3U9d11dXFQWT7xZWeVG5BAChPuGAAECmISEIxDIManfH9M909PT3dM903Ml7+d55klPdVX3m56Zeqveeut9SQgBhmEYpn4TE24BGIZhmPDDyoBhGIZhZcAwDMOwMmAYhmHAyoBhGIYBKwOGYRgGrAwYhmEYsDJgGIZhwMqAYRiGARAXbgHM0qxZM5GVlRVuMRiGYaKG9evXHxdCZJipGzXKICsrC3l5eeEWg2EYJmogooNm67KZiGEYhmFlwDAMw7AyYBiGYcDKgGEYhgErA4ZhGAasDBiGYRiwMmAYhmHAyoBhGMY21uwvxd6i8nCL4ResDBiGqTccP1OJGkdt0K5/y/RcXPnW8qBdP5iwMmAYpl5Qfr4aOf+3GC/9sCPcokQkrAwYph6wYm8J/rvxSLjFCBtnKmvQ64WFAIAF24vCLE1kEjWxiRiG8Z/bP1wLALiub5swSxIeth85FZTrDn71J7Rp3ABfTLwkKNcPJTwzYJgQ8/K8nVi0g0en4YLIvmsdLjuH3P1lpuuv2ncc98xch9paYZ8QNmFaGRDRDCIqJqJtirIXiOgIEW2SXmMU5yYTUT4R7SaiqxTlo6SyfCKaZN+/wjDRwb+X7cd9H3ME3nBhpAuW7SlB1qQfcbKiKij3vv+T9fhpVzHKz9cE5fqBYGVmMBPAKI3yt4QQfaTXXAAgou4AxgPoIbX5FxHFElEsgHcBjAbQHcAEqS7DMEzQUI7DyWBqMO3nfADAjqOngyJHXKyzy62pDZ5Hk7+YVgZCiOUAzM6HxgH4QghRKYQ4ACAfwADplS+E2C+EqALwhVSXYWxBCIH1B09AiNBOw/cWleMv32yGIwKn/9HOOz/txX9+OWDrNYUQmt+RuBhnl+jw4/uz2ITpL0ZSRJH4PbFjzeAhItoimZHSpbI2AA4r6hRKZXrlDGMLczYfxY3TVuH7TUdDet8/fLYBX+UVYl/JmZDe97sNhZi19lBI7xlqXl+4By/+z1530H4vLcLgV5d6lcfEODvrGoud9YyVB/A7henv3aX5OH2+2qtenHT96jqoDKYB6ASgD4BjAN6QyrXmYcKgXBMimkhEeUSUV1JSEqCoTH2g6PR5AMDWIHmP+MLqgPLT3INYufe43/d77KvNmPzdVsM6Zyu97dOPfbUJE6bn+n3faEP5uRABJyqqUXjinFc9ubN2OKx9kO8szfd4/9qC3Zj6w06verGysgnixjd/CUgZCCGKhBAOIUQtgPfhNAMBzhF/W0XVTABHDcr1rj9dCJEjhMjJyDCVxpOp56QmxgMAzoR4gU42Qwv9sQ0AoLLG4fH+mf9uw28/XGP5fodKK5BfbC7sQY/nF3jcv8ZRi+82HMHq/aWW72uWakct3ly4W1MRhZLCExX4eXexh0nIyJtI7qwdQqC2VuCDFftN/Q8xGtesqHbgzUV7cOkrP3ldvzoClUFA+wyIqJUQ4pj09noAsqfRHACfE9GbAFoDyAawFs6ZQTYRdQBwBM5F5lsDkYFhlMSY7JTthqRJr6+ZwYOfbbTlfkNeW4qkePdYbu2BMgzo0MRnu67PzEe/do0Dvv/SXcVok94AXVqkaZ6fveEI3v4pH2erHHj26vD5iFzxxjJU1dTio3sG+K4MIFZh05/4yXos3lmEQ2UVmDKup4+W3tqAALy9ZK/n9aUvaFVNFJuJiGgWgNUAuhJRIRHdC+BVItpKRFsADAPwJwAQQmwH8BWAHQDmA3hQmkHUAHgIwAIAOwF8JdVlGFuQR31f5RWGfBHZDIt32re/4Hy1e3S59oD5Uf6GQycDvvfdM9dhpCoGT42jFp/mHkSNoxaV0sj3fLVDq3nIqKpxyuFQeO+QgXNpbKzz3N6iM67P6oyJmYHWbENrtiDXq1V8N0vKK/HAp+tN3SeYmJ4ZCCEmaBR/aFB/KoCpGuVzAcw1e1+GsYKy/xfC3g1GkUyVRRu3VU5VVOPxrzdj65GTWPPUCM06n+QexIv/24Fle0owpEtkmXWrHSbNRNLJM5XVXmVGlJRXepXFaLSTS5QL1H9fvAfztv2KQZ2b4faB7X3eK1jwDmQmorn9wzWYvnyf6fp7i93ePP64BxqxrqBM00MEUKwZhGkyIo+AZQ6WnkW5jqxKOk7+ET9sOYpx76zEodIK3Xq9pyzE4p1FKDrt3enJnKxw3i8Sd1fvLznrOlZ20cv2eDqmuG367g9ybUGZyzFBia9FYOV+BtmV1Ipr6flqR0hdUFkZMBHNir3H8de5u3D/J+vxwKfrfdb/cKXbH73Wxp75XJUDN723Gvd9ZLxzWGut4rYPcjHmHyu8yiuq3GaBb9YX4unZxl5BRnyae9C1v6Lj5B9x+Ws/4zfTVvtsVyuAhz7fiM2FpzDkNW9Xy1Ay4s1leGHOdlQ7al2LtnaZ+v42f5fruECh9O6csdajnqwMlO66B0srMPbtFV4mr5mrCgzvqZwYyAvGcpmZTr7bs/Px2FebfNazC1YGTFQwf/uvmLftV0tt7NzkWS1dbM0B7X2XRrtaf8kvxY5j3jtal+wsdh0/8fVmfLbG//0CZyprcOO0VXDUCsj9zO6icpyq8D07UPO/zUex8dAJy+2Uj2CdznMyIr/4DGauKsCdM9a6PKA+9fOZ/LSrCMUao3lfyK6llaqZ1vEzVej27HwPu37ZWeOQFco1gypZGcD3zKD8fDU2H3au64RyvwxHLWXqLL5mBnM2H8UfZ23E0ieGokOzFMO6el39jJUH8EnuQSTFx2qe36mhBGS0bMqBot4s1XvKQsvXeHiW0+Op4JWxmuf19kUoF2bnbHZ3YlmTfgQA5E8d7QrHYMSqfe7F8Gf/u82gpj73zMxDq0ZJltvFaq36Kjh9rhqpic5u09fYXvk8LnxhIV79zYWmZgb3fpSHtX4o00DhmQETtZyqqMZTs7diS+FJTZ979UYgNX+UOr31B32PgvV+ulN+2IEDx9326F3HPOW4+T19U00wFrdPnbM+E/CFei3Bn30RgHs9x6zpx0iRGiFf/9gp8zODc1UOVNY4fCqD42fcaya+/o0Y1bX+vmiP61grNlHZmSqcr3Zgg4nvYzBgZcBELf/8aS8+X3MI177zC0a86Z1qcNrP5haen/h6s886985cZ3he7oAeV12rymCR0V9dYNSZfrO+0M+r6rPXxOa22lqBtxbv8SpXSiqbXv7w2QZMmJ6L95btM9x8NVpjncUMVtdcz1U5cMFz89H1mfmaXkFKrn3nF9exr81oDTRmi/JsUGvW+tbiPZjwfq6ta11WYGXARBVqr5lgMn/bMRw96QxZsK7AeLSm5W0CGJsS9GYG6w+WIb9YP8ZRtYEbaTD2VpjpXMt1Okbloqtsw5+37Ves3l+KV+btQp70XE/4sL9bQasz7ZShbwa84Ln5rmMr61Kf5B40PJ+c4K0M5LWje2bm4fYP16Do9HmPtaKNh06GeLukG1YGTFRxrsrdufia0qs5X+3A3xfvcYWEMOoghBC4/9MNuOFfq7zOTfw4D79TeRWdUCzULt3tXhhW/rKf+96c/fvGaasx4s1luueN7M12eyLe/O/VeGuR94hfzWkd85RSGUz8xNsbLIaAKf/bgb4vLfJfSAXVjlqXi6uSeBNrFXbjy2S3Yu9xTPp2i1d5uNyTWRkwUcX2Y+4AdBVVvne3nqyociUqmfHLAfx98V7M/KUAgGfHqc48JS/E/nr6vNdoe+GOIsOdxLmKBVClq+nHqz1Hkmbt+7W1Aq/O3+UaWVcbuEkFamL4eHWBx/u1B8o0PaHU6O00rvGxGa5WOD8XM8zfdsxnnSvfXIaLpi72Kj9n405oIQT2m4hO62vmAABLd/sOwOnLa8kuWBkwYafaUYsTZ6tMLeTe+r578VL9Y5MXhJX0mbIIfaY4R51y+IaKKge2Fp7yWPhVe+EoO7HV+7RDPZSe0bYvN2wQ7zo26ptfX+g94j6ucc28gyfwr5/3udYjjDrYOQG6Ij73vTs6jBWTk96MRD2LUStdK5uqfthirAzyi8s99hAoOWdi4GCWOZuP4oOV5hSYEqthsWX+Nm+X70o2wMqACSvbjpxC9tPz0PelRbhxmrdJxgpKd0YtZKtSfvEZbFD50as7pXJFOIJbP9D2nhmuY8pRmq+Mfv5aO1hz/s97VCvXkxdbjbJk7VcouEAx03dtk0KF6wUGVC+gT/nBMy+BlYxfvmY9x8/oj6CLfSwMW2H3r+XIK7Du+umvDF/mHfZdyQZYGTBhZePhwIOmmUX2+/5xq/cIU90pPfqF752fWrZpwKl0Xl+w27UJTA+zA0W5Wu7+Mpw4W+XT9GIXZkbtV/9zJQD9DX7qzVvqXbtWZgZ6C+fVjlrsKSoPma29QXws9hSFNolRKOBNZ0xYCWUcOaP1ZmVn9v2mIx4bn6xCIJ97HAALawaKXm7DoROu3anB5rYPzCe/0ZsZ+Irbr2UaTIqP8YjIKqMXC+jF/23Hp7mH8NYtvU1IGjhmNs5FI3Xzv2LCSumZSjzz3626i4plZ6tcPtrP6OwwNQp9/ObC3X4F8FJvAlIizwy2HTmFR0zMCoyYOtc7w1UgKEe8awvKsCVEWdx8udPKnKtyYOzbKzXP+VIG/9LYC6I3A9Czua/Z7zTZlIc4oVFdg5UBYztvLtqDT3MP6S5m9ntpEUa8ucxlb1YihMDsjYXo9ux8LN1VrNEaePunfCzYbi1O0ay1h/Dagt2u9+rdrbLHjBkPpVCjVHz/XrbftfchUjh6Sl8ef/aF6Cl6R61A0enz+FW1s1je2WwUH8pOwrUpLNiwMmBsJyHO+bXSC/cMOEMFyPZmJUIA86WNPx+s3K/b3qp3iDpP8BfrPBfl7pCiV0Zi/oNXFUoMQFTZq+2UtcYhcPFfl2Dgy0tcZUrF6G8cI6uEMqx0KGFlwPjF52sOabpBAnAF8vJnlL1sT4krHr7RAMyKF4oZhHDOSoxi+ocLvRg9DwztFGJJtAlVRrnCE56fzeyNhRikyC8cKiIxf7EdsDJgLHPg+Fk8NXsrHvxsg+Z5ebruTx9x98x1prxsjEIy+MsdM9Z6xRaKZBJCvJDZO7ORZrkyXk8wOaowD52rcuBPX/r+rF6/yb2o/NnvLrZFjoXbzSfvaZqSYMs9Q4GVHMgziKiYiLYpyl4jol1EtIWIZhNRY6k8i4jOEdEm6fWeok1/KW9yPhG9TaEy9DG2IduBfe2M3F10Gr//xBmDxR+MlEkwpuordEIzRyqyOS5U6IX/CMc6y8vzzC3SZzVNdh23VxybQS+s+e4i34H7ZKKpe7PybZoJYJSqbBGAnkKICwHsATBZcW6fEKKP9LpfUT4NwEQA2dJLfU0mSvD1PZ+79Vcs2F7kdyer564ImEtSXtdJlJRBzzYN8dpvLrTcflCnppbqB+JS+e0Dg/xu++zV3b3K1KE99FDGJLKqPJ+9+gJT9bSik8pEkS4wrwyEEMsBlKnKFgoh5F9lLoBMo2sQUSsADYUQq4XT0PgxgOusicxEPDbZkHP36+/yrLQx1kyk8OMfL7NUf3SvVrgoKx3v3toPl+skoF/55DDd9u/fkWPpfnEWAwMqiY/1v+3oni39bquczSTG6nfaWphJPvTpvRcjKd6zG310RDZGXNACAHyGxDbCKJBiMLBznnkPgHmK9x2IaCMRLSOiwVJZGwDKgOuFUhkTRRiN2Bn/iYvR/jle2b2FZnlqYhy+vn8Q2jdNQfOG2lm9khPi8PE9AzTPpSRa23NqNUqsXW0TbTKHKWcG1/Zubam+HpdlN0NinKeSyUhL9Ixc6ycXtGoIAGielhjwtcxgy1MmoqcB1AD4TCo6BqCdEKIvgMcAfE5EDaG94VS3ZyGiiUSUR0R5JSW+o/sxoUEe+JPF/cMl5ZVY50dMFy38DfoVCfRr11izXM8KoxUXHwDSVJ35+Ivael+TyHBEr9VGDzMzg5eu66nT1v+uJtHADOML5SRV2bmbWT9Qd/KPX9lFs95rN3mb6ALQfWEjYGVARHcCuBrAbZLpB0KISiFEqXS8HsA+AF3gnAkoTUmZAHSjiwkhpgshcoQQORkZ2tNgJnxYtYfeMO0X3GSQBtIK0ezrfeegLM1yPbOEniujekf1yzf0wtherVR1jIPladnj9TCzZpCRmqiZezhOZSa6xsTIHABm3TfQ5aocCFlNky3PTtQzkoeHZ2vWG5zt3TcZDZT0lDsAPDWmm+vYVyY1uwlIGRDRKABPArhWCFGhKM8goljpuCOcC8X7hRDHAJQT0UDJi+gOAN8HIgMTPnb9Wm7J5/pwmX07Z6N5ZqDslJQdnbqz+lZKYWk2MB0ReXVgvuzeVjpIc2sGAnP/OBgpqg5P2Xb67f3xtxt7ud7fpaMcUxPjcInFRW41HTNSkJIQi+ev6eGz7v2Xe+7b8JUQZ8KAdprlBDIcKOntYN7ywkhMHOKW4Z7LOjivF6JZhhXX0lkAVgPoSkSFRHQvgHcApAFYpHIhHQJgCxFtBvANgPuFELJ94AEAHwDIh3PGoFxnYCKA42cqkTXpR+Tu1w7WpvTk0XIrDEU3vfaA/+amVZOusFES6yg76NsvaQ/AuVio7pjlPQ8Ld5j3a79toGcHFRtDhuv5VjKAmZkZ1AogPSUBI1TrHEoTzcgeLZGc4FaCbZtom2z8WWfITG+AxY8NQcuGSXhqTDekJMZh+5RRGNatOQBg6RND8fE9A3Ts1Z4PSj2bUZOe7M5b8f4dOchunup6Pzi7mW67W3KcprkdU65CwStjXeUNk9zXa9O4AbKbpxne325Mz7+EEBM0ij/UqfstgG91zuUB0DYsMhHBxkPOqJgfrNiPgR29R2bKVH3hMtdsDSBYW1qSf2aHi7LSTQdvM0KpDDplpGJk9xb44/Bszc5PL4GOHv3bN0HBK2ORNelHAL5HlVY6XDNVZcXz4rU98L0iNlXjBt6br9KT43Giohp6fa5yNtGlRaqp0BZCAJ2bpyH3qeGa5zs0S0GHZilYc8B7oKNWmla8p67s3gJLdha54iQ9OaobFu8sRpvGDXBEFUtq8pgL8IdhnT0UopLFjw1BRlqSrQl5zMAhrBkv5J+AVj8vhMC+EncCFS1l8M+fvMM364UfDgZ/GtEF7y3bp5vq0F/PFq2omI+OyMbZyhq8v8J85ivl7eNjCdMlF085raWSQPdTxBLZ5v1lzkjkvJdylAs4w1IP7NjEw104JTEOJyqqka6zS1f5Ob18Qy/cOM33epPZwcnZSud3IzO9AQpPODtrOazG+IvaolWjBmgSwO7hDMkD6P6hnXBhm0ZITYrD8j0laN24AZLiY5FksCjeWZoRsDJgwo7s+KEVc0b9W5NjBG0pPImys1UY2rW55jVX5odud++Dwzrhp11F2FyoPXsw4z+uhZYS6dYyzXCWclnnZl7/u/I6+cXu0a5WiO3/+cje5ovYGEKbxg08ykb1aIm+Co8m5UzCCDMmJfkro/5fiAiz7hvoMfqW15s6NEvBDw9fhqdnb/X4zMYoFsP7t2/i894A4DC5x6VZqrOjH9ChCQpPHAHg/m53ykjFfUM6+lTEY1SL9b+/vBPWHzyBUT1bonFyAg68PMZjB3KnjFT1JQxp0TARd17SHuN11ibshmMTMV7IX2CtQVbpWU+zhby4ee07v+Cu/6zTvWYoo/7GxcYYdgpqZWDGj/25q7trKgMiMoyTlBQfg55tGureX7kQrs4PDAClASZDJyJ0zEj1WCcZ0b0Ffn+59SB3T43xvSPX6GMmIg8lIT+3xLhY9GzTyGtN4vlrzHs6yWg9Qy1+f3knvPfb/hjVw72hzeUyLYmo/LS7tvC23/ds4xmrqUOzFCx67HLXjMJsKIq3J/TFDw97bzgkIrw4rqdrv0GwYWXAeOE2E7l/WL+eOo/y89V4db5nOGWz3kR3z9RXFMHASCx1n27GNqy3EBtL5OXto1Qaq/aVerVTdohKRZSqsZZhlxJtrZgdaHmz9FJ0bCMu0J7dpackYP6jgzXPyShnk/LazGM6/vnVUowreQevUq6UhFifnenH9wxAt5aenbTZmUF8bAxG9WzpcQ/5/nKZ8vaTRnfzaG+nh8+1vVt7KZZwwMqA8ULLjDLw5SUY8/YKL9t/tUN4xJSf+uMOdVPbmKz6Qap5akw3rH3auXBoNEJUj/DNjOBiSHvnNZF3OG2lu2hyQqyrQ1d6n8goTS9aC4qr9tlvXtPy2//mgUtcx0dPeq9dyHRraX6U2l0a0XbRGFUDQJVDVgZO+7nyI9P6Dn7/4KUe74d0yUD31p7yWO2jtRQjuf66r1ZZ42m/72zR5BMNsDJgvJB/h+rfyeGyc17dYdHp8/hSkSjGykKqVRprdKZKUhPj0TzNueHJaIRodvqujKdDRJpJ32MUZiJ5BKxUNrXCbTqRA5ops3/5ygUQjEQ2WrF+1LttZV690VoAPOW/4zK36DxueVYpz448noVGm95tNXZuC8O3PlEOGmTFEKMht9oZ4fP7Blq8U+TDyqCesOPoaa+OZ9W+45phqOURkdZIWD3gvmPGWvxjyV6/ZPrH+D64rLO+P7aaWB8hDZQ/XrO2YwB46IrOmuW3XdxecW/S7GhiYsg1W2qWmoj+7dPx1i19XOdrHLWu554kbcRS5nc2awZ6aJi2jP7gSxkqH7Psn28WzdmTTl35I3LPDNxt9Rb5/3PXRaprBGZHk2Vo07gBMlKdHkCyd5Py2uernZ/xjilXYceUq1zeQnUJVgb1gHUFZRjz9gp8uNI9aq+tFbj1/TW47QPvXAPyyMjmZGJeXNa5mU/bqzJcgmzbv66P71AGZm3HgPfOU9c1agVuznFGT4kh7VF8DLnvFR9L+PaBQR4RRKsUZrWkOG9lYFZn9W+f7mG3/vNVXc01VPD6Tb01FyrVKDti+dCsm6XWY/elfOQkPcrvm14TtXJKDjBUhay8+rRtjPuHdsKbN/d2BbFTmvBkN8/khDjd/QHRDiuDeoCcLlBOQF96ptLVgWmmVJR+iDs0ztmZ4rBpqu/RlTI8sDyCbNdUO7Svsv/454S+aNkwCf++vb/f8mWkJbpmI3ExMZodnWfH6d2DJcXHoqMUirip5M5YqTATmR3ZOmqFS2k1TIrDg37MFH7TP9PUQmWMxqKq2a0ZnpYeaSFWp64825EX1JXPwqztf/LobuijZT4yiayMiZyd/w39Ml2fY3xsDKaMc4axaN1YOypsXaJuqjhGEwGg4PhZDH39Z83ImSXllVi6uxh/+ca5w/jUOe+E9kdP2hdfyAzK1I5Xdm+Bt27pjWsubI23fZimLsxsrLsL1Qz/GN8HY3q1QkWlAykJsbimd2v8e/k+1/nEuBhU1tT6nNl8/ftL0KpxA9yc0xbxsTFYsfc4+rVLd51Xq4K+7Rq7doArkTvKFX8ZhmYmlGggKNc8XPsGFP9os9QEHD+j7fKa3cJ7YVXvGT1xVVc8oZjhKBWJ0V6Qjs1SXPdJS4rHI8Oz/fZWk82Jeve7fWB79GjdSDfSbF2CZwb1CCGAQ2XOWcIGVYdTUl6Ji6YudikCmctfW4rDZe5E5Op2/vKqycxcSj/82BjC9X0zERcb4xWCOD6WMMpHEpTuBv7aDyvWDXq0bohxfdogPjYGjZLj8czV3ZEQ554ZPD3mAvzw8GUY1jXDo2PXIrtFGlIT4zC0a3Nc2rkZCl4Z6+kBo5oZ6PmUy8qgbZNkNDCIemkHylmA293SXbb0iaFY/mfvpDlrnxqOCzO9O02zbphXKT6/Jw08x356Yij+fbsiMY/i+lZzH8izTS1PL8A52+vfPj2q0lf6C88M6gFTf3Tmi3UIofvDHPDXxZrlB0srvBSEHaQne27MuSWnLb7MO+xVTy98sdq68sPDg9E42diu/dX9l+CEziaux0d2RVbTFDz+9WZdV8gOzVKwt/gMRvdqicz0ZPznbilpTACWM7NrBpdaWGgPFGXHpzUzSEuKR4qG3VydYMe9ectcR/ro8GxkNm6AzPQGGGTh/1Ve/bPfWfPyGdm9BZ4ZewFuvTg0u3wjGVYGdZjNh09i3Lu/uN7XOGp1p8NGputgZDZT28pH9WqpqQy0zA5aMpmxaacmxpmKja93qTdu7o28gyeQma4dZVPZ7n8PXYYDpWc16ylR/x96n0NakrFbrZ0o4/vInf7Qrp4x+73DTRhc0OSgOiaGcLOFZDuudtLNkxNi0bm5Nf//mBjC7wZ3tHzPugibieowH60u8HgvhPVNOcFCttUOkzqZ9nphjInw+JVd8Nfre3mUqztNo86ofdNkU2kO3X7x2hdLS4rHMI3YS1r9d6/MRqbu2aqRZ9wgf7yE7GLZn4di3dMjPDYWNkqOx8onh2HKOO9AwwdeHuM6NhpMBPs7pxU+grEOzwwimFPnqpEUH6O7IcgqtX5qAzmqo53Ig8+7BmVhXJ82uq6LMTGkmWHKu/PR/8eWadi3tRh7YSusKziBJ0Zqh0/whT9m5VtVQciapCQgq2kyCkorDBdqg0F7yUsrMz3ZI2Cc7kyICC9e2wPPz9muuSHQl3K1i+LTznhZZ0Mc5bOuwTODCKb3iwsxYXqu3+3VttrFO4vxxFebver5chcNjjJwx4Ex8mGPNdmR+Jtz9qb+7iysiXGxePmGXqZcXpUE4m6rFalUXtQMRBG8fEMv3OanHfyVG3v5riQhz36Uu6rVBHvEXlxuLecDow0rgwhH6b1zsPQsZq09ZLqtVj969JR33Jkv1nnb6oONWf96PV2g7oD9HX2+dlNvv9oFE6seMVpMGNAOU68336krsbI+IQejM0oYFGxHHDlb3B+GWo/EyrhhM1EUceO0VTh+pgo357Q1TNBy+nw1thw2nwnskMJ1NFRo6YKCV8bikpeX4JhCYenpDHWx1f5m/TMjcN5gNBtOXr+pN95ctAdVSj9FAAAcUklEQVRjerXCw7M2hk2O8SYWc+NiY/DGTb2Rk+XtYmvWiyhQUhPjPNJHMv5hSRkQ0QwAVwMoFkL0lMqaAPgSQBaAAgA3CyFOSAnv/wFgDIAKAHcJITZIbe4E8Ix02f8TQnwU+L9S95HNBs5RsfYPraqmFhe+sNDSdaf9vM93JZvRmxmcVSUU0atnZQFZC6umIDPYNQLObpGGab917pwOlzKw0rneqDC1aRHKXBaM/1idj84EMEpVNgnAEiFENoAl0nsAGA0gW3pNBDANcCmP5wFcDGAAgOeJyHjnDuOB0W+rxGLO3HBxUZZ25ip1SGK9/7VKFUrb31SWdhDMvq5vPdj5ykQGlmYGQojlRJSlKh4HYKh0/BGAnwE8KZV/LJzD2FwiakxEraS6i4QQZQBARIvgVDCz/PoP6iHq0fKh0grc9Z+1mDUx8sPqrvjLMLTVcSMF3MHKMtISUVJe6Qr7rKZaZeKJ8xHRNBQEwyzyzf2DAo7MGW7qwebdOoEdv6AWQohjACD9lR2x2wBQrkwWSmV65YxJ1H3D+yv2Y//xs5i79Zil0M2B8tXvL9E9d4VO6GMjRQC4R/x/v6UPvpg40CNDl5JK1cwgLjaMM4MgPvLYGDKVeziSiXJdVm8I5rdM69epZ+zW/LoQ0UQiyiOivJKSEluFi2bUP66TUkC5JikJHrF8gs2ADtqmnqfGdNM1A/nihn7OcUHXlmkY2LGpbj1Z6cmjTjOpK5nQwjOC6MIOZVAkmX8g/S2WygsBKN0RMgEcNSj3QggxXQiRI4TIycjI0KpSL6kVAt9vOoK/zt0JIYSHz77ZnMSBMkvK9KTlzieEtt//5udH+rzu7QPbY/9fx/iMzPntA4M88hCok6kzDGMNO35BcwDcKR3fCeB7Rfkd5GQggFOSGWkBgJFElC4tHI+Uyhgdlu3xnBWVna3CI19swvTl+3G47JxrXkUADhz3HQ/HDuRE5Hojcq1NSI0a+PZfJyLNjVhqerdtjEmju7lmSZEwM+CRsDZsJYoOrLqWzoJzAbgZERXC6RX0CoCviOheAIcA3CRVnwunW2k+nK6ldwOAEKKMiF4CIAcgnyIvJjNulJuqjqh2AJ9RuF+ePFelSEBCePSLTSGRT04N2Cbd26Zffr4GvTJ9J1Gxk3AqA+7smLqAVW+iCTqnvLKISF5ED+pcZwaAGVbuXd9Qmv7VkS3VuVnlt0TA1Re2wtfrC0MhIgDgxn6ZePLbrR5lRafP44keoQ24Fk7XUoapC7ChNQKpqqnFZ2sO6p5XLiAr1wxiCEgJICfsi9f2MFXvhWsUeYljY5ClSjQTwjVs9JZSHoYz+YidqUDrEvJsrSZE61hMYLAyiECm/bwPz32/Xfe8su+pFcDCHUUAgM/XHvYwIVnFbAatDhmeMePvG+KMB/+SlC9W7hx3TLkKL13nHfrYTj65dwDmPzo4qPcwixWFlN081e/getGCnDi+gqOJRgWsDMJIVU0t8gq8l0t+PW0cJfSad1a6jie8745qunxPCb4JwET0m36ZHukf9VD3Ybdd3B4Fr4xFqhSszCEpg+SEOK8QzXbTMCke3Vrqp7OMVBY8OgR7p47xXTGKkQcX51gZRAWsDMLIK/N24TfvrcbOY6c9yitVnjhCODv6YBMTQ3h8ZFdsfPZKUxnBvNpLI2OlmaiOD379JiaG6vw6xzVSeOsL24bWmYDxD1YGYWRPUTkA4LgqnpDaLdNRK3DHjLUhkys9JUE33aQRbmXg1gb1wd2SVwy0ubxLBg68PCYqZ271EVYGYUQeGd7+4VoPBVDj8OxeQrmrWMYwjaFOBy8rA+GhDOqBNmB04c8/emBlEEaUvvFlZ91ZrdSByfzxVplxV47/ggFo5yOGkBbyv+MIg/IKJ9dc6DSH9GrD5hAmemFlEEaUwdWUMwN1V9rQQuYpmY7NrJl51AO4l2+wniWLNNYM6gOjerZEwStj0aFZSrhFYRi/YWUQRpRhl6scbo8L9UTAHzOR1RbqHbxa+xXk/QR6oZrlS7DfPcNEH6wMwojSm+R8dS2qamrx4coDWLyzyKPe/pIzlq+d1TQZEyX/f6uyyDRP8wwWpxdOWkbLmwgAWjZMwjNjLzAtC8MwoYeVQRhRjsZPn6vGPTPX4aUfdnjV+2DlAcvXJiI8NcbdAWdKMYRuH9hes/7QLt75B9Y+PQJ3DcrSuLb2PS/q0AQtGybhj8OzPcpznxqO3w02r5gYhgk9/scuYAJGORq/9YM1Qb3XyievwPlqB5LiY/FJrneoi7+P76PZTiljTlYTrNpXihYNtcNLN2oQj9ynvMJUMQwTBfDMIIyEOjtXkpRC8vsHL9U9p0Y5e3lkeDaWPH45OjdPC46ADMOEDVYGYSRYO1B95Q3IkNYCfCWQAYAxvVq5jmNjCJ0yrG9GYxgm8mFlEEaClcT9oqx0w/OtGzfAW7f0xsI/DfF5LTkqKMMwdRtWBiFmzf5SvLs0H4Db+8ZuJo327blzfd9MNElJQMMkXjZiGIYXkEPOLdOdUUYz0xsg2WTIaKso7fxdW6ThUFmFbt3lfxkWUNhrhmHqBqwMwsQjX2zC41d2Ccq1lW7+vmL9N05OQOPkhKDIwTBM9BCwmYiIuhLRJsXrNBE9SkQvENERRfkYRZvJRJRPRLuJ6KpAZYhWqm2K2/DnqzxTTCbGuT9WIgo4WNi3DwyKmAQyDMMEh4CVgRBitxCijxCiD4D+ACoAzJZOvyWfE0LMBQAi6g5gPIAeAEYB+BcRBcdeEuG8vWSvLddRevwAvncKW6V/+3QOQ8wwdRy7F5CHA9gnhNBP4AuMA/CFEKJSCHEAQD6AATbLUa/gIMEMwwSK3cpgPIBZivcPEdEWIppBRLK/YxsAhxV1CqUyRoPemb7DInPIeIZhAsU2ZUBECQCuBfC1VDQNQCcAfQAcA/CGXFWjuabxnIgmElEeEeWVlAQ/7WMk0qk5b/JiGCb42DkzGA1ggxCiCACEEEVCCIcQohbA+3CbggoBtFW0ywRwVOuCQojpQogcIURORkaGjaJGD1Ov851XoL7lD2AYxn7sVAYToDAREZFyVfN6ANuk4zkAxhNRIhF1AJANIHQJfiMIM/sMGpioo8yM9t5v+wUkE8Mw9RNblAERJQO4EsB3iuJXiWgrEW0BMAzAnwBACLEdwFcAdgCYD+BBIYQD9ZCMtER0sZh4fnTPll5lcjKZThkpGNWzldd5hmEYX9iy6UwIUQGgqarsdoP6UwFMtePe0cT5ak+dd7isAhe0suayOaxrc8zb9qvrfbsmyS4zUbDCWzAMU/fhHcgh5LUFuz3e1wogPtbi5EzR3//nrovQo3VDlFVUAWBlwDCM/7AyCCHF5ZVeZQlx1pSBssMf1s2ZnazkjPO6rAsYhvEXjloaQnL3l3qVxZtIcLPh2SsxsnsLADp+uZKZKFj5ERiGqfuwMrCBqppaXPPPlVi177hhvRKNmYEZ006TlATXDEIrBUJWsxQAzkxkDMMw/sDKwAYOn6jA1iOn8PTsbb4rq7Bq5yeNuUFqYhwKXhmLkT28PY0YhmHMwMrABgIxzizbU4J/Tuiree6BoZ2878WWIIZhggArgwhgbC/tvQFPjuoWYkkYhqmvsDKIAGIsLPwGmpuAYRhGC1YGNnLg+Fm/22amG+cgkENX1HIgIoZhggArgwhh5ZNXeLxPVO0/mDz6Aoy/qC2GX9A8lGIxDFNP4E1nAXKuyoEr3ljms17R6fOa5fcN7uBVtuDRIUhPifcoS09JwCs3XuifkAzDMD5gZRAgx06dM1VPa48BAM1k9F1bpgUkE8MwjFVYGQRITYA2fH92DbdulIT7hnQM6L4MwzBKeM0gQGocnsrg3aX5mvWEjs5YuqvY8j1XTR6Ouy/1Ni8xDMP4CyuDAKmprfV4/9qC3dh8+KTr/dGT5/B13mGPBDRKCk+YMzMxDMMEE1YGAaJlJlq4w51vYPJ3W/Hnb7Z4rC00iHdnL+NtAwzDRAKsDAJEbSYCPBXE9qOnAACVNbVe9QBWBgzDRAa8gBwgszcWepXVOAS+XV/o4R5arVAaSgWgFXiOYRgm1LAyCJBZaw97lTlqBR7/ejMAZ/hpQN+1lGEYJhKwzUxERAVEtJWINhFRnlTWhIgWEdFe6W+6VE5E9DYR5RPRFiLqZ5ccoSYt0VufzlxV4DouO+tMSfm3+bs023M+GoZhIgG71wyGCSH6CCFypPeTACwRQmQDWCK9B4DRALKl10QA02yWI2RM9MPfX9n/c+A5hmEigWAvII8D8JF0/BGA6xTlHwsnuQAaE5F2HOcIp6C0wnKb56/p4TpWqoIvJw7ER/cMsEEqhmEYa9ipDASAhUS0nogmSmUthBDHAED6K0dZawNAaWwvlMo8IKKJRJRHRHklJSU2imoPp89X49sN3gvIRsx7ZDBuvqit673SF+nijk1xeZcMm6RjGIYxj50LyJcKIY4SUXMAi4hI20juRDOvu1eBENMBTAeAnJyciIvdXFHpsNwmTrVIIPS2JjMMw4QQ25SBEOKo9LeYiGYDGACgiIhaCSGOSWYgOfZCIYC2iuaZAI7aJUuw2VNUjk2HT+Iv32yx3DYu1jkZS4iNQZWjFtf3zbRbPIZhGMvYYiYiohQiSpOPAYwEsA3AHAB3StXuBPC9dDwHwB2SV9FAAKdkc1KkM3frMYx8a7lfigBwzwxaNEoEANzQz8s6xjAME3Lsmhm0ADBb8oyJA/C5EGI+Ea0D8BUR3QvgEICbpPpzAYwBkA+gAsDdNskRdHb9Wh5Q+wQpaQ1bhxiGiSRsUQZCiP0AemuUlwIYrlEuADxox71DRX5xOUa8uRwjdDKN7ZwyChc8N9/ndeT0lQzDMJEExyYyyYLtRQCAxTu1Q04nxcfgN/192/+TE5z694Z+zrqNk+ONqjMMw4QEVgYmcfhIYkNEeP0mr8mRF3Iymz+NyMbOKaOQlsTKgGGY8MPKwCSBZjRTQ0RowCYjhmEiBFYGJqk1qQw+5h3EDMNEIawMTGI0M/hl0hWu41aNkkIhDsMwjK2wMjCJXtpKAEiKcz9GDjzHMEw0wsrAJFoZzWSUtn852kRaYhy2vjAy2GIxDMPYAisDkxjNDBLjlMrAPTNIS4rHtw8MwvxHBwdVNoZhmEDhTGcmqanVzmEMuN1FAe+cxv3bp7sWn0f1aBkU2RiGYQKFlYFJHPq6wAPXzEChFGJiCLmTh3vkRGYYhokkWBmYxKEzM/jsdxd7vNdbP27JXkYMw0QwvGZgEr2ZwaWdm3m8j2FvIoZhohBWBiYxm9HMpQw4KinDMFEEKwMTWMlGFsMTA4ZhohBWBiawEpaIN50xDBONsDIwgZFbqRqeGTAME42wN5EB+0rO4EDJWTSykHOAF5AZholGWBkYMPyNZZbbsDJgGCYaCdhMRERtiWgpEe0kou1E9IhU/gIRHSGiTdJrjKLNZCLKJ6LdRHRVoDJEFKwLGIaJQuyYGdQAeFwIsYGI0gCsJ6JF0rm3hBCvKysTUXcA4wH0ANAawGIi6iKEcNggC8MwDOMHASsDIcQxAMek43Ii2gmgjUGTcQC+EEJUAjhARPkABgBYHagsoaJH64Z4aFhntG+a4nWOrUQMw0QjtnoTEVEWgL4A1khFDxHRFiKaQUTpUlkbAIcVzQqhozyIaCIR5RFRXklJiZ2iGrL58ElkTfpR9/w/J/TF6F6t0L11w5DJxDAME0xsUwZElArgWwCPCiFOA5gGoBOAPnDOHN6Qq2o01/TkF0JMF0LkCCFyMjIy7BLVJ0t2FhmeT03Un1DFSb6lnZqn2ioTwzBMMLHFm4iI4uFUBJ8JIb4DACFEkeL8+wB+kN4WAmiraJ4J4KgdctiFr41jyQbKIDkhDh/fMwC92jSyWyyGYZigYYc3EQH4EMBOIcSbivJWimrXA9gmHc8BMJ6IEomoA4BsAGsDlcMuTlZUYdW+47rnv/vDIMOZAQAM6ZKB9JQEu0VjGIYJGnbMDC4FcDuArUS0SSp7CsAEIuoDpwmoAMDvAUAIsZ2IvgKwA05PpAcjyZPogU83YF3BCd3z/dql655jGIaJVuzwJloJ7XWAuQZtpgKYGui9g8Ghsopwi8AwDBNyODaRipTEWN+VGIZh6hisDFSQwRbi3w5sF0JJGIZhQgcrAws8fmXXcIvAMAwTFDhQncTyPSV4Y+Fuw3DVHISOYZi6CisDice/3oyS8kqk64Sr/sf4PpZCWTMMw0QTbCaSkMf8eknNxvZqpXOGYRgm+mFlICFbgE5WVGueZxMRwzB1GVYGEtUO40THMZzPkmGYOgwrA4mys1XhFoFhGCZssDIAUHqm0vD88j8PC5EkDMMw4aHeK4Odx06j//8tNqzTrmlyiKRhGIYJD/VeGYz+x4pwi8AwDBN26p0yEEKgxqG/sUzNKzf0CqI0DMMwkUG9Uwaf5h5E56fnobj8vKn6qUm8L49hmLpPvVMGszceAQAcKq1AVY35GQLDMExdpt4pg7gY57887ed96PLMPFzRrblhfaMopgzDMHWFeqcMYqXNY0t2FQMAfpL+MgzD1GfqnTKwGlWCo1AwDFMfCJsyIKJRRLSbiPKJaFIo7rlkZxFW7Su11IZ1AcMw9YGwKAMiigXwLoDRALoDmEBE3YN930U7iiy34ZkBwzD1gXDNDAYAyBdC7BdCVAH4AsC4YN+UO3aGYRhtwuVE3wbAYcX7QgAXqysR0UQAEwGgXTv/8w+fr3bg1vdzseHQScttUxJ5nwHDMHWfcM0MtMboXjGkhRDThRA5QoicjIwMv2+24dAJvxTB327shcs6N/P7vgzDMNFCuIa9hQDaKt5nAjgarJvJewuscstF/s9GGIZhoolwzQzWAcgmog5ElABgPIA5wbpZXCwvFjAMwxgRlpmBEKKGiB4CsABALIAZQojtwbpfHGcpYxiGMSRsq6NCiLkA5obiXrGsDBiGYQypFzuQhXF6Y4ZhmHpPvVAGjlrf2uCb+y8JgSQMwzCRSf1QBiamBjlZTUIgCcMwTGRSL3ZU1ZqYGSh54ZruOHWuJkjSMAzDRB51Xhlc8frPaJQcb6nNXZd2CJI0DMMwkUmdNxP9evo8NmrsPn5gaCfXcUZaYihFYhiGiTjqvDKI1YlONzjbGWbi6/svwbqnRwAAmqUm4KoeLUImG8MwTKRQ581EepFKB3Vqhr1TRyM+1q0P8565MkRSMQzDRBZ1f2ZgsOFMqQgYhmHqM3W+N+TdxwzDML6p88rg+Jkqr7LebRuHQRKGYZjIpc6vGSj5w9BO2HT4JD651yuPDsMwTL2mXimDv4zqFm4RGIZhIpI6byZiGIZhfMPKgGEYhmFlwDAMw7AyYBiGYcDKgGEYhkGAyoCIXiOiXUS0hYhmE1FjqTyLiM4R0Sbp9Z6iTX8i2kpE+UT0NpFewAiGYRgmVAQ6M1gEoKcQ4kIAewBMVpzbJ4ToI73uV5RPAzARQLb0GhWgDAzDMEyABKQMhBALhRByFphcAJlG9YmoFYCGQojVQggB4GMA1wUigy+++8OgYF6eYRimTmDnprN7AHypeN+BiDYCOA3gGSHECgBtABQq6hRKZUGjb9vG+PNVXTG2V6tg3oZhGCaq8akMiGgxgJYap54WQnwv1XkaQA2Az6RzxwC0E0KUElF/AP8loh4AtNYHdHNSEtFEOE1KaNeunS9R9a6BB4d19qstwzBMfcGnMhBCjDA6T0R3ArgawHDJ9AMhRCWASul4PRHtA9AFzpmA0pSUCeCowb2nA5gOADk5OdYSGTMMwzCmCdSbaBSAJwFcK4SoUJRnEFGsdNwRzoXi/UKIYwDKiWig5EV0B4DvA5GBYRiGCZxA1wzeAZAIYJHkIZoreQ4NATCFiGoAOADcL4Qok9o8AGAmgAYA5kkvhmEYJowEpAyEEJrGeCHEtwC+1TmXB6BnIPdlGIZh7IV3IDMMwzCsDBiGYRhWBgzDMAxYGTAMwzAASNoaEPEQUQmAg342bwbguI3i2AXLZQ2WyxoslzXqolzthRAZZipGjTIIBCLKE0LkhFsONSyXNVgua7Bc1qjvcrGZiGEYhmFlwDAMw9QfZTA93ALowHJZg+WyBstljXotV71YM2AYhmGMqS8zA4ZhGMaAOq0MiGgUEe2W8i1PCvG92xLRUiLaSUTbiegRqfwFIjqiyA89RtFmsiTrbiK6KoiyFUh5qDcRUZ5U1oSIFhHRXulvulROUq7qfCnXdb8gydRV8Uw2EdFpIno0XM+LiGYQUTERbVOUWX5GRHSnVH+vFO49GHKFPRe5jlyWPzu7f7M6cn2pkKmAiDZJ5SF5XgZ9Q3i/X0KIOvkCEAtgH4COABIAbAbQPYT3bwWgn3ScBmeO6O4AXgDwhEb97pKMiQA6SLLHBkm2AgDNVGWvApgkHU8C8DfpeAyckWUJwEAAa0L02f0KoH24nheckXf7Adjm7zMC0ATAfulvunScHgS5RgKIk47/ppArS1lPdZ21AC6RZJ4HYHQQ5LL02QXjN6sll+r8GwCeC+XzMugbwvr9qsszgwEA8oUQ+4UQVQC+ADAuVDcXQhwTQmyQjssB7IRxis9xAL4QQlQKIQ4AyIfzfwgV4wB8JB1/BHdu6nEAPhZOcgE0Jmcu62AyHMA+IYTRJsOgPi8hxHIAZapiq8/oKgCLhBBlQogTABYBGGW3XCICcpHrPC899D4723+zRnJJo/ubAcwyuobdz8ugbwjr96suK4M2AA4r3gc937IeRJQFoC+ANVLRQ9J0b4Y8FURo5RUAFhLRenKmFgWAFsKZfAjS3+ZhkEtmPDx/oOF+XjJWn1E4ZLwHnjlCOhDRRiJaRkSDpbJQ5iK38tmF+nkNBlAkhNirKAvp81L1DWH9ftVlZWAp33LQhCBKhTO3w6NCiNMApgHoBKAPnLmi35CrajQPlryXCiH6ARgN4EEiGmJQN6TPkYgSAFwL4GupKBKely/0ZAn1s9PLRd4XwGMAPieihiGUy+pnF+rPdAI8Bx0hfV4afYNuVZ372ypXXVYGhQDaKt4b5lsOBkQUD+eH/ZkQ4jsAEEIUCSEcQohaAO/DbdoImbxCiKPS32IAsyUZimTzj/S3ONRySYwGsEEIUSTJGPbnpcDqMwqZjOTORX6bZMqAZIYplY7Xw2mPt5yL3F/8+OxC+bziANwA4EuFvCF7Xlp9A8L8/arLymAdgGwi6iCNNscDmBOqm0v2yA8B7BRCvKkoV9rbrwcgeznMATCeiBKJqAOceaPXBkGuFCJKk4/hXHzcJt1f9ka4E+7c1HMA3CF5NAwEcEqeygYJj9FauJ+XCqvPaAGAkUSULplIRkpltkIRmovcj88ulL/ZEQB2CSFc5p9QPS+9vgHh/n75u/IcDS84V+H3wKnhnw7xvS+Dc8q2BcAm6TUGwCcAtkrlcwC0UrR5WpJ1NwL07jCQqyOcXhqbAWyXnwuApgCWANgr/W0ilROAdyW5tgLICeIzSwZQCqCRoiwszwtOhXQMQDWcI7B7/XlGcNrw86XX3UGSKx9O27H8PXtPqnuj9BlvBrABwDWK6+TA2TnvgzOXOQVBLsufnd2/WS25pPKZcOZmV9YNyfOCft8Q1u8X70BmGIZh6rSZiGEYhjEJKwOGYRiGlQHDMAzDyoBhGIYBKwOGYRgGrAwYhmEYsDJgGIZhwMqAYRiGAfD/oYdb+cCWdaYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_rewards_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 10000 episodes there seems to be not much learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
